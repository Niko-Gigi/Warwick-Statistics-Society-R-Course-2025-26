[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Warwick Statistics Society R Course 2025-2026",
    "section": "",
    "text": "R is a very popular coding language, primarily used by statisticians around the world. As such, it is important for new statisticians (or those who will require substantial statistics in their degrees/jobs) to get to grips with the language. This website and the attached Github aim to cover the key steps from first lines of code after you have installed R, to how to fit linear models and perform Explanatory Data Analysis (EDA).\nThis Github was created for the Warwick Statistics Society’s R course. It accompanies the 2 core Warwick R modules: ST117 Introduction to Statistical Modelling and ST221 Linear Statistical Modelling. While I (Nikoloz) have revamped this R course, the foundational work was set by my predecessor, Maria-Louiza Van den Bergh, who originally compiled this Github. The course builds off of the work by previous R Course Coordinators: Neel Shah, Mia Carla Chapman, and Viresh Shah.\nIf you find any corrections, or believe a section would benefit from more explanations, please do reach out and email me 1.\n\n\n\nQR code for website",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Using R",
    "section": "",
    "text": "Accessing R\nPlease ensure that you have access to R. This can be a local version on your computer (Warwick download link) or online.\nIf you have a local version, it is also worth downloading an IDE (integrated development envrionment). I personally use RStudio.\n\n\nCreating a new document\nThere are many possible files that you can create in R. The simplest is to create an R script. This allows you to write code, run code and produce output. You can also annotate your code through comments by writing # and then adding any relevant text.\nAlternatively, you can create an R Markdown file. This combines chunks of R code with normal text. Thus, you can create lovely reports that blend code output (e.g. figures and tables) with analysis. RStudio has created a detailed guide available here on how to use an R Markdown file.\n\n\nR Studio and R Markdown\nThe following are two cheatsheets produced by R Studio on using their software.\n(If the files are not loading, there also available on the GitHub under the Cheatsheets folder link.)"
  },
  {
    "objectID": "index.html#general-structure",
    "href": "index.html#general-structure",
    "title": "Warwick Statistics Society R Course 2024-2025",
    "section": "",
    "text": "Each section begins with a Welcome that indicates what will be covered by the section. There are then several subsections that cover theory and code examples.\nKey functions appear in purple text like this function(). Longer passages of code appear inside grey boxes with comments denoted using ###| and #. For some code examples, you are able to scroll left and right to see the entire code (this is typically for comments). The code can be hidden or shown by clicking on the triangle (\\(\\triangleright\\)). By default, all code examples are shown and any answers are hidden.\nThe entire code can be copied to the clipboard using the icon in the right corner. Please note that the ```{r} and ``` are used to create code chunks. These are not needed in an R Script or if the code is being copied into an already existing code chunk in an R Markdown or Quarto document.\nFinally, there will be several ‘Check you understanding’ subsections. Answers to these questions (and the code used to produce them) are available just below the questions. However, I strongly recommend you attempt to answer them before reviewing the solutions!\nThe sections of this github cover the following material:\n\n\nThis first section, titled “R-course - Introduction to R”, covers operations, variables, data types and data structures.\nThe second section, titled “R-course - Control structures and functions”, covers if, while and for loops. It also teaches you how to write your own functions.\nThe third section, titled “R-course - Random variables and Plotting”, covers how to generate a random number and how to plot various graphs using base R commands.\nThe fourth and fifth sections, titled “R-course - Linear modelling” and “R-course - Linear modelling assumptions” respectively, cover what a linear model is, how to fit a linear model on categorical and numerical data and how to ensure a linear model meets the necessary assumptions.\n\n\nThe necessary data for this github can be found in the Datasets folder. There is guidance in the relevant sections on how to load in data.\nPlease note that future sections aim to include: model selection; time series modelling; generalised linear modelling; exploratory data analysis; using the ggplot2 package to produce figures; and using the kableExtra and dplyr packages to produce tables. Time permitting, these will be created for the 2024/25 R course and aim to go alongside ST404 Applied Statistical Modelling and provide additional support for ST346 Generalised Linear Models."
  },
  {
    "objectID": "index.html#accessing-r",
    "href": "index.html#accessing-r",
    "title": "Warwick Statistics Society R Course 2024-2025",
    "section": "",
    "text": "Please ensure that you have access to R. This can be a local version on your computer (Warwick download link) or online.\nIf you have a local version, it is also worth downloading an IDE (integrated development envrionment). I personally use RStudio."
  },
  {
    "objectID": "index.html#creating-a-new-document",
    "href": "index.html#creating-a-new-document",
    "title": "Warwick Statistics Society R Course 2024-2025",
    "section": "",
    "text": "There are many possible files that you can create in R. The simplest is to create an R script. This allows you to write code, run code and produce output. You can also annotate your code through comments by writing # and then adding any relevant text.\nAlternatively, you can create an R Markdown file. This combines chunks of R code with normal text. Thus, you can create lovely reports that blend code output (e.g. figures and tables) with analysis. RStudio has created a detailed guide available here on how to use an R Markdown file."
  },
  {
    "objectID": "index.html#footnotes",
    "href": "index.html#footnotes",
    "title": "Warwick Statistics Society R Course 2025-2026",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nMy University of Warwick email is: niko.gigiberia@warwick.ac.uk.↩︎",
    "crumbs": [
      "Home",
      "Welcome"
    ]
  },
  {
    "objectID": "about.html#creating-a-new-document",
    "href": "about.html#creating-a-new-document",
    "title": "Using R",
    "section": "",
    "text": "There are many possible files that you can create in R. The simplest is to create an R script. This allows you to write code, run code and produce output. You can also annotate your code through comments by writing # and then adding any relevant text.\nAlternatively, you can create an R Markdown file. This combines chunks of R code with normal text. Thus, you can create lovely reports that blend code output (e.g. figures and tables) with analysis. RStudio has created a detailed guide available here on how to use an R Markdown file."
  },
  {
    "objectID": "Intro.html",
    "href": "Intro.html",
    "title": "Intro to R",
    "section": "",
    "text": "These pages will cover will introduce some key theory about how R operates and the basic building blocks of coding. It ends with generating plots using inbuilt R commands.\nThis section covers: :::{} * how R stores data/values (data types, data structures and variables), * how to perform operations (arithmetic, comparison and logical), * how to control the structure of your code (if, else, for and while), * how to write your own functions, * random numbers, and * plotting graphs."
  },
  {
    "objectID": "Intro.html#what-data-type-is-this-variable",
    "href": "Intro.html#what-data-type-is-this-variable",
    "title": "Intro to R",
    "section": "What data type is this variable?",
    "text": "What data type is this variable?\nWe can check what data type a variable is using class(). Alternatively, we can confirm if a variable is an integer, logical or a character using is.integer(), is.logical() and is.character() respectively.\n\n\nCode\n```{r}\n###| Character\ncharacter &lt;- \"a banana\"\n\nclass(character)\nis.character(character)\n\n###| Numeric v integer\nnumber &lt;- 5\ninteger &lt;- as.integer(5)\n\nclass(number)\nis.integer(number)\n\nclass(integer)\nis.integer(integer)\n\n###| Decimal value\ndecimal &lt;- 0.5\n\nclass(decimal)\nis.integer(decimal)\n\n###| Logical value\nis.logical(TRUE)\nis.logical(FALSE)\n```\n\n\n[1] \"character\"\n[1] TRUE\n[1] \"numeric\"\n[1] FALSE\n[1] \"integer\"\n[1] TRUE\n[1] \"numeric\"\n[1] FALSE\n[1] TRUE\n[1] TRUE"
  },
  {
    "objectID": "Intro.html#coercion-changing-data-type",
    "href": "Intro.html#coercion-changing-data-type",
    "title": "Intro to R",
    "section": "Coercion (changing data type)",
    "text": "Coercion (changing data type)\nWe can force a variable to switch data type using as.numeric(), as.logical(), as.character() and as.integer().\n\n\nCode\n```{r}\n###| Coercing a boolean variable\nas.numeric(TRUE)\nas.numeric(FALSE)\nas.character(TRUE)\nas.character(FALSE)\n```\n\n\n[1] 1\n[1] 0\n[1] \"TRUE\"\n[1] \"FALSE\"\n\n\n\n\nCode\n```{r}\n###| Coercing a character variable\nas.numeric(\"a\") # this will produce an error!\n```\n\n\nWarning: NAs introduced by coercion\n\n\n[1] NA\n\n\n\n\nCode\n```{r}\n###| Coercing a character variable\nas.character(\"a\")\n\n###| Coercing a numeric variable\nas.numeric(2) \nas.character(2)\n\n###| Coercing to a logic variable\nas.logical(1)\nas.logical(0)\n\nas.logical(\"TRUE\")\nas.logical(\"FALSE\")\n```\n\n\n[1] \"a\"\n[1] 2\n[1] \"2\"\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] FALSE"
  },
  {
    "objectID": "Intro.html#check-your-understanding",
    "href": "Intro.html#check-your-understanding",
    "title": "Intro to R",
    "section": "Check your understanding",
    "text": "Check your understanding\nThe following will test your understanding of all of the main data structures in R.\nQuestion 1\n\n\nCreate a nums vector which contains the numbers from 50 to 100 inclusive. Now create a new vector, numsEven, which indicates which elements are even using nums.\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\n\nnums &lt;- c(50:100)\nnums\n\nnumsEven &lt;- nums[seq(from = 1, to = 51, by = 2)]\nnumsEven\n```\n\n\n [1]  50  51  52  53  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68\n[20]  69  70  71  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87\n[39]  88  89  90  91  92  93  94  95  96  97  98  99 100\n [1]  50  52  54  56  58  60  62  64  66  68  70  72  74  76  78  80  82  84  86\n[20]  88  90  92  94  96  98 100\n\n\nQuestion 2 :::{} * Consider a gambler, with casino winnings from Monday to Friday defined as: + poker_vector &lt;- c(140,-50,20,-120,240) + roulette_vector &lt;- c(-24,-50,100,-350,10) + days_vector &lt;- c(“Monday”,“Tuesday”,“Wednesday”,“Thursday”,“Friday”) + names(poker_vector) &lt;- days_vector + names(roulette_vector) &lt;- days_vector * Calculate the following statistics and print total_daily and total_week: + daily earnings: total_daily + total poker winnings: total_poker + total roulette winnings: total_roulette + total winnings overall: total_week :::\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\npoker_vector &lt;- c(140,-50,20,-120,240)\nroulette_vector &lt;- c(-24,-50,100,-350,10)\ndays_vector &lt;- c(\"Monday\",\"Tuesday\",\"Wednesday\",\"Thursday\",\"Friday\")\nnames(poker_vector) &lt;- days_vector\nnames(roulette_vector) &lt;- days_vector\n\ntotal_daily &lt;- poker_vector + roulette_vector\ntotal_poker &lt;- sum(poker_vector)\ntotal_roulette &lt;- sum(roulette_vector)\n\ntotal_week &lt;- total_poker + total_roulette\ntotal_week_2 &lt;- sum(total_daily)\n\ntotal_daily\ntotal_week\ntotal_week_2\n```\n\n\n   Monday   Tuesday Wednesday  Thursday    Friday \n      116      -100       120      -470       250 \n[1] -84\n[1] -84\n\n\nQuestion 3 :::{} * The sum of squares of the first ten natural numbers is \\(385\\). The square of the sum of the first ten natural numbers is \\(55^2 = 3025\\). Hence the difference between the sum of the squares of the first ten natural numbers and the square of the sum of the first ten natural numbers is \\(3025 - 385 = 2640\\). * Write R code to check this. * Find the difference between the sum of the squares of the first 100 natural numbers and the square of the sum of the first 100 natural numbers. :::\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\nten &lt;- c(1:10)\nsum_of_squares_10 &lt;- sum(ten^2)\nsquare_of_sums_10 &lt;- sum(ten)^2\n\ndifference_10 &lt;- square_of_sums_10 - sum_of_squares_10\ndifference_10\n\nhundred &lt;- c(1:100)\nsum_of_squares &lt;- sum(hundred^2)\nsquare_of_sums &lt;- sum(hundred)^2\n\ndifference &lt;- square_of_sums - sum_of_squares\ndifference\n```\n\n\n[1] 2640\n[1] 25164150\n\n\nQuestion 4\n\n\nThe following vectors represent the box office numbers from the first three Star Wars movies. The first element for each vector corresponds to the US box office revenue and the second element represent the non-US box office revenue.\n\nNew_hope &lt;- c(460.998, 314.4)\nEmpire_strikes &lt;- c(290.475, 247.900)\nReturn_jedi &lt;- c(309.306, 165.8)\n\nConstruct a matrix, star_wars_matrix, with one row for each movie. The first column should be the US revenue and the second the non-US revenue.\nCalculate the US and non-US revenue.\nWhich movie generated the most revenue?\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\nNew_hope &lt;- c(460.998, 314.4)\nEmpire_strikes &lt;- c(290.475, 247.900)\nReturn_jedi &lt;- c(309.306, 165.8)\n\nstar_wars_matrix &lt;- matrix(data = c(New_hope, Empire_strikes,Return_jedi), ncol = 2, nrow = 3, byrow = TRUE)\n\nUS_revenue &lt;- sum(star_wars_matrix[,1])\nnon_US_revenue &lt;- sum(star_wars_matrix[,2])\n\nUS_revenue\nnon_US_revenue\n```\n\n\n[1] 1060.779\n[1] 728.1\n\n\nQuestion 5\n\n\nDefine the following vectors\n\nplanets &lt;- c(“Mercury”,“Venus”,“Earth”,“Mars”,“Jupiter”,“Saturn”,“Uranus”,“Neptune”)\ntype &lt;- c(“Terrestrial planet”,“Terrestrial planet”,“Terrestrial planet”,“Terrestrial planet”,“Gas giant”,“Gas giant”,“Gas giant”,“Gas giant”)\ndiameter &lt;- c(0.382,0.949,1,0.532,11.209,9.449,4.007,3.883)\nrings &lt;- c(FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE)\n\nCreate a dataframe from these vectors\nThe dataframe is unclear. Add the units to the diameter header.\nPrint the information of the planets who have rings.\nAdd additional column to indicate which planet has humans.\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\nplanets &lt;- c(\"Mercury\",\"Venus\",\"Earth\",\"Mars\",\"Jupiter\",\"Saturn\",\"Uranus\",\"Neptune\")\ntype &lt;- c(\"Terrestrial planet\",\"Terrestrial planet\",\"Terrestrial planet\",\"Terrestrial planet\",\"Gas giant\",\"Gas giant\",\"Gas giant\",\"Gas giant\")\ndiameter &lt;- c(0.382,0.949,1,0.532,11.209,9.449,4.007,3.883)\nrings &lt;- c(FALSE,FALSE,FALSE,FALSE,TRUE,TRUE,TRUE,TRUE)\n\ndf &lt;- data.frame(\"planets\" = planets,\n                 \"type\" = type,\n                 \"diameter\" = diameter,\n                 \"rings\" = rings)\n\nnames(df$diameter) &lt;- \"diameter, as a fraction of Earth's\"\n\ndf[which(df$type == \"Gas giant\"),]\n\ndf$humans &lt;- c(0,0,1,0,0,0,0,0)\n```\n\n\n  planets      type diameter rings\n5 Jupiter Gas giant   11.209  TRUE\n6  Saturn Gas giant    9.449  TRUE\n7  Uranus Gas giant    4.007  TRUE\n8 Neptune Gas giant    3.883  TRUE"
  },
  {
    "objectID": "Intro.html#vectors",
    "href": "Intro.html#vectors",
    "title": "Intro to R",
    "section": "Vectors",
    "text": "Vectors\nA vector is a one-dimensional sequence of data elements of the same type. We create a vector using c().\n\n\nCode\n```{r}\n###| Examples of vectors\nc(\"S\",\"T\",\"A\",\"T\",\"S\")\nc(1,2,3,4,5)\nc(1:5) # will output the same as before!\nc(1,2,\"A\")\n```\n\n\n[1] \"S\" \"T\" \"A\" \"T\" \"S\"\n[1] 1 2 3 4 5\n[1] 1 2 3 4 5\n[1] \"1\" \"2\" \"A\"\n\n\n\n\n\n\n\n\nNote\n\n\n\nAs we see in the last example, if we mix data types, the vector will default to storing all entries as characters!\n\n\n\nVectors: Arithmetic\nWe can apply arithmetic to vectors. We can multiple, add and subtract vectors.\n\n\nCode\n```{r}\n###| Vector arithmetic\nx &lt;- c(1,2,3)\n\nx + 3\nx * 3\nx * x\n```\n\n\n[1] 4 5 6\n[1] 3 6 9\n[1] 1 4 9\n\n\n\n\nVectors: Sequences and repetition\nWe can generate vectors using seq() and rep() to generate a sequence or repeat a value.\n\n\nCode\n```{r}\n###| Sequence 1\n\n# by will create a vector where each subsequent value is 0.1 larger than the previous\nseq(from = 0, to = 1, by = 0.1) \n\n# length.out will ensure 4 values are stored in the vector\nseq(from = 0, to = 1, length.out = 4) \n```\n\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n[1] 0.0000000 0.3333333 0.6666667 1.0000000\n\n\n\n\nCode\n```{r}\n###| Sequence 2\n\nseq(from = 0, to = 10, by = 2.7) # note that 10 is not in this vector!\n\nseq(from = 0, to = 10, length.out = 4) \n```\n\n\n[1] 0.0 2.7 5.4 8.1\n[1]  0.000000  3.333333  6.666667 10.000000\n\n\n\n\nCode\n```{r}\n###| Repeat\nrep(1,6)\n\nrep(1:3,each = 2) # will repeat each value twice\n\nrep(1:3,2) # will repeat the entire 1:3 twice\n\nrep(1:3,length.out=6) # will repeat 1:3 until it reaches the length.out\n\nrep(1:3,length.out=5) # note that this is only 5 long and so only has one 3!\n```\n\n\n[1] 1 1 1 1 1 1\n[1] 1 1 2 2 3 3\n[1] 1 2 3 1 2 3\n[1] 1 2 3 1 2 3\n[1] 1 2 3 1 2\n\n\n\n\nVectors: Indexing\nYou can extract data from a vector if you know it’s index (for example, you can extract the 5th value of a vector) using [].\nWe can extract 1 element at a time (see the first example) or we can extract multiple elements at once using vectors (see the second and third examples). We can also extract using a variable (fourth example).\n\n\nCode\n```{r}\n###| Indexing\nx &lt;- c(1,2,3,4,5)\n\nx[3] # extracts the 3rd element\nx[1:3] # extracts elements 1, 2 and 3\nx[c(1,3,5)] # extracts elements 1, 3 and 5\n\n###| Indexing with a named vector\ny &lt;- c(2,4)\nx[y] # extracts elements 2 and 4\n```\n\n\n[1] 3\n[1] 1 2 3\n[1] 1 3 5\n[1] 2 4\n\n\nYou can change the values of a vector using indexes as well.\n\n\nCode\n```{r}\n###| Changing an element\nx &lt;- c(1,2,3,4,5)\n\nx[1] &lt;- 7\nx\n```\n\n\n[1] 7 2 3 4 5\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe number of replacing values must match the number of replaced values, otherwise there is an error (see next example)!\n\n\n\n\nCode\n```{r}\n###| Mismatch in replacement length\nx[2] &lt;- c(1,3)\n```\n\n\nWarning in x[2] &lt;- c(1, 3): number of items to replace is not a multiple of\nreplacement length\n\n\nCode\n```{r}\nx # only the 1st element is used to replace the 2nd value\n```\n\n\n[1] 7 1 3 4 5\n\n\nYou can also remove an element from a vector using [- ].\n\n\nCode\n```{r}\n###| Changing an element\nx &lt;- c(1,2,3,4,5)\nx[-1] \n```\n\n\n[1] 2 3 4 5\n\n\n\n\nVectors: Check your understanding\nTo practise sequences and repetition, do the following exercises.\n\n\nCreate a vector from 0 to 100.\nCreate a vector from 100 to 0.\nCreate a vector containing all strictly positive even numbers up to and including 100.\nCreate a vector containing 1 once, 2 twice and 3 thrice, in that order.\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n\n###| Answers\nseq(from = 0, to = 100)\nseq(from = 100, to = 0)\nseq(from = 0, to = 100, by = 2)\n\nrep(1:3,1:3)\n```\n\n\n  [1]   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n [19]  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n [37]  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n [55]  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n [73]  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n [91]  90  91  92  93  94  95  96  97  98  99 100\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1   0\n [1]   0   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36\n[20]  38  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74\n[39]  76  78  80  82  84  86  88  90  92  94  96  98 100\n[1] 1 2 2 3 3 3"
  },
  {
    "objectID": "Intro.html#lists",
    "href": "Intro.html#lists",
    "title": "Intro to R",
    "section": "Lists",
    "text": "Lists\nA list is a one-dimensional sequence of data of different types. Each element of a list can be of different dimensions and types.\nWe start a list using the list() command.\n\n\n\n\n\n\nNote\n\n\n\nIt is important to be careful with indexing for lists. As lists can contain multiple levels, we use [[]] for the first index and then [] for the second.\n\n\n\n\nCode\n```{r}\n###| Example list\nx &lt;- list(c(1,2,3),\n          100,\n          c(TRUE,FALSE,TRUE),\n          list(\"a\",\"b\",\"c\"))\n\nx[3] # Returns the 3rd element of the list\nx[[3]] # Returns the 3rd element of the list\nx[[3]][2] # Returns 2nd element of the 3rd element of the list\nx[3][2] # Empty!\n```\n\n\n[[1]]\n[1]  TRUE FALSE  TRUE\n\n[1]  TRUE FALSE  TRUE\n[1] FALSE\n[[1]]\nNULL\n\n\nWe can also replace elements in a list and and elements to a list using append().\n\n\nCode\n```{r}\n###| Replacing \nx &lt;- list(c(1,2,3),\n          100,\n          c(TRUE,FALSE,TRUE),\n          list(\"a\",\"b\",\"c\"))\n\nx[[2]] &lt;- c(27,29)\nx\n```\n\n\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] 27 29\n\n[[3]]\n[1]  TRUE FALSE  TRUE\n\n[[4]]\n[[4]][[1]]\n[1] \"a\"\n\n[[4]][[2]]\n[1] \"b\"\n\n[[4]][[3]]\n[1] \"c\"\n\n\n\n\nCode\n```{r}\n###| Append\nappend(x, values = c(\"m\",\"n\"))\n```\n\n\n[[1]]\n[1] 1 2 3\n\n[[2]]\n[1] 27 29\n\n[[3]]\n[1]  TRUE FALSE  TRUE\n\n[[4]]\n[[4]][[1]]\n[1] \"a\"\n\n[[4]][[2]]\n[1] \"b\"\n\n[[4]][[3]]\n[1] \"c\"\n\n\n[[5]]\n[1] \"m\"\n\n[[6]]\n[1] \"n\"\n\n\nHowever, it is again important that we make these changes very carefully to avoid errors as there are 2 layers in indexing.\n\n\n\n\n\n\nNote\n\n\n\nWe can find the number of elements in the first level of the list using length() (i.e. how many different things are in the list). We can find the total number of elements within each first level element of the list using lengths().\n\n\n\n\nCode\n```{r}\n###| Lengths of a list\nx &lt;- list(c(1,2,3),\n          100,\n          c(TRUE,FALSE,TRUE),\n          list(\"a\",\"b\",\"c\"))\n\nlength(x)\nlengths(x)\n```\n\n\n[1] 4\n[1] 3 1 3 3"
  },
  {
    "objectID": "Intro.html#matrix",
    "href": "Intro.html#matrix",
    "title": "Intro to R",
    "section": "Matrix",
    "text": "Matrix\nA matrix is a 2 dimensional storage of data of the same type. We create a matrix using the matrix() command.\n\n\nCode\n```{r}\n###| Example matrix\nmatrix(data = c(1:9), nrow = 3, ncol = 3)\n```\n\n\n     [,1] [,2] [,3]\n[1,]    1    4    7\n[2,]    2    5    8\n[3,]    3    6    9\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, a matrix is filled column-wise first, then row. Using byrow=TRUE inside the matrix() function will switch this to filling row-wise first.\n\n\n\n\nCode\n```{r}\n###| Example matrix\nmatrix(data = c(1:9), nrow = 3, ncol = 3, byrow=TRUE)\n```\n\n\n     [,1] [,2] [,3]\n[1,]    1    2    3\n[2,]    4    5    6\n[3,]    7    8    9\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is very important to ensure the number of columns and rows is correct!\n\n\n\n\nCode\n```{r}\n###| Errors in matrix \nmatrix(c(1,2,3), nrow = 1, ncol=2) \n```\n\n\nWarning in matrix(c(1, 2, 3), nrow = 1, ncol = 2): data length [3] is not a\nsub-multiple or multiple of the number of columns [2]\n\n\n     [,1] [,2]\n[1,]    1    2\n\n\nAs with vectors and lists, we can select only some elements of a matrix using indexing.\n\n\nCode\n```{r}\n###| Matrix indexing\nx &lt;- matrix(data = c(1:9), nrow = 3, ncol = 3)\n\nx[1,]\nx[1,2]\nx[6]\n```\n\n\n[1] 1 4 7\n[1] 4\n[1] 6\n\n\nAs with vectors, we can perform arithmetic operations of matrices, assuming that the dimensions are correct. Not that for matrix multiplication, we use %\\*%.\n\n\nCode\n```{r}\n###| Matrix arithmetic\nx &lt;- x &lt;- matrix(data = c(1:9), nrow = 3, ncol = 3)\n\nx + 3\nx * 3\nx * x     # entry wise multiplication\nx %*% x   # matrix multiplication\n```\n\n\n     [,1] [,2] [,3]\n[1,]    4    7   10\n[2,]    5    8   11\n[3,]    6    9   12\n     [,1] [,2] [,3]\n[1,]    3   12   21\n[2,]    6   15   24\n[3,]    9   18   27\n     [,1] [,2] [,3]\n[1,]    1   16   49\n[2,]    4   25   64\n[3,]    9   36   81\n     [,1] [,2] [,3]\n[1,]   30   66  102\n[2,]   36   81  126\n[3,]   42   96  150"
  },
  {
    "objectID": "Intro.html#dataframes",
    "href": "Intro.html#dataframes",
    "title": "Intro to R",
    "section": "Dataframes",
    "text": "Dataframes\nA dataframe is a 2 dimensional storage of data of different types. It is always filled vertically - i.e. the first list or vector specified is the first column of the dataframe.\nDataframes can have named columns, allowing us to have more control.\n\n\nCode\n```{r}\n###| Example dataframe\ndf &lt;- data.frame(name = c(\"Harry\",\"Ron\",\"Hermione\"),\n                 age = c(17,18,19),\n                 smoker = c(FALSE,TRUE,FALSE))\n\ndf\n\n###| Indexing a dataframe\n\ndf[3,2]\ndf[,\"smoker\"]\ndf[1:2,\"name\"]\ndf$smoker # we can use $ sign to pull named columns\n\n###| Named columns\n\ncolnames(df)\nnames(df)\n```\n\n\n      name age smoker\n1    Harry  17  FALSE\n2      Ron  18   TRUE\n3 Hermione  19  FALSE\n[1] 19\n[1] FALSE  TRUE FALSE\n[1] \"Harry\" \"Ron\"  \n[1] FALSE  TRUE FALSE\n[1] \"name\"   \"age\"    \"smoker\"\n[1] \"name\"   \"age\"    \"smoker\""
  },
  {
    "objectID": "Intro.html#array",
    "href": "Intro.html#array",
    "title": "Intro to R",
    "section": "Array",
    "text": "Array\nAn array is an n-dimensional storage of data of the same type.\n\n\nCode\n```{r}\n###| Example array\narray(data = c(1:6),dim = c(2,2,2))\narray(data = c(1:12),dim = c(2,3,2))\n```\n\n\n, , 1\n\n     [,1] [,2]\n[1,]    1    3\n[2,]    2    4\n\n, , 2\n\n     [,1] [,2]\n[1,]    5    1\n[2,]    6    2\n\n, , 1\n\n     [,1] [,2] [,3]\n[1,]    1    3    5\n[2,]    2    4    6\n\n, , 2\n\n     [,1] [,2] [,3]\n[1,]    7    9   11\n[2,]    8   10   12"
  },
  {
    "objectID": "Intro.html#check-your-understanding-1",
    "href": "Intro.html#check-your-understanding-1",
    "title": "Intro to R",
    "section": "Check your understanding",
    "text": "Check your understanding\nThe following are some extercises to test your understanding about operations.\n\n\nUse R to determine the area of a circle with diameter of 20cm.\n\nHint: pi is recognised in R.\n\nCalculate the cube root of 14 \\(\\times\\) 0.51.\n\nHint: think carefully about the order of operations.\n\nTest the following statement in R: Not (4 squared is greater than or equal to 15, or (the sum of 7 and 10 is less than 16)).\n\nHint: the output should be FALSE.\n\n\n\n(The code is hidden to give you a chance to practise. However, if you toggle the &gt;, example code will appear.)\n\n\nCode\n```{r}\n#| code-fold: true\n\n###| Answers\n(10^2)*pi\n(14 * 0.51)^(3)\n!( 4&lt;=15 | 7+10 &lt; 16)\n```\n\n\n[1] 314.1593\n[1] 363.9943\n[1] FALSE"
  },
  {
    "objectID": "Intro.html#arithmetic-operators",
    "href": "Intro.html#arithmetic-operators",
    "title": "Intro to R",
    "section": "Arithmetic operators",
    "text": "Arithmetic operators\nThe 4 base arithmetic operators are addition (+), subtraction (-), multiplication (*) and division (/). The following are some base examples of how these can be used.\n\n\nCode\n```{r}\n###| Addition\n3+10\n\n###| Subtraction\n6-2\n\n###| Multiplication\n5*15\n\n###| Division\n23/7\n```\n\n\n[1] 13\n[1] 4\n[1] 75\n[1] 3.285714\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note that R follows the traditional BODMASS order of operations. As such, we can use brackets to control the order in which operations are performed.\n\n\n\n\nCode\n```{r}\n###| BODMASS\n(1+7)/2\n1 + 7/2\n3*5+2/3\n3*(5+2)/3\n```\n\n\n[1] 4\n[1] 4.5\n[1] 15.66667\n[1] 7\n\n\nWe can also do exponents (to the power of) using ^ or **, and we can find the modulo using %% and %/%\n\n\nCode\n```{r}\n###| Exponents \n5^2\n5**2\n\n4^(1/2)\n4**0.5\n```\n\n\n[1] 25\n[1] 25\n[1] 2\n[1] 2\n\n\n\n\nCode\n```{r}\n###| Modulo\n10 %% 2 # remainder of 10 divided by 2\n10 %% 4\n\n10 %/% 2 # how many times 2 goes into 10\n10 %/% 4\n```\n\n\n[1] 0\n[1] 2\n[1] 5\n[1] 2"
  },
  {
    "objectID": "Intro.html#comparison-operators",
    "href": "Intro.html#comparison-operators",
    "title": "Intro to R",
    "section": "Comparison operators",
    "text": "Comparison operators\nComparison operators compare 2 values to one another and will return TRUE or FALSE.\nThe 2 operators that can be used for any data type are equal to == and not equal to !=.\nWe can also compare two numbers using less than &lt;, less than or equal to &lt;=, greater than &gt;, and greater than or equal to &gt;=.\n\n\nCode\n```{r}\n###| Equal to\n\"a banana\" == \"a banana\"\n\"a banana\" == \"a fruit\"\n\n7 == 5\n5 == 5\n\n###| Not wqual to\n\"a banana\" != \"a banana\"\n\"a banana\" != \"a fruit\"\n\n7 != 5\n5 != 5\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] FALSE\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] TRUE\n[1] FALSE\n\n\n\n\nCode\n```{r}\n###| Less than\n5 &lt; 7 \n7 &lt; 5\n5 &lt; 5 # note that this will return FALSE!\n\n###| Greater than\n5 &gt; 7 \n7 &gt; 5\n5 &gt; 5 # note that this will return FALSE!\n\n###| Less than or equal to\n5 &lt;= 7 \n7 &lt;= 5\n5 &lt;= 5 # note that this will return TRUE!\n\n###| Greater than or equal to\n5 &gt;= 7 \n7 &gt;= 5\n5 &gt;= 5 # note that this will return TRUE!\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] FALSE\n[1] FALSE\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] TRUE"
  },
  {
    "objectID": "Intro.html#logical-operators",
    "href": "Intro.html#logical-operators",
    "title": "Intro to R",
    "section": "Logical operators",
    "text": "Logical operators\nLogical operators allow us to combine things or negate something.\nIf we want to find the negative, we use the ! operator. For example, we may want to say something is not true. we do this with !TRUE. Or we may want to check if 5 is not less than 7, !(5&lt;7).\n\n\nCode\n```{r}\n###| Negation of TRUE FALSE\nTRUE\n!(TRUE)\n\nFALSE\n!(TRUE)\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] FALSE\n[1] FALSE\n\n\n\n\nCode\n```{r}\n###| Example of negation on relational operators\n5&lt;7\n!(5&lt;7)\n\n5==7\n!(5==7)\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] FALSE\n[1] TRUE\n\n\nWe can also combine operators using AND, &. For example, we may want to check that 5&lt;7 and 10&lt;12. Alternatively, we may want to allow A or B to be true. We do this with |.\n\n\nCode\n```{r}\n###| And on TRUE FALSE\nTRUE & TRUE\nFALSE & FALSE\nTRUE & FALSE\n\n###| And on relational operators example\n(7&gt;5) & (10&lt;12)\n(7&lt;5) & (10&gt;12)\n(7&lt;5) & (10&lt;12)\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] FALSE\n[1] TRUE\n[1] FALSE\n[1] FALSE\n\n\n\n\nCode\n```{r}\n###| OR on TRUE FALSE\nTRUE | TRUE\nFALSE | FALSE\nTRUE | FALSE\n\n###| OR on relational operators example\n(7&gt;5) | (10&lt;12)\n(7&lt;5) | (10&gt;12)\n(7&lt;5) | (10&lt;12)\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] TRUE\n[1] FALSE\n[1] TRUE"
  },
  {
    "objectID": "Intro.html#check-your-understanding-2",
    "href": "Intro.html#check-your-understanding-2",
    "title": "Intro to R",
    "section": "Check your understanding",
    "text": "Check your understanding\nThe following are some extercises to test your understanding about variables.\n\n\nDefine variables called velocity, time and acceleration with 5, 10 and 9.8 respectively. Calculate the displacement.\n\nHint: \\(s = ut+0.5at^2\\).\n\nDefine variables height and width of a rectangle with initial values 5 and 10. Calculate the area of the rectangle.\nExtension: given \\(f(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp{(-\\frac{1}{2}x^2)}\\), find \\(f(5)\\).\n\nHint: look at sqrt() and exp().\n\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n\n###| Answers\nvecolity &lt;- 5\ntime &lt;- 10\nacceleration &lt;- 9.8\ndisplacement &lt;- vecolity * time + 0.5 * acceleration * time^2\ndisplacement\n\nheight &lt;- 5\nwidth &lt;- 10\narea &lt;- height * width\narea\n\n###| Extension question\nx &lt;- 5\nf_x &lt;- 1/sqrt(2*pi)*exp(-0.5*x^2)\nf_x\n```\n\n\n[1] 540\n[1] 50\n[1] 1.48672e-06"
  },
  {
    "objectID": "Intro.html#check-your-understanding-3",
    "href": "Intro.html#check-your-understanding-3",
    "title": "Intro to R",
    "section": "Check your understanding",
    "text": "Check your understanding\nAnswer the following questions using if(), for() and while() loops.\n\n\nGiven a number, print “Positive” or “Negative” depending on if the number is positive or negative. If the number is 0, print both.\nGiven a vector, find the maximum and minimum value. Thus find out the range.\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answers\nx=-1\nif (x &gt; 0) {\n  print(\"positive\")\n} else if (x &lt; 0) {\n  print(\"negative\")\n} else {print(\"both\")}\n```\n\n\n[1] \"negative\"\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answers\nx &lt;- c(0,1,2,7,-5)\nn &lt;- length(x)\nmin &lt;- x[1]\nmax &lt;- x[1]\n\nfor(i in 1:n){\n  if (x[i] &lt; min) {\n    min &lt;- x[i]\n  }\n  if (x[i] &gt; max) {\n    max &lt;- x[i]\n  }\n}\n\nprint(paste(\"Range is\",max-min))\n```\n\n\n[1] \"Range is 12\""
  },
  {
    "objectID": "Linear Modelling.html",
    "href": "Linear Modelling.html",
    "title": "Linear Modelling",
    "section": "",
    "text": "Test"
  },
  {
    "objectID": "Intro.html#if-statements",
    "href": "Intro.html#if-statements",
    "title": "Intro to R",
    "section": "If statements",
    "text": "If statements\nThe code of code after an if statement is only exectued if the conditoin within the if statement is met.\nIf statements are of the following form:\n\n\nif (condition with TRUE/FALSE answer) {code to be executed if true}\n\n\nWe have several examples:\n\n\nCode\n```{r}\n###| Example of if()\nif (5&lt;7) {\n  print(\"Hello world.\")\n}\n\n###| Example of if() and else()\nif (5 &gt; 7) {\n  print(\"Not it.\")\n} else {\n    print(\"It.\")\n}\n```\n\n\n[1] \"Hello world.\"\n[1] \"It.\""
  },
  {
    "objectID": "Intro.html#for-loops",
    "href": "Intro.html#for-loops",
    "title": "Intro to R",
    "section": "For loops",
    "text": "For loops\nThe for loop allows code to be exectured repeatedly until a given iteration.\nIf follows the general structure:\n\n\nfor (variable in condition) {exectute code}\n\n\n\n\nCode\n```{r}\n###| For loop example\nfor (x in c(1:4)) {\n  print(x)\n}\n\n###| Nested loop\nfor (x in c(1:4)) {\n  print(x)\n  for (y in c(1:3)) {\n    print(y)\n  }\n}\n```\n\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 1\n[1] 1\n[1] 2\n[1] 3\n[1] 2\n[1] 1\n[1] 2\n[1] 3\n[1] 3\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 1\n[1] 2\n[1] 3"
  },
  {
    "objectID": "Intro.html#while-loops",
    "href": "Intro.html#while-loops",
    "title": "Intro to R",
    "section": "While loops",
    "text": "While loops\nWhile loop tests if a condition is true and will continue running the code until the condition is false.\nThey follow the general structure:\n\n\nwhile (condition) {execute code}\n\n\nIt is very important to avoid infinite loops!!\n\n\nCode\n```{r}\n###| While loop example\ni &lt;- 1\nwhile (i &lt;= 5){\n  print(i)\n  i &lt;- i + 1\n}\n```\n\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5"
  },
  {
    "objectID": "Intro.html#check-your-understanding-4",
    "href": "Intro.html#check-your-understanding-4",
    "title": "Intro to R",
    "section": "Check your understanding",
    "text": "Check your understanding\nWrite functions that do the following things:\n\n\nCreate a function which returns integer division of x by y and the remainder\nCreate a function which finds the area and circumference of a circle given a radius r.\nCreate a function that calculates the factorial of some number n.\n\nHint: make sure that n is an integer!\n\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\n\nmodulus &lt;- function(x,y) {\n  remainder &lt;- x %% y\n  multiple &lt;- x %/% y\n  print(paste(\"x modulo y is\", multiple, \"with remainder\", remainder))\n}\n\nmodulus(6,3)\n```\n\n\n[1] \"x modulo y is 2 with remainder 0\"\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\n\ncircle &lt;- function(r){\n  area &lt;- r^2 * pi\n  circumference &lt;- 2*r*pi\n  print(paste(\"Area is\",area, \"and the circumference is\",circumference))\n}\n\ncircle(5)\n```\n\n\n[1] \"Area is 78.5398163397448 and the circumference is 31.4159265358979\"\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\n\nfactorial &lt;- function(x) {\n  \n  if (x%%1 == 0) {\n    \n    factorial &lt;- 1\n    \n    while (x &gt;= 1) {\n      factorial &lt;- factorial * x\n      x &lt;- x - 1\n    }\n    \n    print(factorial)\n  } else {\n    print(paste(x,\" is not a positive integer\"))\n  }\n}\n\nfactorial(2) \n```\n\n\n[1] 2"
  },
  {
    "objectID": "Intro.html#check-your-understanding-5",
    "href": "Intro.html#check-your-understanding-5",
    "title": "Intro to R",
    "section": "Check your understanding",
    "text": "Check your understanding\nWrite functions that do the following things:\n\n\nCreate a function which returns integer division of x by y and the remainder\nCreate a function which finds the area and circumference of a circle given a radius r.\nCreate a function that calculates the factorial of some number n.\n\nHint: make sure that n is an integer!\n\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\n\nmodulus &lt;- function(x,y) {\n  remainder &lt;- x %% y\n  multiple &lt;- x %/% y\n  print(paste(\"x modulo y is\", multiple, \"with remainder\", remainder))\n}\n\nmodulus(6,3)\n```\n\n\n[1] \"x modulo y is 2 with remainder 0\"\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\n\ncircle &lt;- function(r){\n  area &lt;- r^2 * pi\n  circumference &lt;- 2*r*pi\n  print(paste(\"Area is\",area, \"and the circumference is\",circumference))\n}\n\ncircle(5)\n```\n\n\n[1] \"Area is 78.5398163397448 and the circumference is 31.4159265358979\"\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\n\nfactorial &lt;- function(x) {\n  \n  if (x%%1 == 0) {\n    \n    factorial &lt;- 1\n    \n    while (x &gt;= 1) {\n      factorial &lt;- factorial * x\n      x &lt;- x - 1\n    }\n    \n    print(factorial)\n  } else {\n    print(paste(x,\" is not a positive integer\"))\n  }\n}\n\nfactorial(2) \n```\n\n\n[1] 2"
  },
  {
    "objectID": "figures.html",
    "href": "figures.html",
    "title": "Figures",
    "section": "",
    "text": "In R, it is very easy to create scatter plots, line graphs, histograms and box plots. We will cover how to do all 4 using base R commands.\nOther sections will cover how to do these in other packages (namely ‘ggplot2’) to get more control over formatting.\n\n\nWe will start by generating a sample from a \\(\\text{Uniform}[0,1]\\) distribution and plotting a scatter plot.\n\n\nCode\n```{r}\n##| Generating a sample\nsample &lt;- runif(n = 100, min = 0, max = 1)\n\n###| Scatterplot\nplot(sample)\n```\n\n\n\n\n\n\n\n\n\nIf we sort the sample by size, we can create a plot that increases from left to right.\n\n\nCode\n```{r}\n###| Ordered sample scatter plot\nplot(sort(sample))\n```\n\n\n\n\n\n\n\n\n\nThe x and y labs are not clear. We should change these by adding in the xlab and ylab arguments. We should also add a title for clarity!\n\n\nCode\n```{r}\n###| x and y labels and title (called main) \nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\")\n```\n\n\n\n\n\n\n\n\n\nWe observe a linear relationship between the ordered samples. Let us test this by adding a line to this graph using ‘abline()’.\n\n\nCode\n```{r}\n###| Using abline\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\")\nabline(a = 0,b=0.01)\n```\n\n\n\n\n\n\n\n\n\nIt is quite hard to see both the points and the line at the same time. We can change the aesthetics (colour, shape and size) to make this easier to see. Additional point shapes are available here.\n\n\nCode\n```{r}\n###| Aesthetics - colour and points\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\", \n     col = \"red\", pch = 3)        # we start a new line for readability :)\nabline(a = 0,b=0.01, col = \"blue\")\n```\n\n\n\n\n\n\n\n\n\nSuppose we wanted to add additional lines, at the Sample = 0.5 and at the 50th index. We can add these using ‘abline()’. We can also distinguish them by changing colour, line thickness and line type. Additional details are available here.\n\n\nCode\n```{r}\n###| Aesthetics - line\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\", \n     col = \"red\", pch = 3)\nabline(a = 0,b=0.01, col = \"blue\") \nabline(h=0.5, col = \"black\", lwd = 1, lty = 3) # Note that h here is a horizontal line at sample = 0.5\nabline(v=50, col = \"black\", lwd = 1, lty = 4)  # v here is a vertical line at index = 50\n```\n\n\n\n\n\n\n\n\n\n\n\n\nNow, we move to generating a line graph. We will generate a new sample, this time from a \\(\\text{Normal}(0,1)\\) distribution.\n\n\nCode\n```{r}\n###| Generate sample\nsample &lt;- rnorm(n = 30, mean = 0, sd = 1)\n\n###| Line graph\nplot(sample, type = \"l\")\n```\n\n\n\n\n\n\n\n\n\nSuppose we also want to show the points. We change the type from “l” to “b”.\n\n\nCode\n```{r}\n###| Line graph with points\nplot(sample, type = \"b\")\n```\n\n\n\n\n\n\n\n\n\nAgain, we can play with the aesthetics of these graphs.\n\n\nCode\n```{r}\n###| Line graph aesthetics\nplot(sample, type = \"l\", lwd = 2, lty = 3)\nplot(sample, type = \"b\", lwd = 1, lty = 5, pch = 17, col = 3)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose we have another sample generated from a \\(\\text{Normal}(0,2)\\). We can show both lines on the same graph.\n\n\nCode\n```{r}\n###| For reproducibility\nset.seed(68)\n\n###| Generate 2 samples\nsample1 &lt;- rnorm(n=30, mean = 0, sd = 1)\nsample2 &lt;- rnorm(n=30, mean = 0, sd = 2)\n\n###| Line plot with multiple lines\nplot(sample1, type = \"l\")\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe have something, but it appears the top is cut off the graph (this is beacuse we are adding sample 2 onto sample 1 and sample 2 has a larger range). We can change the y axis limits.\n\n\nCode\n```{r}\n###| Line plot with multiple lines with limits\nplot(sample1, type = \"l\", ylim = c(-4,4))\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe should also change the colours for readability.\n\n\nCode\n```{r}\n###| Line plot with multiple lines with limits\nplot(sample1, type = \"l\", ylim = c(-4,4), col = \"blue\")\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe can have different types of lines in the same graph as well.\n\n\nCode\n```{r}\n###| Line plot with multiple line types\nplot(sample1, type = \"l\", ylim = c(-4,4), col = \"blue\")\nlines(sample2, lty = 4, type = \"b\")\n```\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose instead we want to look at the distribution of the sample, and test if it aligns with the distribution we expect. To do this, we will up the number of samples (200) and will look at the histogram graph.\n\n\nCode\n```{r}\n###| Sample\nsample &lt;- rnorm(n=200, mean = 0, sd =1)\n\n###| Histogram\nhist(sample)\n```\n\n\n\n\n\n\n\n\n\nIf we want to have the probability on the y axis instead, we need to change the freq argument to FALSE or we change the prob argument to TRUE.\n\n\nCode\n```{r}\n###| Density graph\nhist(sample, freq = FALSE)\nhist(sample, prob = TRUE)\n```\n\n\n\n\n\n\n\n\n\nThe widths of the boxes are quite large. We can change this with the breaks argument.\n\n\nCode\n```{r}\n###| Set number of breaks\nhist(sample, breaks = 10) # 10 boxes\nhist(sample, breaks = 20) # 20 boxes\n\n###| Different size boxes\nhist(sample, breaks = c(min(sample),-1.5,0,0.5,1,max(sample))) \n      # need min and max to ensure range works\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also change the labels and add a title.\n\n\nCode\n```{r}\n###| Labels and title\nhist(sample, breaks = 20, xlab = \"Sample\", main = \"Title\")  \n```\n\n\n\n\n\n\n\n\n\nNow, lets compare this to the normal distribution. First, we can add the density curve of the sample.\n\n\nCode\n```{r}\nsample &lt;- rnorm(n = 1000, mean = 0, sd = 1) # Increasing sample count\n\n###| Density curve\nhist(sample, prob = TRUE)\nlines(density(sample))\n```\n\n\n\n\n\n\n\n\n\nWe can then add the normal distribution curve on top to allow comparison.\n\n\nCode\n```{r}\n###| Normal\nseq &lt;- seq(from = -3, to = 3, length = 1000)\nnorm_seq &lt;- dnorm(seq(from = -3, to = 3, length = 1000), mean = 0, sd = 1)\n\n###| Density curve\nhist(sample, prob = TRUE, ylim = c(0,max(norm_seq)))\nlines(density(sample))\nlines(seq,norm_seq, col = \"red\",lwd = 2)\n```\n\n\n\n\n\n\n\n\n\nThe sample distribution is very similar to the normal distribution curve (although the sample has 2 peaks rather than 1). However, it is confusing to have 2 lines without any labels. So, let us add a legend. Additional details appear here.\n\n\nCode\n```{r}\n###| Normal\nseq &lt;- seq(from = -3, to = 3, length = 1000)\nnorm_seq &lt;- dnorm(seq(from = -3, to = 3, length = 1000), mean = 0, sd = 1)\n\n###| Density curve\nhist(sample, prob = TRUE, ylim = c(0,max(norm_seq)))\nlines(density(sample))\nlines(seq,norm_seq, col = \"red\",lwd = 2)\nlegend(x = \"topright\", # position\n       legend = c(\"Sample curve\",\"Normal curve\"),\n       col = c(\"black\",\"red\"),\n       lwd = c(1,2))\n```\n\n\n\n\n\n\n\n\n\n\n\n\nFinally, we will consider box plots. We can plot the box plot of a single sample.\n\n\nCode\n```{r}\n###| Generating a sample\nsample &lt;- rnorm(n = 100, mean = 5, sd = 2)\n\n###| Boxplot\nboxplot(sample)\n```\n\n\n\n\n\n\n\n\n\nFor multiple boxplots, we can construct a data frame and plot accordingly.\n\n\nCode\n```{r}\n###| Data frame\ndf &lt;- data.frame(\"A\" = rnorm(n = 10, mean = 0, sd = 1),\n                 \"B\" = rnorm(n = 10, mean = 0, sd = 1),\n                 \"C\" = rnorm(n = 10, mean = 0, sd = 1))\n\n###| Box plot\nboxplot(df)\n```\n\n\n\n\n\n\n\n\n\nWe can add labels and titles.\n\n\nCode\n```{r}\n###| Box plot with labels\nboxplot(df, main = \"Title\", ylab = \"Sample values\", xlab = \"Categories\")\n```\n\n\n\n\n\n\n\n\n\n\n\n\nGenerate scatterplots, line graphs, histograms and box plots with samples from Poisson and Binomial distributions. Add legends and appropriate titles to each graph."
  },
  {
    "objectID": "figures.html#scatterplots",
    "href": "figures.html#scatterplots",
    "title": "Figures",
    "section": "",
    "text": "We will start by generating a sample from a \\(\\text{Uniform}[0,1]\\) distribution and plotting a scatter plot.\n\n\nCode\n```{r}\n##| Generating a sample\nsample &lt;- runif(n = 100, min = 0, max = 1)\n\n###| Scatterplot\nplot(sample)\n```\n\n\n\n\n\n\n\n\n\nIf we sort the sample by size, we can create a plot that increases from left to right.\n\n\nCode\n```{r}\n###| Ordered sample scatter plot\nplot(sort(sample))\n```\n\n\n\n\n\n\n\n\n\nThe x and y labs are not clear. We should change these by adding in the xlab and ylab arguments. We should also add a title for clarity!\n\n\nCode\n```{r}\n###| x and y labels and title (called main) \nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\")\n```\n\n\n\n\n\n\n\n\n\nWe observe a linear relationship between the ordered samples. Let us test this by adding a line to this graph using ‘abline()’.\n\n\nCode\n```{r}\n###| Using abline\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\")\nabline(a = 0,b=0.01)\n```\n\n\n\n\n\n\n\n\n\nIt is quite hard to see both the points and the line at the same time. We can change the aesthetics (colour, shape and size) to make this easier to see. Additional point shapes are available here.\n\n\nCode\n```{r}\n###| Aesthetics - colour and points\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\", \n     col = \"red\", pch = 3)        # we start a new line for readability :)\nabline(a = 0,b=0.01, col = \"blue\")\n```\n\n\n\n\n\n\n\n\n\nSuppose we wanted to add additional lines, at the Sample = 0.5 and at the 50th index. We can add these using ‘abline()’. We can also distinguish them by changing colour, line thickness and line type. Additional details are available here.\n\n\nCode\n```{r}\n###| Aesthetics - line\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\", \n     col = \"red\", pch = 3)\nabline(a = 0,b=0.01, col = \"blue\") \nabline(h=0.5, col = \"black\", lwd = 1, lty = 3) # Note that h here is a horizontal line at sample = 0.5\nabline(v=50, col = \"black\", lwd = 1, lty = 4)  # v here is a vertical line at index = 50\n```"
  },
  {
    "objectID": "figures.html#line-graphs",
    "href": "figures.html#line-graphs",
    "title": "Figures",
    "section": "",
    "text": "Now, we move to generating a line graph. We will generate a new sample, this time from a \\(\\text{Normal}(0,1)\\) distribution.\n\n\nCode\n```{r}\n###| Generate sample\nsample &lt;- rnorm(n = 30, mean = 0, sd = 1)\n\n###| Line graph\nplot(sample, type = \"l\")\n```\n\n\n\n\n\n\n\n\n\nSuppose we also want to show the points. We change the type from “l” to “b”.\n\n\nCode\n```{r}\n###| Line graph with points\nplot(sample, type = \"b\")\n```\n\n\n\n\n\n\n\n\n\nAgain, we can play with the aesthetics of these graphs.\n\n\nCode\n```{r}\n###| Line graph aesthetics\nplot(sample, type = \"l\", lwd = 2, lty = 3)\nplot(sample, type = \"b\", lwd = 1, lty = 5, pch = 17, col = 3)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose we have another sample generated from a \\(\\text{Normal}(0,2)\\). We can show both lines on the same graph.\n\n\nCode\n```{r}\n###| For reproducibility\nset.seed(68)\n\n###| Generate 2 samples\nsample1 &lt;- rnorm(n=30, mean = 0, sd = 1)\nsample2 &lt;- rnorm(n=30, mean = 0, sd = 2)\n\n###| Line plot with multiple lines\nplot(sample1, type = \"l\")\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe have something, but it appears the top is cut off the graph (this is beacuse we are adding sample 2 onto sample 1 and sample 2 has a larger range). We can change the y axis limits.\n\n\nCode\n```{r}\n###| Line plot with multiple lines with limits\nplot(sample1, type = \"l\", ylim = c(-4,4))\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe should also change the colours for readability.\n\n\nCode\n```{r}\n###| Line plot with multiple lines with limits\nplot(sample1, type = \"l\", ylim = c(-4,4), col = \"blue\")\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe can have different types of lines in the same graph as well.\n\n\nCode\n```{r}\n###| Line plot with multiple line types\nplot(sample1, type = \"l\", ylim = c(-4,4), col = \"blue\")\nlines(sample2, lty = 4, type = \"b\")\n```"
  },
  {
    "objectID": "figures.html#histograms",
    "href": "figures.html#histograms",
    "title": "Figures",
    "section": "",
    "text": "Suppose instead we want to look at the distribution of the sample, and test if it aligns with the distribution we expect. To do this, we will up the number of samples (200) and will look at the histogram graph.\n\n\nCode\n```{r}\n###| Sample\nsample &lt;- rnorm(n=200, mean = 0, sd =1)\n\n###| Histogram\nhist(sample)\n```\n\n\n\n\n\n\n\n\n\nIf we want to have the probability on the y axis instead, we need to change the freq argument to FALSE or we change the prob argument to TRUE.\n\n\nCode\n```{r}\n###| Density graph\nhist(sample, freq = FALSE)\nhist(sample, prob = TRUE)\n```\n\n\n\n\n\n\n\n\n\nThe widths of the boxes are quite large. We can change this with the breaks argument.\n\n\nCode\n```{r}\n###| Set number of breaks\nhist(sample, breaks = 10) # 10 boxes\nhist(sample, breaks = 20) # 20 boxes\n\n###| Different size boxes\nhist(sample, breaks = c(min(sample),-1.5,0,0.5,1,max(sample))) \n      # need min and max to ensure range works\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also change the labels and add a title.\n\n\nCode\n```{r}\n###| Labels and title\nhist(sample, breaks = 20, xlab = \"Sample\", main = \"Title\")  \n```\n\n\n\n\n\n\n\n\n\nNow, lets compare this to the normal distribution. First, we can add the density curve of the sample.\n\n\nCode\n```{r}\nsample &lt;- rnorm(n = 1000, mean = 0, sd = 1) # Increasing sample count\n\n###| Density curve\nhist(sample, prob = TRUE)\nlines(density(sample))\n```\n\n\n\n\n\n\n\n\n\nWe can then add the normal distribution curve on top to allow comparison.\n\n\nCode\n```{r}\n###| Normal\nseq &lt;- seq(from = -3, to = 3, length = 1000)\nnorm_seq &lt;- dnorm(seq(from = -3, to = 3, length = 1000), mean = 0, sd = 1)\n\n###| Density curve\nhist(sample, prob = TRUE, ylim = c(0,max(norm_seq)))\nlines(density(sample))\nlines(seq,norm_seq, col = \"red\",lwd = 2)\n```\n\n\n\n\n\n\n\n\n\nThe sample distribution is very similar to the normal distribution curve (although the sample has 2 peaks rather than 1). However, it is confusing to have 2 lines without any labels. So, let us add a legend. Additional details appear here.\n\n\nCode\n```{r}\n###| Normal\nseq &lt;- seq(from = -3, to = 3, length = 1000)\nnorm_seq &lt;- dnorm(seq(from = -3, to = 3, length = 1000), mean = 0, sd = 1)\n\n###| Density curve\nhist(sample, prob = TRUE, ylim = c(0,max(norm_seq)))\nlines(density(sample))\nlines(seq,norm_seq, col = \"red\",lwd = 2)\nlegend(x = \"topright\", # position\n       legend = c(\"Sample curve\",\"Normal curve\"),\n       col = c(\"black\",\"red\"),\n       lwd = c(1,2))\n```"
  },
  {
    "objectID": "figures.html#box-plots",
    "href": "figures.html#box-plots",
    "title": "Figures",
    "section": "",
    "text": "Finally, we will consider box plots. We can plot the box plot of a single sample.\n\n\nCode\n```{r}\n###| Generating a sample\nsample &lt;- rnorm(n = 100, mean = 5, sd = 2)\n\n###| Boxplot\nboxplot(sample)\n```\n\n\n\n\n\n\n\n\n\nFor multiple boxplots, we can construct a data frame and plot accordingly.\n\n\nCode\n```{r}\n###| Data frame\ndf &lt;- data.frame(\"A\" = rnorm(n = 10, mean = 0, sd = 1),\n                 \"B\" = rnorm(n = 10, mean = 0, sd = 1),\n                 \"C\" = rnorm(n = 10, mean = 0, sd = 1))\n\n###| Box plot\nboxplot(df)\n```\n\n\n\n\n\n\n\n\n\nWe can add labels and titles.\n\n\nCode\n```{r}\n###| Box plot with labels\nboxplot(df, main = \"Title\", ylab = \"Sample values\", xlab = \"Categories\")\n```"
  },
  {
    "objectID": "figures.html#check-your-understanding",
    "href": "figures.html#check-your-understanding",
    "title": "Figures",
    "section": "",
    "text": "Generate scatterplots, line graphs, histograms and box plots with samples from Poisson and Binomial distributions. Add legends and appropriate titles to each graph."
  },
  {
    "objectID": "Introduction to R/Random numbers.html",
    "href": "Introduction to R/Random numbers.html",
    "title": "Random numbers",
    "section": "",
    "text": "Inbuilt into R, there are many functions focused on statistical distributions. These are all stored in the ‘stats’ package, which is loaded in by default in R. However, for practise, (and to ensure everything will work), we will load in the package directly.\n\n\nCode\n```{r}\n###| Loading in stats package\nlibrary(stats)\n```\n\n\nWe will begin by looking at generating a uniform number before doing on to look at continuous and discrete random variables.\n\nRandom uniform numbers\nTo generate a random number, we are going to use the uniform distribution. Let \\(X\\sim \\text{Uniform}[a,b]\\). Then, the probability mass function of X is\n\\[\nf(x) = \\begin{cases}\n\\frac{1}{b-a} & \\text{for } a\\leq x\\leq b, \\\\\n0 & \\text{for } x&lt;a \\text{ or } x&gt;b\n\\end{cases}\n\\]\nWe are going to use the inbuilt R functions to generate a random number, x, in the interval \\([a,b]\\) and find the probability we generated x.\nFor simplicity, we will demonstrate this with 2 uniform random variables: \\(X\\sim \\text{Uniform}[0,1]\\) and \\(Y\\sim \\text{Uniform}[1,5]\\).\n\n\nCode\n```{r}\n###| Generate a random number, x\nx &lt;- runif(n = 1, min = 0, max = 1)\nx\n\n###| Probability we generated x\nprob_x &lt;- punif(x, min = 0, max = 1)\nprob_x\n\n###| Generate a random number, y\ny &lt;- runif(n = 1, min = 1, max = 5)\ny\n\n###| Probability we generated y\nprob_y &lt;- punif(y, min = 1, max = 5)\nprob_y\n```\n\n\n[1] 0.9805846\n[1] 0.9805846\n[1] 3.822689\n[1] 0.7056722\n\n\nWe can also find the value, \\(x\\), that corresponds to the probability \\(P(X\\leq x) = a\\).\n\n\nCode\n```{r}\n###| a = 0.5, X ~ Uniform[0,1]\nqunif(0.5, min = 0, max = 1)\n\n###| a = 0.5, X ~ Uniform[0,5]\nqunif(0.5, min = 0, max = 5)\n\n###| a = 0.75, X ~ Uniform[0,5]\nqunif(0.75, min = 0, max = 5)\n```\n\n\n[1] 0.5\n[1] 2.5\n[1] 3.75\n\n\nWe can also find the density of values \\(x\\) and \\(y\\).\n\n\nCode\n```{r}\n###| Density of x, X ~ Uniform[0,1]\nx &lt;- runif(n = 1, min = 0, max = 1)\ndunif(x, min = 0,max = 1)\n\n###| Density of y, Y ~ Uniform[1,5]\ny &lt;- runif(n = 1, min = 1, max = 5)\ndunif(y, min = 1,max = 5)\n```\n\n\n[1] 1\n[1] 0.25\n\n\nUp until now, we have only generated 1 number at a time. However, we can generate several numbers at once by changing the $n = $ argument. However, this only works for the ‘runif()’ function!\n\n\nCode\n```{r}\n###| Multiple values at once\nrunif(n = 5, min = 0, max = 1)\nrunif(n = 3, min = -1, max = 1)\n```\n\n\n[1] 0.3304437 0.5607250 0.5797799 0.9419879 0.6397729\n[1] -0.6708021 -0.1787737 -0.1669490\n\n\n\n\nBinomial distribution\nNow, suppose we want to sample from a binomial distribution, i.e. we want to generate a random number so that the probability \\(X=x\\) is the same as the probability \\(Y=x\\), where \\(Y \\sim \\text{Binomial}(m,p)\\). We do this using ‘rbinom()’, ‘dbinom()’, ‘pbinom()’ and ‘qbinom()’. Note that \\(n\\) refers to the number of samples while size refers to the number of trials!\n\n\nCode\n```{r}\n###| Generate a random value from X ~ Binom(m = 5, p = 0.2)\nx &lt;- rbinom(n=1, size = 5, prob = 0.2)\nx\n\n###| Probability generated x\nprob_x &lt;- pbinom(x, size = 5, prob = 0.2)\nprob_x\n\n###| f(x), density of x\ndensity_x &lt;- dbinom(x, size = 5, prob = 0.2)\ndensity_x\n\n###| Quantile of 0.75\nqbinom(p = 0.75, size = 5, prob = 0.2)\n```\n\n\n[1] 2\n[1] 0.94208\n[1] 0.2048\n[1] 2\n\n\n\n\nCheck your understanding - Normal and Poisson distribution\nIn the above sections, we have gone over how to sample from Uniform and Binomial distribution. To check your understanding, try the following:\n\n\nSample 5 values from a Poisson distribution with rate \\(\\lambda = 5\\) and find the probability and density of each value. Return this as a dataframe.\n\nHint: type ?rpois into the command line or into the help section.\n\nSample 5 values from a Normal distribution with mean 0 and variance 4. Find the density of each value, to 3 significant figures.\n\nHint: type ?rnorm and ?round into the command line or into the help section.\n\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Generate and store 5 values from Poisson\nsample &lt;- rpois(n = 5, lambda = 5)\nsample\n\n###| Find probability, density and quantile\ndf &lt;- data.frame(\"sample\" = sample,\n                 \"prob\" = rep(NA,length(sample)),\n                 \"density\" = rep(NA,length(sample)))\n\n\nfor (i in 1:length(sample)) {\n  df$prob[i] &lt;- ppois(df$sample[i], lambda = 5)\n  df$density[i] &lt;- dpois(df$sample[i], lambda = 5)\n}\n\ndf\n```\n\n\n[1] 10  8  3  2  4\n  sample      prob    density\n1     10 0.9863047 0.01813279\n2      8 0.9319064 0.06527804\n3      3 0.2650259 0.14037390\n4      2 0.1246520 0.08422434\n5      4 0.4404933 0.17546737\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Generate Normal distribution\nnorm &lt;- rnorm(n = 5, mean = 0, sd = 2)\nnorm\n\n###| Find density of each variable\nfor (i in 1:length(norm)){\n  print(paste(round(norm[i],3),\" has density \", round(dnorm(norm[i],mean = 0, sd = 2),3)))\n}\n\n# Alternatively, we can do this in one step without the loop as dnorm() \n# allows vector input\ndnorm(norm, mean=0,sd=2) \n```\n\n\n[1] -0.33344264 -0.08099315 -1.41945430  0.80238623  0.03565988\n[1] \"-0.333  has density  0.197\"\n[1] \"-0.081  has density  0.199\"\n[1] \"-1.419  has density  0.155\"\n[1] \"0.802  has density  0.184\"\n[1] \"0.036  has density  0.199\"\n[1] 0.1967181 0.1993076 0.1550602 0.1840471 0.1994394",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Random numbers"
    ]
  },
  {
    "objectID": "Introduction to R/figures.html",
    "href": "Introduction to R/figures.html",
    "title": "Figures",
    "section": "",
    "text": "In R, it is very easy to create scatter plots, line graphs, histograms and box plots. We will cover how to do all 4 using base R commands.\nOther sections will cover how to do these in other packages (namely ‘ggplot2’) to get more control over formatting.\n\nScatterplots\nWe will start by generating a sample from a \\(\\text{Uniform}[0,1]\\) distribution and plotting a scatter plot.\n\n\nCode\n```{r}\n##| Generating a sample\nsample &lt;- runif(n = 100, min = 0, max = 1)\n\n###| Scatterplot\nplot(sample)\n```\n\n\n\n\n\n\n\n\n\nIf we sort the sample by size, we can create a plot that increases from left to right.\n\n\nCode\n```{r}\n###| Ordered sample scatter plot\nplot(sort(sample))\n```\n\n\n\n\n\n\n\n\n\nThe x and y labs are not clear. We should change these by adding in the xlab and ylab arguments. We should also add a title for clarity!\n\n\nCode\n```{r}\n###| x and y labels and title (called main) \nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\")\n```\n\n\n\n\n\n\n\n\n\nWe observe a linear relationship between the ordered samples. Let us test this by adding a line to this graph using ‘abline()’.\n\n\nCode\n```{r}\n###| Using abline\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\")\nabline(a = 0,b=0.01)\n```\n\n\n\n\n\n\n\n\n\nIt is quite hard to see both the points and the line at the same time. We can change the aesthetics (colour, shape and size) to make this easier to see. Additional point shapes are available here.\n\n\nCode\n```{r}\n###| Aesthetics - colour and points\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\", \n     col = \"red\", pch = 3)        # we start a new line for readability :)\nabline(a = 0,b=0.01, col = \"blue\")\n```\n\n\n\n\n\n\n\n\n\nSuppose we wanted to add additional lines, at the Sample = 0.5 and at the 50th index. We can add these using ‘abline()’. We can also distinguish them by changing colour, line thickness and line type. Additional details are available here.\n\n\nCode\n```{r}\n###| Aesthetics - line\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\", \n     col = \"red\", pch = 3)\nabline(a = 0,b=0.01, col = \"blue\") \nabline(h=0.5, col = \"black\", lwd = 1, lty = 3) # Note that h here is a horizontal line at sample = 0.5\nabline(v=50, col = \"black\", lwd = 1, lty = 4)  # v here is a vertical line at index = 50\n```\n\n\n\n\n\n\n\n\n\n\n\nLine graphs\nNow, we move to generating a line graph. We will generate a new sample, this time from a \\(\\text{Normal}(0,1)\\) distribution.\n\n\nCode\n```{r}\n###| Generate sample\nsample &lt;- rnorm(n = 30, mean = 0, sd = 1)\n\n###| Line graph\nplot(sample, type = \"l\")\n```\n\n\n\n\n\n\n\n\n\nSuppose we also want to show the points. We change the type from “l” to “b”.\n\n\nCode\n```{r}\n###| Line graph with points\nplot(sample, type = \"b\")\n```\n\n\n\n\n\n\n\n\n\nAgain, we can play with the aesthetics of these graphs.\n\n\nCode\n```{r}\n###| Line graph aesthetics\nplot(sample, type = \"l\", lwd = 2, lty = 3)\nplot(sample, type = \"b\", lwd = 1, lty = 5, pch = 17, col = 3)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose we have another sample generated from a \\(\\text{Normal}(0,2)\\). We can show both lines on the same graph.\n\n\nCode\n```{r}\n###| For reproducibility\nset.seed(68)\n\n###| Generate 2 samples\nsample1 &lt;- rnorm(n=30, mean = 0, sd = 1)\nsample2 &lt;- rnorm(n=30, mean = 0, sd = 2)\n\n###| Line plot with multiple lines\nplot(sample1, type = \"l\")\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe have something, but it appears the top is cut off the graph (this is beacuse we are adding sample 2 onto sample 1 and sample 2 has a larger range). We can change the y axis limits.\n\n\nCode\n```{r}\n###| Line plot with multiple lines with limits\nplot(sample1, type = \"l\", ylim = c(-4,4))\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe should also change the colours for readability.\n\n\nCode\n```{r}\n###| Line plot with multiple lines with limits\nplot(sample1, type = \"l\", ylim = c(-4,4), col = \"blue\")\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe can have different types of lines in the same graph as well.\n\n\nCode\n```{r}\n###| Line plot with multiple line types\nplot(sample1, type = \"l\", ylim = c(-4,4), col = \"blue\")\nlines(sample2, lty = 4, type = \"b\")\n```\n\n\n\n\n\n\n\n\n\n\n\nHistograms\nSuppose instead we want to look at the distribution of the sample, and test if it aligns with the distribution we expect. To do this, we will up the number of samples (200) and will look at the histogram graph.\n\n\nCode\n```{r}\n###| Sample\nsample &lt;- rnorm(n=200, mean = 0, sd =1)\n\n###| Histogram\nhist(sample)\n```\n\n\n\n\n\n\n\n\n\nIf we want to have the probability on the y axis instead, we need to change the freq argument to FALSE or we change the prob argument to TRUE.\n\n\nCode\n```{r}\n###| Density graph\nhist(sample, freq = FALSE)\nhist(sample, prob = TRUE)\n```\n\n\n\n\n\n\n\n\n\nThe widths of the boxes are quite large. We can change this with the breaks argument.\n\n\nCode\n```{r}\n###| Set number of breaks\nhist(sample, breaks = 10) # 10 boxes\nhist(sample, breaks = 20) # 20 boxes\n\n###| Different size boxes\nhist(sample, breaks = c(min(sample),-1.5,0,0.5,1,max(sample))) \n      # need min and max to ensure range works\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also change the labels and add a title.\n\n\nCode\n```{r}\n###| Labels and title\nhist(sample, breaks = 20, xlab = \"Sample\", main = \"Title\")  \n```\n\n\n\n\n\n\n\n\n\nNow, lets compare this to the normal distribution. First, we can add the density curve of the sample.\n\n\nCode\n```{r}\nsample &lt;- rnorm(n = 1000, mean = 0, sd = 1) # Increasing sample count\n\n###| Density curve\nhist(sample, prob = TRUE)\nlines(density(sample))\n```\n\n\n\n\n\n\n\n\n\nWe can then add the normal distribution curve on top to allow comparison.\n\n\nCode\n```{r}\n###| Normal\nseq &lt;- seq(from = -3, to = 3, length = 1000)\nnorm_seq &lt;- dnorm(seq(from = -3, to = 3, length = 1000), mean = 0, sd = 1)\n\n###| Density curve\nhist(sample, prob = TRUE, ylim = c(0,max(norm_seq)))\nlines(density(sample))\nlines(seq,norm_seq, col = \"red\",lwd = 2)\n```\n\n\n\n\n\n\n\n\n\nThe sample distribution is very similar to the normal distribution curve (although the sample has 2 peaks rather than 1). However, it is confusing to have 2 lines without any labels. So, let us add a legend. Additional details appear here.\n\n\nCode\n```{r}\n###| Normal\nseq &lt;- seq(from = -3, to = 3, length = 1000)\nnorm_seq &lt;- dnorm(seq(from = -3, to = 3, length = 1000), mean = 0, sd = 1)\n\n###| Density curve\nhist(sample, prob = TRUE, ylim = c(0,max(norm_seq)))\nlines(density(sample))\nlines(seq,norm_seq, col = \"red\",lwd = 2)\nlegend(x = \"topright\", # position\n       legend = c(\"Sample curve\",\"Normal curve\"),\n       col = c(\"black\",\"red\"),\n       lwd = c(1,2))\n```\n\n\n\n\n\n\n\n\n\n\n\nBox plots\nFinally, we will consider box plots. We can plot the box plot of a single sample.\n\n\nCode\n```{r}\n###| Generating a sample\nsample &lt;- rnorm(n = 100, mean = 5, sd = 2)\n\n###| Boxplot\nboxplot(sample)\n```\n\n\n\n\n\n\n\n\n\nFor multiple boxplots, we can construct a data frame and plot accordingly.\n\n\nCode\n```{r}\n###| Data frame\ndf &lt;- data.frame(\"A\" = rnorm(n = 10, mean = 0, sd = 1),\n                 \"B\" = rnorm(n = 10, mean = 0, sd = 1),\n                 \"C\" = rnorm(n = 10, mean = 0, sd = 1))\n\n###| Box plot\nboxplot(df)\n```\n\n\n\n\n\n\n\n\n\nWe can add labels and titles.\n\n\nCode\n```{r}\n###| Box plot with labels\nboxplot(df, main = \"Title\", ylab = \"Sample values\", xlab = \"Categories\")\n```\n\n\n\n\n\n\n\n\n\n\n\nCheck your understanding\nGenerate scatterplots, line graphs, histograms and box plots with samples from Poisson and Binomial distributions. Add legends and appropriate titles to each graph.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Figures"
    ]
  },
  {
    "objectID": "Introduction to R/figures.html#scatterplots",
    "href": "Introduction to R/figures.html#scatterplots",
    "title": "Figures",
    "section": "Scatterplots",
    "text": "Scatterplots\nWe will start by generating a sample from a \\(\\text{Uniform}[0,1]\\) distribution and plotting a scatter plot.\n\n\nCode\n```{r}\n##| Generating a sample\nsample &lt;- runif(n = 100, min = 0, max = 1)\n\n###| Scatterplot\nplot(sample)\n```\n\n\n\n\n\n\n\n\n\nIf we sort the sample by size, we can create a plot that increases from left to right.\n\n\nCode\n```{r}\n###| Ordered sample scatter plot\nplot(sort(sample))\n```\n\n\n\n\n\n\n\n\n\nThe x and y labs are not clear. We should change these by adding in the xlab and ylab arguments. We should also add a title for clarity!\n\n\nCode\n```{r}\n###| x and y labels and title (called main) \nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\")\n```\n\n\n\n\n\n\n\n\n\nWe observe a linear relationship between the ordered samples. Let us test this by adding a line to this graph using ‘abline()’.\n\n\nCode\n```{r}\n###| Using abline\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\")\nabline(a = 0,b=0.01)\n```\n\n\n\n\n\n\n\n\n\nIt is quite hard to see both the points and the line at the same time. We can change the aesthetics (colour, shape and size) to make this easier to see. Additional point shapes are available here.\n\n\nCode\n```{r}\n###| Aesthetics - colour and points\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\", \n     col = \"red\", pch = 3)        # we start a new line for readability :)\nabline(a = 0,b=0.01, col = \"blue\")\n```\n\n\n\n\n\n\n\n\n\nSuppose we wanted to add additional lines, at the Sample = 0.5 and at the 50th index. We can add these using ‘abline()’. We can also distinguish them by changing colour, line thickness and line type. Additional details are available here.\n\n\nCode\n```{r}\n###| Aesthetics - line\nplot(sort(sample), xlab = \"Ordered index\", ylab = \"Sample\", main = \"Title\", \n     col = \"red\", pch = 3)\nabline(a = 0,b=0.01, col = \"blue\") \nabline(h=0.5, col = \"black\", lwd = 1, lty = 3) # Note that h here is a horizontal line at sample = 0.5\nabline(v=50, col = \"black\", lwd = 1, lty = 4)  # v here is a vertical line at index = 50\n```",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Figures"
    ]
  },
  {
    "objectID": "Introduction to R/figures.html#line-graphs",
    "href": "Introduction to R/figures.html#line-graphs",
    "title": "Figures",
    "section": "Line graphs",
    "text": "Line graphs\nNow, we move to generating a line graph. We will generate a new sample, this time from a \\(\\text{Normal}(0,1)\\) distribution.\n\n\nCode\n```{r}\n###| Generate sample\nsample &lt;- rnorm(n = 30, mean = 0, sd = 1)\n\n###| Line graph\nplot(sample, type = \"l\")\n```\n\n\n\n\n\n\n\n\n\nSuppose we also want to show the points. We change the type from “l” to “b”.\n\n\nCode\n```{r}\n###| Line graph with points\nplot(sample, type = \"b\")\n```\n\n\n\n\n\n\n\n\n\nAgain, we can play with the aesthetics of these graphs.\n\n\nCode\n```{r}\n###| Line graph aesthetics\nplot(sample, type = \"l\", lwd = 2, lty = 3)\nplot(sample, type = \"b\", lwd = 1, lty = 5, pch = 17, col = 3)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSuppose we have another sample generated from a \\(\\text{Normal}(0,2)\\). We can show both lines on the same graph.\n\n\nCode\n```{r}\n###| For reproducibility\nset.seed(68)\n\n###| Generate 2 samples\nsample1 &lt;- rnorm(n=30, mean = 0, sd = 1)\nsample2 &lt;- rnorm(n=30, mean = 0, sd = 2)\n\n###| Line plot with multiple lines\nplot(sample1, type = \"l\")\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe have something, but it appears the top is cut off the graph (this is beacuse we are adding sample 2 onto sample 1 and sample 2 has a larger range). We can change the y axis limits.\n\n\nCode\n```{r}\n###| Line plot with multiple lines with limits\nplot(sample1, type = \"l\", ylim = c(-4,4))\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe should also change the colours for readability.\n\n\nCode\n```{r}\n###| Line plot with multiple lines with limits\nplot(sample1, type = \"l\", ylim = c(-4,4), col = \"blue\")\nlines(sample2)\n```\n\n\n\n\n\n\n\n\n\nWe can have different types of lines in the same graph as well.\n\n\nCode\n```{r}\n###| Line plot with multiple line types\nplot(sample1, type = \"l\", ylim = c(-4,4), col = \"blue\")\nlines(sample2, lty = 4, type = \"b\")\n```",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Figures"
    ]
  },
  {
    "objectID": "Introduction to R/figures.html#histograms",
    "href": "Introduction to R/figures.html#histograms",
    "title": "Figures",
    "section": "Histograms",
    "text": "Histograms\nSuppose instead we want to look at the distribution of the sample, and test if it aligns with the distribution we expect. To do this, we will up the number of samples (200) and will look at the histogram graph.\n\n\nCode\n```{r}\n###| Sample\nsample &lt;- rnorm(n=200, mean = 0, sd =1)\n\n###| Histogram\nhist(sample)\n```\n\n\n\n\n\n\n\n\n\nIf we want to have the probability on the y axis instead, we need to change the freq argument to FALSE or we change the prob argument to TRUE.\n\n\nCode\n```{r}\n###| Density graph\nhist(sample, freq = FALSE)\nhist(sample, prob = TRUE)\n```\n\n\n\n\n\n\n\n\n\nThe widths of the boxes are quite large. We can change this with the breaks argument.\n\n\nCode\n```{r}\n###| Set number of breaks\nhist(sample, breaks = 10) # 10 boxes\nhist(sample, breaks = 20) # 20 boxes\n\n###| Different size boxes\nhist(sample, breaks = c(min(sample),-1.5,0,0.5,1,max(sample))) \n      # need min and max to ensure range works\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nWe can also change the labels and add a title.\n\n\nCode\n```{r}\n###| Labels and title\nhist(sample, breaks = 20, xlab = \"Sample\", main = \"Title\")  \n```\n\n\n\n\n\n\n\n\n\nNow, lets compare this to the normal distribution. First, we can add the density curve of the sample.\n\n\nCode\n```{r}\nsample &lt;- rnorm(n = 1000, mean = 0, sd = 1) # Increasing sample count\n\n###| Density curve\nhist(sample, prob = TRUE)\nlines(density(sample))\n```\n\n\n\n\n\n\n\n\n\nWe can then add the normal distribution curve on top to allow comparison.\n\n\nCode\n```{r}\n###| Normal\nseq &lt;- seq(from = -3, to = 3, length = 1000)\nnorm_seq &lt;- dnorm(seq(from = -3, to = 3, length = 1000), mean = 0, sd = 1)\n\n###| Density curve\nhist(sample, prob = TRUE, ylim = c(0,max(norm_seq)))\nlines(density(sample))\nlines(seq,norm_seq, col = \"red\",lwd = 2)\n```\n\n\n\n\n\n\n\n\n\nThe sample distribution is very similar to the normal distribution curve (although the sample has 2 peaks rather than 1). However, it is confusing to have 2 lines without any labels. So, let us add a legend. Additional details appear here.\n\n\nCode\n```{r}\n###| Normal\nseq &lt;- seq(from = -3, to = 3, length = 1000)\nnorm_seq &lt;- dnorm(seq(from = -3, to = 3, length = 1000), mean = 0, sd = 1)\n\n###| Density curve\nhist(sample, prob = TRUE, ylim = c(0,max(norm_seq)))\nlines(density(sample))\nlines(seq,norm_seq, col = \"red\",lwd = 2)\nlegend(x = \"topright\", # position\n       legend = c(\"Sample curve\",\"Normal curve\"),\n       col = c(\"black\",\"red\"),\n       lwd = c(1,2))\n```",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Figures"
    ]
  },
  {
    "objectID": "Introduction to R/figures.html#box-plots",
    "href": "Introduction to R/figures.html#box-plots",
    "title": "Figures",
    "section": "Box plots",
    "text": "Box plots\nFinally, we will consider box plots. We can plot the box plot of a single sample.\n\n\nCode\n```{r}\n###| Generating a sample\nsample &lt;- rnorm(n = 100, mean = 5, sd = 2)\n\n###| Boxplot\nboxplot(sample)\n```\n\n\n\n\n\n\n\n\n\nFor multiple boxplots, we can construct a data frame and plot accordingly.\n\n\nCode\n```{r}\n###| Data frame\ndf &lt;- data.frame(\"A\" = rnorm(n = 10, mean = 0, sd = 1),\n                 \"B\" = rnorm(n = 10, mean = 0, sd = 1),\n                 \"C\" = rnorm(n = 10, mean = 0, sd = 1))\n\n###| Box plot\nboxplot(df)\n```\n\n\n\n\n\n\n\n\n\nWe can add labels and titles.\n\n\nCode\n```{r}\n###| Box plot with labels\nboxplot(df, main = \"Title\", ylab = \"Sample values\", xlab = \"Categories\")\n```",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Figures"
    ]
  },
  {
    "objectID": "Introduction to R/figures.html#check-your-understanding",
    "href": "Introduction to R/figures.html#check-your-understanding",
    "title": "Figures",
    "section": "Check your understanding",
    "text": "Check your understanding\nGenerate scatterplots, line graphs, histograms and box plots with samples from Poisson and Binomial distributions. Add legends and appropriate titles to each graph.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Figures"
    ]
  },
  {
    "objectID": "Introduction to R/data types.html",
    "href": "Introduction to R/data types.html",
    "title": "Data types",
    "section": "",
    "text": "Before we start coding in R, we need to be familiar with how R stores data - i.e. we need to review the data types in R. The following table states the 6 data types and what they store.\n\nData types\n\n\n\n\n\n\nData type name\nStores\n\n\n\n\nLogical or Boolean\nTRUE or FALSE\n\n\nNumeric\na real number, including decimal values\n\n\nInteger\nan integer number (i.e. no decimal values)\n\n\nComplex\nan imaginary value\n\n\nCharacter\ncharacter or string values, for example a letter ‘A’, typically surrounded by '' or \"\"\n\n\nRaw\nraw bytes (0 or 1s)\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nNumeric and integer are different data types! When inputting data, any integer value will be stored as a numeric data type unless we add an L to the end. For example, 5 will be stored as a numeric data type but 5L will be stored as a integer.\n\n\n\nWhat data type is this variable?\nWe can check what data type a variable is using class(). Alternatively, we can confirm if a variable is an integer, logical or a character using is.integer(), is.logical() and is.character() respectively.\n\n\nCode\n```{r}\n###| Character\ncharacter &lt;- \"a banana\"\n\nclass(character)\nis.character(character)\n\n###| Numeric v integer\nnumber &lt;- 5\ninteger &lt;- as.integer(5)\n\nclass(number)\nis.integer(number)\n\nclass(integer)\nis.integer(integer)\n\n###| Decimal value\ndecimal &lt;- 0.5\n\nclass(decimal)\nis.integer(decimal)\n\n###| Logical value\nis.logical(TRUE)\nis.logical(FALSE)\n```\n\n\n[1] \"character\"\n[1] TRUE\n[1] \"numeric\"\n[1] FALSE\n[1] \"integer\"\n[1] TRUE\n[1] \"numeric\"\n[1] FALSE\n[1] TRUE\n[1] TRUE\n\n\n\n\nCoercion (changing data type)\nWe can force a variable to switch data type using as.numeric(), as.logical(), as.character() and as.integer().\n\n\nCode\n```{r}\n###| Coercing a boolean variable\nas.numeric(TRUE)\nas.numeric(FALSE)\nas.character(TRUE)\nas.character(FALSE)\n```\n\n\n[1] 1\n[1] 0\n[1] \"TRUE\"\n[1] \"FALSE\"\n\n\n\n\nCode\n```{r}\n###| Coercing a character variable\nas.numeric(\"a\") # this will produce an error!\n```\n\n\nWarning: NAs introduced by coercion\n\n\n[1] NA\n\n\n\n\nCode\n```{r}\n###| Coercing a character variable\nas.character(\"a\")\n\n###| Coercing a numeric variable\nas.numeric(2) \nas.character(2)\n\n###| Coercing to a logic variable\nas.logical(1)\nas.logical(0)\n\nas.logical(\"TRUE\")\nas.logical(\"FALSE\")\n```\n\n\n[1] \"a\"\n[1] 2\n[1] \"2\"\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] FALSE\n\n\n\n\nCheck your understanding\nIdentify the following variable’s data type with and without using R:\n\n\na = 2025\nb = “3.14”\nc = pi\nd = TRUE\ne = c(1,0)\nf = 2022L\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n\n###| Answers\nclass(2025)\nclass(\"3.14\")\nclass(pi)\nclass(TRUE)\nclass(c(1,0))\nclass(2022L)\n```\n\n\n[1] \"numeric\"\n[1] \"character\"\n[1] \"numeric\"\n[1] \"logical\"\n[1] \"numeric\"\n[1] \"integer\"",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Data types"
    ]
  },
  {
    "objectID": "Introduction to R/control structures.html",
    "href": "Introduction to R/control structures.html",
    "title": "Control structures",
    "section": "",
    "text": "Control structures allow us to influence the order in which lines of code are executed. It allows for repetition and conditional arguments and more.\nThe main 2 are:\n\n\nConditional:\n\nif()\nif() else()\n\nRepeat:\n\nfor()\nwhile()\n\n\n\nThese are not always efficient as they can take a relatively long processing time compared to other code. However, for now, do not worry about computation time.\n\nIf statements\nThe code of code after an if statement is only executed if the condition within the if statement is met.\n\n\n\n\n\nflowchart LR\n  A{statement A} --&gt; B[Do this if true.]\n  A --&gt; C[Do nothing.]\n\n\n\n\n\n\nFor example, we can check ‘if 5 \\(&lt;\\) 7’ and ask the computer to output “Hello world.” if it is true.\n\n\nCode\n```{r}\n###| Example of if()\nif (5&lt;7) {\n  print(\"Hello world.\")\n}\n```\n\n\n[1] \"Hello world.\"\n\n\nMoreover, we can add an additional condition. If ‘if 5 \\(&lt;\\) 7’ is false, we can specify what we would like the computer to do instead using an else() command. (In this case, we output “It.”. )\n\n\nCode\n```{r}\n###| Example of if() and else()\nif (5 &gt; 7) {\n  print(\"Not it.\")\n} else {\n    print(\"It.\")\n}\n```\n\n\n[1] \"It.\"\n\n\nNow, our flow diagram looks like this.\n\n\n\n\n\nflowchart LR\n  A{statement A} --&gt; B[Do X if true.]\n  A --&gt; C[Do Y if false]\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWe can nest if statements inside of each other.\n\n\nOne such example is the following function that checks if a number is a multiple of \\(2\\), \\(3\\), \\(5\\) or \\(7\\).\n\n\nCode\n```{r}\n###| Nested if statement example\n\n# Function to check divisibility by 2, 3, 5, or 7\ncheck_divisibility &lt;- function(num) {\n  # Ensure the input is numeric\n  if (!is.numeric(num)) {\n    return(\"Please provide a numeric input.\")\n  }\n  \n  # Check divisibility by 2\n  if (num %% 2 == 0) {\n    return(\"The number is divisible by 2.\")\n  } else if (num %% 3 == 0) { # Check divisibility by 3\n    return(\"The number is divisible by 3.\")\n  } else if (num %% 5 == 0) { # Check divisibility by 5\n    return(\"The number is divisible by 5.\")\n  } else if (num %% 7 == 0) { # Check divisibility by 7\n    return(\"The number is divisible by 7.\")\n  } else { # If none of the above conditions are true\n    return(\"The number is not divisible by 2, 3, 5, or 7.\")\n  }\n}\n\n# Example usage:\ncheck_divisibility(14) # Output: \"The number is divisible by 2.\"\ncheck_divisibility(21) # Output: \"The number is divisible by 3.\"\ncheck_divisibility(25) # Output: \"The number is divisible by 5.\"\ncheck_divisibility(49) # Output: \"The number is divisible by 7.\"\ncheck_divisibility(11) # Output: \"The number is not divisible by 2, 3, 5, or 7.\"\n```\n\n\n[1] \"The number is divisible by 2.\"\n[1] \"The number is divisible by 3.\"\n[1] \"The number is divisible by 5.\"\n[1] \"The number is divisible by 7.\"\n[1] \"The number is not divisible by 2, 3, 5, or 7.\"\n\n\nThe following illustrates this as a flow diagram:\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B{Is input numeric?}\n    B -- No --&gt; C[\"Please provide a numeric input.\"]\n    B -- Yes --&gt; D{Is the number divisible by 2?}\n    D -- Yes --&gt; E[\"The number is divisible by 2.\"]\n    D -- No --&gt; F{Is the number divisible by 3?}\n    F -- Yes --&gt; G[\"The number is divisible by 3.\"]\n    F -- No --&gt; H{Is the number divisible by 5?}\n    H -- Yes --&gt; I[\"The number is divisible by 5.\"]\n    H -- No --&gt; J{Is the number divisible by 7?}\n    J -- Yes --&gt; K[\"The number is divisible by 7.\"]\n    J -- No --&gt; L[\"The number is not divisible by 2, 3, 5, or 7.\"]\n\n\n\n\n\n\nIn this instance, we can stop when we find the smallest number that is a factor of the input. However, we may want to check every case.\n\n\nFor loops\nThe for loop allows code to be executed repeatedly until a given iteration.\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B[Initialize loop variable]\n    B --&gt; C{Condition met?}\n    C -- No --&gt; D[Exit loop]\n    C -- Yes --&gt; E[Execute loop body]\n    E --&gt; F[Update loop variable]\n    F --&gt; C\n    D --&gt; G[End]\n\n\n\n\n\n\nSuppose we want to print the numbers 1, 2, 3 and 4. We can use a for loop for this:\n\n\nCode\n```{r}\n###| For loop example\nfor (x in c(1:4)) {\n  print(x)\n}\n```\n\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n\n\n\n\n\n\n\n\nImportant\n\n\n\nR will automatically increase the counter. We do not need to include x&lt;-x+1 as we do in Python.\n\n\nAlternatively, suppose we wanted to print the number 1, then the numbers 1 to 3, then the number 2, then 1 to 3 again, then 3 … . We can use nested for loops for this:\n\n\nCode\n```{r}\n###| Nested loop\nfor (x in c(1:4)) {\n  print(x)\n  for (y in c(1:3)) {\n    print(y)\n  }\n}\n```\n\n\n[1] 1\n[1] 1\n[1] 2\n[1] 3\n[1] 2\n[1] 1\n[1] 2\n[1] 3\n[1] 3\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 1\n[1] 2\n[1] 3\n\n\n\n\nWhile loops\nWhile loop tests if a condition is true and will continue running the code until the condition is false.\n\n\n\n\n\ngraph TD\n    A[Start] --&gt; B{Condition met?}\n    B -- No --&gt; C[Exit loop]\n    B -- Yes --&gt; D[Execute loop body]\n    D --&gt; B\n    C --&gt; E[End]\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is very important to avoid infinite loops!!\n\n\n\n\nCode\n```{r}\n###| While loop example\ni &lt;- 1\nwhile (i &lt;= 5){\n  print(i)\n  i &lt;- i + 1\n}\n```\n\n\n[1] 1\n[1] 2\n[1] 3\n[1] 4\n[1] 5\n\n\n\n\nExample combining if, for and while loops\n\n\nCode\n```{r}\n# Initialize variables\nx &lt;- 1    # Start value for while loop\nresults &lt;- c()  # Empty vector to store results\n\n# While loop: Run while x is less than or equal to 10\nwhile (x &lt;= 10) {\n  # For loop: Iterate through numbers 1 to 5\n  for (y in 1:5) {\n    # If statement: Check if the product of x and y is even\n    if ((x * y) %% 2 == 0) {\n      # Add the product to the results vector if even\n      results &lt;- c(results, x * y)\n    }\n  }\n  # Increment x to avoid infinite loop\n  x &lt;- x + 1\n}\n\n# Print the results\nprint(\"Results of even products:\")\nprint(results)\n```\n\n\n[1] \"Results of even products:\"\n [1]  2  4  2  4  6  8 10  6 12  4  8 12 16 20 10 20  6 12 18 24 30 14 28  8 16\n[26] 24 32 40 18 36 10 20 30 40 50\n\n\n\n\nCheck your understanding\nAnswer the following questions using if(), for() and while() loops.\n\n\nGiven a number, print “Positive” or “Negative” depending on if the number is positive or negative. If the number is 0, print both.\nGiven a vector, find the maximum and minimum value. Thus find out the range.\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answers\nx=-1\nif (x &gt; 0) {\n  print(\"positive\")\n} else if (x &lt; 0) {\n  print(\"negative\")\n} else {print(\"both\")}\n```\n\n\n[1] \"negative\"\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answers\nx &lt;- c(0,1,2,7,-5)\nn &lt;- length(x)\nmin &lt;- x[1]\nmax &lt;- x[1]\n\nfor(i in 1:n){\n  if (x[i] &lt; min) {\n    min &lt;- x[i]\n  }\n  if (x[i] &gt; max) {\n    max &lt;- x[i]\n  }\n}\n\nprint(paste(\"Range is\",max-min))\n```\n\n\n[1] \"Range is 12\"",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Control structures"
    ]
  },
  {
    "objectID": "Introduction to R/operators.html",
    "href": "Introduction to R/operators.html",
    "title": "Operations",
    "section": "",
    "text": "We will begin by familiarising ourselves with the operations in R, where\n\n“an operator is a character, or characters, that determine what action is to be performed or considered”,\n\naccording to the BBC Bitesize website. These will form the base building blocks of everything we will do in the future.\nWe will start with arithmetic operators (\\(+\\),\\(-\\),\\(\\times\\),\\(\\div\\)), then go to comparison operators (\\(=\\),\\(\\ne\\),\\(\\geq\\),\\(\\leq\\)) and finally logical operators (and, or).\n\nArithmetic operators\nThe 4 base arithmetic operators are addition (+), subtraction (-), multiplication (*) and division (/). The following are some base examples of how these can be used.\n\n\nCode\n```{r}\n###| Addition\n3+10\n\n###| Subtraction\n6-2\n\n###| Multiplication\n5*15\n\n###| Division\n23/7\n```\n\n\n[1] 13\n[1] 4\n[1] 75\n[1] 3.285714\n\n\n\n\n\n\n\n\nNote\n\n\n\nIt is important to note that R follows the traditional BODMASS order of operations. As such, we can use brackets to control the order in which operations are performed.\n\n\n\n\nCode\n```{r}\n###| BODMASS\n(1+7)/2\n1 + 7/2\n3*5+2/3\n3*(5+2)/3\n```\n\n\n[1] 4\n[1] 4.5\n[1] 15.66667\n[1] 7\n\n\nWe can also do exponents (to the power of) using ^ or **, and we can find the modulo using %% and %/%\n\n\nCode\n```{r}\n###| Exponents \n5^2\n5**2\n\n4^(1/2)\n4**0.5\n```\n\n\n[1] 25\n[1] 25\n[1] 2\n[1] 2\n\n\n\n\nCode\n```{r}\n###| Modulo\n10 %% 2 # remainder of 10 divided by 2\n10 %% 4\n\n10 %/% 2 # how many times 2 goes into 10\n10 %/% 4\n```\n\n\n[1] 0\n[1] 2\n[1] 5\n[1] 2\n\n\n\n\nComparison operators\nComparison operators compare 2 values to one another and will return TRUE or FALSE.\nThe 2 operators that can be used for any data type are equal to == and not equal to !=.\nWe can also compare two numbers using less than &lt;, less than or equal to &lt;=, greater than &gt;, and greater than or equal to &gt;=.\n\n\nCode\n```{r}\n###| Equal to\n\"a banana\" == \"a banana\"\n\"a banana\" == \"a fruit\"\n\n7 == 5\n5 == 5\n\n###| Not wqual to\n\"a banana\" != \"a banana\"\n\"a banana\" != \"a fruit\"\n\n7 != 5\n5 != 5\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] FALSE\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] TRUE\n[1] FALSE\n\n\n\n\nCode\n```{r}\n###| Less than\n5 &lt; 7 \n7 &lt; 5\n5 &lt; 5 # note that this will return FALSE!\n\n###| Greater than\n5 &gt; 7 \n7 &gt; 5\n5 &gt; 5 # note that this will return FALSE!\n\n###| Less than or equal to\n5 &lt;= 7 \n7 &lt;= 5\n5 &lt;= 5 # note that this will return TRUE!\n\n###| Greater than or equal to\n5 &gt;= 7 \n7 &gt;= 5\n5 &gt;= 5 # note that this will return TRUE!\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] FALSE\n[1] FALSE\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] TRUE\n\n\n\n\nLogical operators\nLogical operators allow us to combine things or negate something.\nIf we want to find the negative, we use the ! operator. For example, we may want to say something is not true. we do this with !TRUE. Or we may want to check if 5 is not less than 7, !(5&lt;7).\n\n\nCode\n```{r}\n###| Negation of TRUE FALSE\nTRUE\n!(TRUE)\n\nFALSE\n!(TRUE)\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] FALSE\n[1] FALSE\n\n\n\n\nCode\n```{r}\n###| Example of negation on relational operators\n5&lt;7\n!(5&lt;7)\n\n5==7\n!(5==7)\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] FALSE\n[1] TRUE\n\n\nWe can also combine operators using AND, &. For example, we may want to check that 5&lt;7 and 10&lt;12. Alternatively, we may want to allow A or B to be true. We do this with |.\n\n\nCode\n```{r}\n###| And on TRUE FALSE\nTRUE & TRUE\nFALSE & FALSE\nTRUE & FALSE\n\n###| And on relational operators example\n(7&gt;5) & (10&lt;12)\n(7&lt;5) & (10&gt;12)\n(7&lt;5) & (10&lt;12)\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] FALSE\n[1] TRUE\n[1] FALSE\n[1] FALSE\n\n\n\n\nCode\n```{r}\n###| OR on TRUE FALSE\nTRUE | TRUE\nFALSE | FALSE\nTRUE | FALSE\n\n###| OR on relational operators example\n(7&gt;5) | (10&lt;12)\n(7&lt;5) | (10&gt;12)\n(7&lt;5) | (10&lt;12)\n```\n\n\n[1] TRUE\n[1] FALSE\n[1] TRUE\n[1] TRUE\n[1] FALSE\n[1] TRUE\n\n\n\n\nCheck your understanding\nThe following are some extercises to test your understanding about operations.\n\n\nUse R to determine the area of a circle with diameter of 20cm.\n\nHint: pi is recognised in R.\n\nCalculate the cube root of 14 \\(\\times\\) 0.51.\n\nHint: think carefully about the order of operations.\n\nTest the following statement in R: Not (4 squared is greater than or equal to 15, or (the sum of 7 and 10 is less than 16)).\n\nHint: the output should be FALSE.\n\n\n\n(The code is hidden to give you a chance to practise. However, if you toggle the &gt;, example code will appear.)\n\n\nCode\n```{r}\n#| code-fold: true\n\n###| Answers\n(10^2)*pi\n(14 * 0.51)^(3)\n!( 4&lt;=15 | 7+10 &lt; 16)\n```\n\n\n[1] 314.1593\n[1] 363.9943\n[1] FALSE",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Operations"
    ]
  },
  {
    "objectID": "Introduction to R/functions.html",
    "href": "Introduction to R/functions.html",
    "title": "Custom functions",
    "section": "",
    "text": "A function is a piece of code written to carry out a specific task. It can take in inputs and may return one or more values.\nThe 3 main components are:\n\n\nThe formals(): the list of arguments/inputs of the function\nThe body(): the code in the function\nThe environment(): the map of the location of the function’s variable\n\n\nA function follows the following general structure:\n\n\nfunction_name &lt;- function(arguments) {function body}\n\n\nCustom functions are used to incorporate sets of instructions that you want to use repeatedly, or that are easier to store so they can be called when needed.\nFunctions are created using function() and are stored as R objects. They can also be passed to other functions as arguments (as input) and can be nested (functions can be inside other functions).\n\n\nCode\n```{r}\n###| Function example 1\nadd &lt;- function(x,y){\n  x + y\n}\n\nadd(2,4)\n\n###| Function example 2\nsquare &lt;- function(x){\n  x^2\n}\n\nsquare(7)\n```\n\n\n[1] 6\n[1] 49\n\n\n\nCheck your understanding\nWrite functions that do the following things:\n\n\nCreate a function which returns integer division of x by y and the remainder\nCreate a function which finds the area and circumference of a circle given a radius r.\nCreate a function that calculates the factorial of some number n.\n\nHint: make sure that n is an integer!\n\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\n\nmodulus &lt;- function(x,y) {\n  remainder &lt;- x %% y\n  multiple &lt;- x %/% y\n  print(paste(\"x modulo y is\", multiple, \"with remainder\", remainder))\n}\n\nmodulus(6,3)\n```\n\n\n[1] \"x modulo y is 2 with remainder 0\"\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\n\ncircle &lt;- function(r){\n  area &lt;- r^2 * pi\n  circumference &lt;- 2*r*pi\n  print(paste(\"Area is\",area, \"and the circumference is\",circumference))\n}\n\ncircle(5)\n```\n\n\n[1] \"Area is 78.5398163397448 and the circumference is 31.4159265358979\"\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n###| Answer\n\nfactorial &lt;- function(x) {\n  \n  if (x%%1 == 0) {\n    \n    factorial &lt;- 1\n    \n    while (x &gt;= 1) {\n      factorial &lt;- factorial * x\n      x &lt;- x - 1\n    }\n    \n    print(factorial)\n  } else {\n    print(paste(x,\" is not a positive integer\"))\n  }\n}\n\nfactorial(2) \n```\n\n\n[1] 2",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Custom functions"
    ]
  },
  {
    "objectID": "Introduction to R/variables.html",
    "href": "Introduction to R/variables.html",
    "title": "Variables",
    "section": "",
    "text": "Thus far, we have covered some very important content. However, it is not in a practical, useable form. Instead of having to rewrite values each time, we want to be able to have the computer remember them (store them) so that we can recall them. We do this using variables.\nWe can assign a value to a named variable using = or -\\&gt;. (Note that we use = for assigning variables and == for check if equal!) We can check what our variable is by outputting it.\n\n\nCode\n```{r}\n###| Assigning a variable\nx &lt;- 5\ny = TRUE\n\n###| Outputting a variable\nx\ny\n```\n\n\n[1] 5\n[1] TRUE\n\n\nWe can use all our previous operators on variables. The following are just some examples of how we can do this.\n\n\nCode\n```{r}\n###| Assigning variables\na = 1\nb = 2\nc = 3\nd = 4\ne = 5\n\n###| Arithmetic operators\nd %% b\nc + d\na * e - d\n\n###| Relational operators\nd &lt; b\nc &gt; d\nc == d\n\n###| Logical operators \n(d &lt; b) & (c &gt; d)\n(d &lt; b) | (c &gt; d)\n!(d &lt; b)\n```\n\n\n[1] 0\n[1] 7\n[1] 1\n[1] FALSE\n[1] FALSE\n[1] FALSE\n[1] FALSE\n[1] FALSE\n[1] TRUE\n\n\n\nCheck your understanding\nThe following are some extercises to test your understanding about variables.\n\n\nDefine variables called velocity, time and acceleration with 5, 10 and 9.8 respectively. Calculate the displacement.\n\nHint: \\(s = ut+0.5at^2\\).\n\nDefine variables height and width of a rectangle with initial values 5 and 10. Calculate the area of the rectangle.\nExtension: given \\(f(x)=\\frac{1}{\\sqrt{2\\pi}}\\exp{(-\\frac{1}{2}x^2)}\\), find \\(f(5)\\).\n\nHint: look at sqrt() and exp().\n\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n\n###| Answers\nvecolity &lt;- 5\ntime &lt;- 10\nacceleration &lt;- 9.8\ndisplacement &lt;- vecolity * time + 0.5 * acceleration * time^2\ndisplacement\n\nheight &lt;- 5\nwidth &lt;- 10\narea &lt;- height * width\narea\n\n###| Extension question\nx &lt;- 5\nf_x &lt;- 1/sqrt(2*pi)*exp(-0.5*x^2)\nf_x\n```\n\n\n[1] 540\n[1] 50\n[1] 1.48672e-06",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Variables"
    ]
  },
  {
    "objectID": "Introduction to R/data structures.html",
    "href": "Introduction to R/data structures.html",
    "title": "Data structures",
    "section": "",
    "text": "Data structures are the ways in which data is stored. The following table indicates the main 5 ways.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Data structures"
    ]
  },
  {
    "objectID": "Introduction to R/data structures.html#vectors-arithmetic",
    "href": "Introduction to R/data structures.html#vectors-arithmetic",
    "title": "Data structures",
    "section": "Vectors: Arithmetic",
    "text": "Vectors: Arithmetic\nWe can apply arithmetic to vectors. We can multiple, add and subtract vectors.\n\n\nCode\n```{r}\n###| Vector arithmetic\nx &lt;- c(1,2,3)\n\nx + 3\nx * 3\nx * x\n```\n\n\n[1] 4 5 6\n[1] 3 6 9\n[1] 1 4 9",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Data structures"
    ]
  },
  {
    "objectID": "Introduction to R/data structures.html#vectors-sequences-and-repetition",
    "href": "Introduction to R/data structures.html#vectors-sequences-and-repetition",
    "title": "Data structures",
    "section": "Vectors: Sequences and repetition",
    "text": "Vectors: Sequences and repetition\nWe can generate vectors using seq() and rep() to generate a sequence or repeat a value.\n\n\nCode\n```{r}\n###| Sequence 1\n\n# by will create a vector where each subsequent value is 0.1 larger than the previous\nseq(from = 0, to = 1, by = 0.1) \n\n# length.out will ensure 4 values are stored in the vector\nseq(from = 0, to = 1, length.out = 4) \n```\n\n\n [1] 0.0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 0.9 1.0\n[1] 0.0000000 0.3333333 0.6666667 1.0000000\n\n\n\n\nCode\n```{r}\n###| Sequence 2\n\nseq(from = 0, to = 10, by = 2.7) # note that 10 is not in this vector!\n\nseq(from = 0, to = 10, length.out = 4) \n```\n\n\n[1] 0.0 2.7 5.4 8.1\n[1]  0.000000  3.333333  6.666667 10.000000\n\n\n\n\nCode\n```{r}\n###| Repeat\nrep(1,6)\n\nrep(1:3,each = 2) # will repeat each value twice\n\nrep(1:3,2) # will repeat the entire 1:3 twice\n\nrep(1:3,length.out=6) # will repeat 1:3 until it reaches the length.out\n\nrep(1:3,length.out=5) # note that this is only 5 long and so only has one 3!\n```\n\n\n[1] 1 1 1 1 1 1\n[1] 1 1 2 2 3 3\n[1] 1 2 3 1 2 3\n[1] 1 2 3 1 2 3\n[1] 1 2 3 1 2",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Data structures"
    ]
  },
  {
    "objectID": "Introduction to R/data structures.html#vectors-indexing",
    "href": "Introduction to R/data structures.html#vectors-indexing",
    "title": "Data structures",
    "section": "Vectors: Indexing",
    "text": "Vectors: Indexing\nYou can extract data from a vector if you know it’s index (for example, you can extract the 5th value of a vector) using [].\nWe can extract 1 element at a time (see the first example) or we can extract multiple elements at once using vectors (see the second and third examples). We can also extract using a variable (fourth example).\n\n\nCode\n```{r}\n###| Indexing\nx &lt;- c(1,2,3,4,5)\n\nx[3] # extracts the 3rd element\nx[1:3] # extracts elements 1, 2 and 3\nx[c(1,3,5)] # extracts elements 1, 3 and 5\n\n###| Indexing with a named vector\ny &lt;- c(2,4)\nx[y] # extracts elements 2 and 4\n```\n\n\n[1] 3\n[1] 1 2 3\n[1] 1 3 5\n[1] 2 4\n\n\nYou can change the values of a vector using indexes as well.\n\n\nCode\n```{r}\n###| Changing an element\nx &lt;- c(1,2,3,4,5)\n\nx[1] &lt;- 7\nx\n```\n\n\n[1] 7 2 3 4 5\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe number of replacing values must match the number of replaced values, otherwise there is an error (see next example)!\n\n\n\n\nCode\n```{r}\n###| Mismatch in replacement length\nx[2] &lt;- c(1,3)\n```\n\n\nWarning in x[2] &lt;- c(1, 3): number of items to replace is not a multiple of\nreplacement length\n\n\nCode\n```{r}\nx # only the 1st element is used to replace the 2nd value\n```\n\n\n[1] 7 1 3 4 5\n\n\nYou can also remove an element from a vector using [- ].\n\n\nCode\n```{r}\n###| Changing an element\nx &lt;- c(1,2,3,4,5)\nx[-1] \n```\n\n\n[1] 2 3 4 5",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Data structures"
    ]
  },
  {
    "objectID": "Introduction to R/data structures.html#vectors-check-your-understanding",
    "href": "Introduction to R/data structures.html#vectors-check-your-understanding",
    "title": "Data structures",
    "section": "Vectors: Check your understanding",
    "text": "Vectors: Check your understanding\nTo practise sequences and repetition, do the following exercises.\n\n\nCreate a vector from 0 to 100.\nCreate a vector from 100 to 0.\nCreate a vector containing all strictly positive even numbers up to and including 100.\nCreate a vector containing 1 once, 2 twice and 3 thrice, in that order.\n\n\n\n\nCode\n```{r}\n#| code-fold: true\n\n###| Answers\nseq(from = 0, to = 100)\nseq(from = 100, to = 0)\nseq(from = 0, to = 100, by = 2)\n\nrep(1:3,1:3)\n```\n\n\n  [1]   0   1   2   3   4   5   6   7   8   9  10  11  12  13  14  15  16  17\n [19]  18  19  20  21  22  23  24  25  26  27  28  29  30  31  32  33  34  35\n [37]  36  37  38  39  40  41  42  43  44  45  46  47  48  49  50  51  52  53\n [55]  54  55  56  57  58  59  60  61  62  63  64  65  66  67  68  69  70  71\n [73]  72  73  74  75  76  77  78  79  80  81  82  83  84  85  86  87  88  89\n [91]  90  91  92  93  94  95  96  97  98  99 100\n  [1] 100  99  98  97  96  95  94  93  92  91  90  89  88  87  86  85  84  83\n [19]  82  81  80  79  78  77  76  75  74  73  72  71  70  69  68  67  66  65\n [37]  64  63  62  61  60  59  58  57  56  55  54  53  52  51  50  49  48  47\n [55]  46  45  44  43  42  41  40  39  38  37  36  35  34  33  32  31  30  29\n [73]  28  27  26  25  24  23  22  21  20  19  18  17  16  15  14  13  12  11\n [91]  10   9   8   7   6   5   4   3   2   1   0\n [1]   0   2   4   6   8  10  12  14  16  18  20  22  24  26  28  30  32  34  36\n[20]  38  40  42  44  46  48  50  52  54  56  58  60  62  64  66  68  70  72  74\n[39]  76  78  80  82  84  86  88  90  92  94  96  98 100\n[1] 1 2 2 3 3 3",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Data structures"
    ]
  },
  {
    "objectID": "Introduction to R/introduction to r.html",
    "href": "Introduction to R/introduction to r.html",
    "title": "Introduction",
    "section": "",
    "text": "These pages will cover will introduce some key theory about how R operates and the basic building blocks of coding. It ends with generating plots using inbuilt R commands.\nThis section covers:\n\n\nhow R stores data/values (data types, data structures and variables),\nhow to perform operations (arithmetic, comparison and logical),\nhow to control the structure of your code (if, else, for and while),\nhow to write your own functions,\nrandom numbers, and\nplotting graphs.",
    "crumbs": [
      "Home",
      "Introduction to R",
      "Introduction"
    ]
  },
  {
    "objectID": "cheatsheets.html",
    "href": "cheatsheets.html",
    "title": "Cheatsheets",
    "section": "",
    "text": "There is a lot to learn to create beautiful and informative reports using R. In this section, there are several cheatsheets that will hopefully make this learning curve a little easier.\n(If the files are not loading, there also available on the GitHub under the Cheatsheets folder link.)\n\nLatex\nLatex is a language that allows for additional control over figures, tables and maths. The following is a cheatsheet created by Winston Chang, with the original version available here.\n\n\n\nggplot2\nA very popular package for figures is the ggplot2 package. It can be installed and loaded with the following code (although it is included in the tidyverse package).\n\n\nCode\n```{r}\n###| ggplot2 package\n\n#install.packages(\"ggplot2\")\nlibrary(ggplot2)\n```\n\n\nWarning: package 'ggplot2' was built under R version 4.4.3"
  },
  {
    "objectID": "Modelling/What is a linear model.html",
    "href": "Modelling/What is a linear model.html",
    "title": "What is a linear model?",
    "section": "",
    "text": "Important\n\n\n\nA linear model aims predict or explain the relationship between a single outcome variable and one (or more) explanatory variables. It is the simplest form of a regression model.\n\n\nIn this case, a variable is a quantifiable quantity. The observed data is a realisation of the variable, obtained by conducting tests (observations). Variables can be numerical (continuous or discrete) or categorical.\nThe most basic linear model has a single response and explanatory variable. It is of the form:\n\\[y_i = \\alpha + \\beta x_i + \\epsilon_i,\\]\nwhere \\(y_i\\) is the ith observation of the response variable, \\(x_i\\) is the ith observation of the explanatory variable, \\(\\alpha\\) is the intercept, \\(\\beta\\) is the gradient and \\(\\epsilon_i\\) is the error.\n\n\n\n\n\n\nImportant\n\n\n\nRegardless of whether we are fitting a model to predict or explain, our aim is to estimate the parameters.\n\n\n\nDeterministic with error\nIt is important to note that the relationship between explanatory and outcome variables is not deterministic - that is to say, knowing the values of the explanatory variables does not allow you to predict the outcome variable exactly. We can decompose the relationship into the deterministic/systemic component (\\(X\\beta\\)) and random error (\\(\\epsilon\\)):\n\\[\nY = X \\beta + \\epsilon,\n\\] where \\(Y\\in \\mathbb{R}^{n\\times 1}\\) is the outcome/response variable, \\(X \\in \\mathbb{R}^{n\\times p+1}\\) is the design matrix, \\(\\beta \\in \\mathbb{R}^{p+1 \\times 1}\\) is the parameter vector and \\(\\epsilon \\in \\mathbb{R}^{n\\times 1}\\) is the vector of errors.\nHere, we assume we have \\(n\\) data points and \\(p\\) explanatory variables. We have \\(p+1\\) in the dimensions to account for the intercept term.\n\n\nAssumptions\nTo use a linear model, we need to make 4 key assumptions. We will see in later sections how we can relax some of these assumptions to fit a generalised linear model. However, for now, we must make sure our model meets these assumptions:\n\n\nLinearity - the systemic component is linear in parameters, that is, \\[\\mathbb{E}[Y] = \\mathbb{E}[X\\beta].\\] Note that the explanatory variables do not have to be linear. They can be polynomials, log and more!\nHomoscedasticity - the errors must have constant variance, i.e. \\[Var(\\epsilon_1) = Var(\\epsilon_2) = \\; ... \\; = Var(\\epsilon_{n+1}) = \\sigma^2.\\]\nThe errors are independent of each other.\nThe errors are normally distributed, \\[\\epsilon_i \\sim \\text{Normal}(0,\\sigma^2).\\]\n\n\nThere are several things we should note about the response variable as a consequence of these assumptions.\n\n\nIt is univariate.\nIt is continuous.\nIt is stochastic (i.e. has a random element).\nIt has equal variance.\nResponses from different observations are assumed to be uncorrelated. (We will revisit this for time series later.)\n\n\n\n\nLinearity assumption - can we have \\(x^2\\)?\nIt is very important to be careful with the linearity assumption.\nSuppose we are given a dataset. Let \\(y_i\\) denote the ith observation of the response variable. Let \\(x_{i1}\\) denote the ith observation of the first explanatory variable and \\(x_{i2}\\) denote the ith observation of the second explanatory variable.Let \\(\\beta = (a,b,c)^{T}\\).\nThen, which of the following are linear models - i.e. when does \\(\\mathbb{E}[y_i] = \\mathbb{E}[X_i\\beta]\\)?\n\n\n\\(y_i = b x_{i1} + \\epsilon_i\\)\n\\(y_i = a + b x_{i1} + \\epsilon_i\\)\n\\(y_i = a + b^3 x_{i1} + c x_{i2} + \\epsilon_i\\)\n\\(y_i = a + b (x_{i1}-x_{i2})^2 + \\epsilon_i\\)\n\\(y_i = a + b x_{i1} + c \\log(x_{i2}) + \\epsilon_i\\)\n\n\nIn this case, all are linear models except for 3. This is because the parameters are not linear in point 3 as we have \\(b^3\\)!\n\n\nPrinciple of parsimony - simplest is best\nAfter we have checked that our model meets the 4 key assumptions, we must decide what variables to include in our model. We need to balance accuracy and complexity.\nFor example, a model with loads of explanatory variables may give more accurate predictions. However, the more parameters (explanatory variables) we have in a model, the higher the computational cost to fit a linear model.\nThe principle of parsimony states that “given 2 models that behave similarly, we should pick the simpler model”.\nThe exact boundaries of ‘similarly’ are difficult to quantify. Model creation and selection is an art form, and something that requires practise to learn.\n\n\n\n\n\n\nImportant\n\n\n\nImportant exceptions to the ‘pick the simpler model’ principle are the following:\n\n\nIf \\(x^2\\) appears in the model, \\(x\\) must also appear. Similarly, if \\(x^3\\) appears, \\(x\\) and \\(x^2\\) must also appear.\nIf there is an interaction term between explanatory variables \\(x_1\\) and \\(x_2\\), then \\(x_1\\) and \\(x_2\\) must also appear. (We will revisit this when we cover categorical data.)\n\n\n\n\nMore information about this principle can be found here.",
    "crumbs": [
      "Home",
      "Linear modelling",
      "What is a linear model?"
    ]
  },
  {
    "objectID": "Modelling/What is a linear model.html#what-is-a-linear-model",
    "href": "Modelling/What is a linear model.html#what-is-a-linear-model",
    "title": "What is a linear model?",
    "section": "",
    "text": "A linear model aims predict or explain the relationship between a single outcome variable and one (or more) explanatory variables. It is the simplest form of a regression model.\nIn this case, a variable is a quantifiable quantity. The observed data is a realisation of the variable, obtained by conducting tests (observations). Variables can be numerical (continuous or discrete) or categorical.\nThe most basic linear model has a single response and explanatory variable. It is of the form:\n\\[y_i = \\alpha + \\beta x_i + \\epsilon_i,\\]\nwhere \\(y_i\\) is the ith observation of the response variable, \\(x_i\\) is the ith observation of the explanatory variable, \\(\\alpha\\) is the intercept, \\(\\beta\\) is the gradient and \\(\\epsilon_i\\) is the error.\nRegardless of whether we are fitting a model to predict or explain, our aim is to estimate the parameters.\n\n\nIt is important to note that the relationship between explanatory and outcome variables is not deterministic - that is to say, knowing the values of the explanatory variables does not allow you to predict the outcome variable exactly. We can decompose the relationsihp into the deterministic/systemic component and random error:\n\\[\nY = X \\beta + \\epsilon,\n\\] where \\(Y\\in \\mathbb{R}^{n\\times 1}\\) is the outcome/response variable, \\(X \\in \\mathbb{R}^{n\\times p+1}\\) is the design matrix, \\(\\beta \\in \\mathbb{R}^{p+1 \\times 1}\\) is the parameter vector and \\(\\epsilon \\in \\mathbb{R}^{n\\times 1}\\) is the vector of errors. Here, we assume we have \\(n\\) data points and \\(p\\) explanatory variables. We have \\(p+1\\) in the dimensions to account for the intercept term.\n\n\n\nTo use a linear model, we need to make 4 key assumptions. We will see in later pages how we can relax some of these assumptions to fit a generalised linear model. However, for now, we must make sure our model meets these assumptions:\n\n\nLinearity - the systemic component is linear in parameters, that is, \\[\\mathbb{E}[Y] = \\mathbb{E}[X\\beta].\\] Note that the explanatory variables do not have to be linear. They can be polynomials, log and more!\nHomoscedasticity - the errors must have constant variance, i.e. \\[Var(\\epsilon_1) = Var(\\epsilon_2) = \\; ... \\; = Var(\\epsilon_{n+1}) = \\sigma^2.\\]\nThe errors are independent of each other.\nThe errors are normally distributed, \\[\\epsilon \\sim \\text{Normal}(0,\\sigma^2 I_n).\\]\n\n\nThere are several things we should note about the response variable as a consequence of these assumptions.\n\n\nIt is univariate.\nIt is continuous.\nIt is stochastic (i.e. has a random element).\nIt has equal variance.\nResponses from different observations are assumed to be uncorrelated. (We will revisit this for time series later.)\n\n\n\n\n\nIt is very important to be careful with the linearity assumption.\nSuppose we are given a dataset. Let \\(y_i\\) denote the ith observation of the response variable. Let \\(x_{i1}\\) denote the ith observation of the first explanatory variable and \\(x_{i2}\\) denote the ith observation of the second explanatory variable.Let \\(\\beta = (a,b,c)^{T}\\).\nThen, which of the following are linear models - i.e. when does \\(\\mathbb{E}[y_i] = \\mathbb{E}[X_i\\beta]\\)?\n\n\n\\(y_i = b x_{i1} + \\epsilon_i\\)\n\\(y_i = a + b x_{i1} + \\epsilon_i\\)\n\\(y_i = a + b^3 x_{i1} + c x_{i2} + \\epsilon_i\\)\n\\(y_i = a + b (x_{i1}-x_{i2})^2 + \\epsilon_i\\)\n\\(y_i = a + b x_{i1} + c \\log(x_{i2}) + \\epsilon_i\\)\n\n\nIn this case, all are linear models except for 3. This is because the parameters are not linear in 3 (\\(b^3\\))!\n\n\n\nAfter we have checked that our model meets the 4 key assumptions, we must decide what explanatory variables to include in our model. We need to balance accuracy and complexity.\nFor example, a model with loads of explanatory variables may give more accurate predictions. However, the more parameters (explanatory variables) we have in a model, the higher the computational cost to fit a linear model.\nThe principle of parsimony states that “given 2 models that behave similarly, we should pick the simpler model”.\nThe exact boundaries of ‘similarly’ are difficult to quantify. Model creation and selection is an art form, and something that requires practise to learn.\nImportant exceptions to the ‘pick the simpler model’ principle are the following:\n\n\nIf \\(x^2\\) appears in the model, \\(x\\) must also appear. Similarly, if \\(x^3\\) appears, \\(x\\) and \\(x^2\\) must also appear.\nIf there is an interaction term between explanatory variables \\(x_1\\) and \\(x_2\\), then \\(x_1\\) and \\(x_2\\) must also appear. (We will revisit this when we cover categorical data.)"
  },
  {
    "objectID": "Data/Raw data.html",
    "href": "Data/Raw data.html",
    "title": "Data",
    "section": "",
    "text": "To begin, download the data and save it to a folder in your documents. Then, you can use the following commands to call in the data:"
  },
  {
    "objectID": "Data/Raw data.html#leaf-data",
    "href": "Data/Raw data.html#leaf-data",
    "title": "Data",
    "section": "Leaf data",
    "text": "Leaf data\n\n\nCode\n```{r}\n###| Leaf data \n\nleaf &lt;- data.frame(\"Nitrogen\" = c(3.05,4.22,3.34,3.77,3.52,3.54,3.74, 3.78, 2.92, 3.10, 2.86, 2.78, 2.22, 2.67, 3.12, 3.03, 2.45, 4.12, 4.61, 3.94, 4.12, 2.93, 2.66, 3.17, 2.79, 2.61, 3.74, 3.13, 3.49, 2.94),\n                   \"Chlorine\" = c(1.45, 1.35, 0.26, 0.23, 1.10, 0.76, 1.59, 0.39, 0.39, 0.64, 0.82, 0.64, 0.85, 0.90, 0.92, 0.97, 0.18, 0.62, 0.51, 0.45, 1.79, 0.25, 0.31, 0.20, 0.24, 0.20, 2.27, 1.48, 0.25, 2.22),\n                   \"Potassium\" = c(5.67, 4.86, 4.19, 4.42, 3.17, 2.76, 3.81, 3.23, 5.44, 6.16, 5.48, 4.62, 4.49, 5.59, 5.86, 6.60, 4.51, 5.31, 5.16, 4.45, 6.17, 3.38, 3.51, 3.08, 3.98, 3.64, 6.50, 4.28, 4.71, 4.58),\n                   \"lburn\" = c(0.34, 0.11, 0.38, 0.68, 0.18, 0.00, 0.08, 0.11, 1.53, 0.77, 1.17, 1.01, 0.89, 1.40, 1.05, 1.15, 1.49, 0.51, 0.18, 0.34, 0.36, 0.89, 0.91, 0.92, 1.35, 1.33, 0.23, 0.26, 0.73, 0.23))\n```"
  },
  {
    "objectID": "Modelling/Continuous data.html",
    "href": "Modelling/Continuous data.html",
    "title": "Fitting a model with continuous data",
    "section": "",
    "text": "Suppose we are contacted by a group of scientists who are conducting experiments on leaf burn times. They have provided us with the leaf dataset and want to know what the relationship between the elements in the soil and burn time.\nThey tell us that the ‘lburn’ variable is the log of the leaf burn time. They also tell us that the ‘Nitrogen’, ‘Chlorine’ and ‘Potassium’ variables are the percentage of prevalant in soil.\nTo answer their question, and find the relationship between the elements and burn time, we are going to fit a linear model on the leaf dataset. However, before we fit the model, it is important to review the dataset carefully.\n\nA quick look at the data\nWe can load in the data and inspect the first 6 rows as follows.\n\n\n\n\n\n\nNote\n\n\n\nThe data can be found on the associated github here. Please download this to your computer and save it to your documents.\nYou will need to change the location of the file path. You can find this by clicking the file and reading the pop up window.\n\n\n(If you cannot load in the data, a version appears in the Data tab at the top of the website. You should be able to copy and paste that version into your R script and run it directly.)\n\n\nCode\n```{r}\n###| Loading in data\nleaf &lt;- read.csv(\"~/Documents/GitHub/Warwick Statistics Society R Course 2024-2025/Data/leaf.csv\")\n\n###| First 6 rows\nhead(leaf)\n```\n\n\n  Nitrogen Chlorine Potassium lburn\n1     3.05     1.45      5.67  0.34\n2     4.22     1.35      4.86  0.11\n3     3.34     0.26      4.19  0.38\n4     3.77     0.23      4.42  0.68\n5     3.52     1.10      3.17  0.18\n6     3.54     0.76      2.76  0.00\n\n\n\n\nSet up\nFrom the previous section, we observe 4 variables: Nitrogen, Chlorine, Potassium and lburn. We note that all 4 are continuous (as we would expect given what we have been told).\nNow, we want to fit a linear model of the form: \\[\\text{lburn} = \\alpha + \\beta \\times \\text{Nitrogen}+ \\gamma \\times \\text{Chlorine} + \\delta \\times \\text{Potassium} + \\epsilon.\\] Expressing this as a matrix, by reading off the first 2 rows of the dataframe,\n\\[\n\\begin{pmatrix} 0.34 \\\\ 0.11 \\\\ ... \\end{pmatrix} = \\begin{pmatrix} 1 & 3.05 & 1.45 & 5.67 \\\\ 1 & 4.22 & 1.35 & 4.86 \\\\ ... & ... & ... & ... \\end{pmatrix} \\times \\begin{pmatrix} \\alpha \\\\ \\beta \\\\ \\gamma \\\\ \\delta \\end{pmatrix} + \\epsilon.\n\\]\nThus, our goal is to estimate the \\(\\alpha\\),\\(\\beta\\),\\(\\gamma\\) and \\(\\delta\\) that best captures the relationship between the response variable (lburn) and the explanatory variables (Nitrogen, Chlorine and Potassium).\n\n\nUsing ‘lm()’\nIn R, the command ‘lm()’ will fit a linear model for you. We will begin by fitting a model where we only use the Nitrogen soil percentage to predict the burn time. That is, we fit the following model:\n\\[\ny_i = \\alpha + \\beta x_{i,\\text{Nitrogen}} + \\epsilon_i,\n\\]\nwhere \\(x_{i,\\text{Nitrogen}}\\) represents the Nitrogen percentage of the ith observation. We fit this using the lm() command with lburn ~ Nitrogen formula and setting the data=leaf.\n\n\nCode\n```{r}\n###| lburn ~ nitrogen model\nm1 &lt;- lm(lburn ~ Nitrogen, data = leaf) \n\n###| output\nm1\n```\n\n\n\nCall:\nlm(formula = lburn ~ Nitrogen, data = leaf)\n\nCoefficients:\n(Intercept)     Nitrogen  \n     2.6257      -0.5916  \n\n\nFrom the output, we obtain an estimate for \\(\\alpha = 2.6257\\) and an estimate for \\(\\beta = -0.5916\\). We interpret this as follows:\n\n\nIf there is no Nitrogen in the soil, the expected log leaf burn time is 2.6.\nFor every increase of 1% of Nitrogren in the soil, we expect a 0.6 decrease in the log leaf burn time.\n\n(Alternatively, we expect the leaf burn time to decrease by $(0.6).)\n\n\n\nHowever, if we want to get more information about the model, we can use the summary() command on a model, rather than looking at the raw output.\n\n\nCode\n```{r}\n###| Summary output of a model\nsummary(m1)\n```\n\n\n\nCall:\nlm(formula = lburn ~ Nitrogen, data = leaf)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-0.65636 -0.27698  0.03712  0.27876  0.63181 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.6257     0.3610   7.273 6.44e-08 ***\nNitrogen     -0.5916     0.1085  -5.454 8.03e-06 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.3404 on 28 degrees of freedom\nMultiple R-squared:  0.5151,    Adjusted R-squared:  0.4978 \nF-statistic: 29.75 on 1 and 28 DF,  p-value: 8.025e-06\n\n\nNow, the estimates of \\(\\alpha\\) and \\(\\beta\\) are reported in the Estimate column. The rest of the output consists of useful information about the performance of the model. A detailed blogpost of information was created by Felipe Rego and is available here. I have pulled out 2 key values that you need to be very comfortable with calculating and using:\n\n\nPr(&gt;|t|) is the p-value after performing a hypothesis test.\n\nThe asterics (*) indicate the significance. Depending on the context, different significance levels are suitable. In most cases \\(0.05\\) is a ‘good’ level.\n\nThe \\(R^2\\) and adjusted \\(R^2\\) describe how much variance is accounted for by the model (the closer to 1 the better)\n\n\nWe can also get information using ‘anova()’.\n\n\nCode\n```{r}\n###| Anova output\nanova(m1)\n```\n\n\nAnalysis of Variance Table\n\nResponse: lburn\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nNitrogen   1 3.4460  3.4460  29.748 8.025e-06 ***\nResiduals 28 3.2435  0.1158                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis no longer gives the \\(R^2\\) values. However, it will perform consecutive hypothesis tests.\n\n\nUsing ‘anova()’\nTo explore anova further, let us fit a different linear model: \\[\ny_i = \\alpha + \\beta x_{i,\\text{Nitrogen}} + \\gamma x_{i,\\text{Chlorine}}+ \\epsilon_i.\\]\n\n\nCode\n```{r}\n###| lburn ~ Nitrogen + Chlorine\nm2 &lt;- lm(lburn ~ Nitrogen + Chlorine, data = leaf)\n\n###| anova on larger model\nanova(m2)\n```\n\n\nAnalysis of Variance Table\n\nResponse: lburn\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nNitrogen   1 3.4460  3.4460 38.9352 1.127e-06 ***\nChlorine   1 0.8538  0.8538  9.6473  0.004424 ** \nResiduals 27 2.3897  0.0885                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe first row of the anova output is: \\[ Nitrogen \\quad  1 \\quad 3.4460 \\quad 3.4460 \\quad 38.9352 \\quad 1.127e-06 \\text{***}.\\]\nThe p-value \\(1.127e-06\\) comes from a hypothesis test with null hypothsis \\(\\beta = 0\\). That is to say, we compare the following 2 models, one where \\(\\beta =0\\) and one where \\(\\beta \\neq 0\\):\n\n\n\\(y_i = \\alpha + \\epsilon_i\\)\n\\(y_i = \\alpha + \\beta x_{i,\\text{Nitrogen}} + \\epsilon_i\\)\n\n\nAs the p-value is \\(&lt;0.05\\), we say that there is sufficient evidence to reject the null hypothesis.\nThe second row of the anova output is: \\[Chlorine \\quad  1 \\quad 0.8538 \\quad 0.8538 \\quad 9.6473 \\quad 0.004424 \\text{**}.\\]\nThe p-value \\(0.004424\\) comes from a hypothesis test with null hypothesis \\(\\gamma = 0\\). That is to say, we compare the following 2 models, one where \\(\\beta =0\\) and one where \\(\\beta \\neq 0\\):\n\n\n\\(y_i = \\alpha + \\beta x_{i,\\text{Nitrogen}} + \\epsilon_i\\)\n\\(y_i = \\alpha + \\beta x_{i,\\text{Nitrogen}} + \\gamma x_{i,\\text{Chlorine}} + \\epsilon_i\\)\n\n\nAs the p-value is \\(&lt;0.05\\), we say that there is sufficient evidence to reject the null hypothesis.\nMore information about interpreting anova can be found here.\n\n\nFitting a model with all 3 explanatory variables\nNow that we have some familiarity with the outputs, let us fit the model with all possible terms (i.e. the fully saturated model). That is, we fit\n\\[\ny_i = \\alpha + \\beta x_{i,\\text{Nitrogen}} + \\gamma x_{i,\\text{Chlorine}} + \\delta x_{i,\\text{Potassium}} + \\epsilon_i.\n\\]\n\n\nCode\n```{r}\n###| Saturated model\nm3 &lt;- lm(lburn ~ Nitrogen + Chlorine + Potassium, data = leaf)\n\n###| Output\nm3\n```\n\n\n\nCall:\nlm(formula = lburn ~ Nitrogen + Chlorine + Potassium, data = leaf)\n\nCoefficients:\n(Intercept)     Nitrogen     Chlorine    Potassium  \n     1.8110      -0.5315      -0.4396       0.2090  \n\n\nFrom fitting this model, we obtain estimates for \\(\\alpha\\),\\(\\beta\\),\\(\\gamma\\) and \\(\\delta\\). We interpret these as follows:\n\nFor a leaf with no Nitrogen, Chlorine and Potassium in the soil, we expect the log burning time to be \\(1.8\\). Thus, the burning time is \\(\\exp(1.8)\\approx 6\\).\nHolding all other element concentrations, an increase of \\(1\\%\\) in Nitrogen concentration corresponds to a decrease of 0.5 in the log burning time.\nHolding all other element concentrations, an increase of \\(1\\%\\) in Chlorine concentration corresponds to a decrease of 0.4 in the log burning time.\nHolding all other element concentrations, an increase of \\(1\\%\\) in Potassium concentration corresponds to an increase of 0.2 in the log burning time.\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is very important that we state that the other explanatory variables are held constant when interpreting the estimates! The estimate only explains the relationship when that specific explanatory variable is altered.\n\n\nNow, we can consider if we need all of these variables in our model. We do this by using ‘anova()’.\n\n\nCode\n```{r}\n###| Saturated model anova\nanova(m3)\n```\n\n\nAnalysis of Variance Table\n\nResponse: lburn\n          Df Sum Sq Mean Sq F value    Pr(&gt;F)    \nNitrogen   1 3.4460  3.4460  75.622 3.573e-09 ***\nChlorine   1 0.8538  0.8538  18.738 0.0001976 ***\nPotassium  1 1.2049  1.2049  26.441 2.311e-05 ***\nResiduals 26 1.1848  0.0456                      \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nIn all 3 consecutive tests (\\(\\beta=0\\),\\(\\gamma=0\\) and \\(\\delta=0\\)), we observe evidence to reject the null hypothesis. Thus, we keep all 3 variables and propose the following linear model to the scientists:\n\\[\ny_i = 1.8 -0.5 \\; x_{i,\\text{Nitrogen}} -0.4 \\; x_{i,\\text{Chlorine}} + 0.2 \\; x_{i,\\text{Potassium}} + \\epsilon_i.\n\\]",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Fitting a model with continuous data"
    ]
  },
  {
    "objectID": "Modelling/Loading in data.html",
    "href": "Modelling/Loading in data.html",
    "title": "Loading in data",
    "section": "",
    "text": "NEED TO WRITE UP HOW TO DO THIS."
  },
  {
    "objectID": "Modelling/Categorical data.html",
    "href": "Modelling/Categorical data.html",
    "title": "Fitting a model with categorical data",
    "section": "",
    "text": "Thus far, we have fit models with continuous data. However, we will often encounter variables that have a finite number of categories (i.e. categorical variables). We can either assign a reference category (treatment coding) or have no intercept term.\n\nSet up\nTo explore categorical data, let us look at the agriculture dataset.\n\n\nCode\n```{r}\n###| Load in the data\nagriculture &lt;- read.csv(\"~/Documents/GitHub/Warwick Statistics Society R Course 2024-2025/Data/agriculture.csv\")\n\n###| First 6 rows\nhead(agriculture)\n```\n\n\n  Variety Block SIZE YIELD\n1       A     1   28    28\n2       B     1   23    23\n3       C     1   27    27\n4       D     1   24    24\n5       E     1   30    30\n6       F     1   30    30\n\n\nIn this dataset, there are 4 variables: Variety, Block, SIZE and YIELD. From the first 6 rows, we see that Variety is a categorical variable with categories at least A-F. However, as we can only see 1 for Block, it is unclear what type of variable it is. Instead, let us look at the summary of the dataset, using the summary() function.\n\n\nCode\n```{r}\n###| Dataset summary\nsummary(agriculture)\n```\n\n\n   Variety              Block           SIZE           YIELD      \n Length:48          Min.   :1.00   Min.   : 19.0   Min.   : 19.0  \n Class :character   1st Qu.:2.75   1st Qu.: 27.0   1st Qu.: 27.0  \n Mode  :character   Median :4.50   Median : 82.0   Median : 82.0  \n                    Mean   :4.50   Mean   :113.0   Mean   :113.0  \n                    3rd Qu.:6.25   3rd Qu.:201.2   3rd Qu.:201.2  \n                    Max.   :8.00   Max.   :261.0   Max.   :261.0  \n\n\nFrom this, we see that SIZE is a continuous variable from 19 to 261 and YIELD is a continuous variable from 19 to 261. (Exploring this further, we note that the size and yield observations are the same for every entry.) We also see that Block takes values from 1 to 8 and Variety is always a character. However, we do not know what values Variety takes.\nTo fix this, we will alter the data using the factor() command.\n\n\nCode\n```{r}\n###| Factor command\nagriculture$Variety &lt;- factor(agriculture$Variety)\n\n###| First 6 rows\nhead(agriculture)\n\n###| Summary\nsummary(agriculture)\n```\n\n\n  Variety Block SIZE YIELD\n1       A     1   28    28\n2       B     1   23    23\n3       C     1   27    27\n4       D     1   24    24\n5       E     1   30    30\n6       F     1   30    30\n Variety     Block           SIZE           YIELD      \n A:8     Min.   :1.00   Min.   : 19.0   Min.   : 19.0  \n B:8     1st Qu.:2.75   1st Qu.: 27.0   1st Qu.: 27.0  \n C:8     Median :4.50   Median : 82.0   Median : 82.0  \n D:8     Mean   :4.50   Mean   :113.0   Mean   :113.0  \n E:8     3rd Qu.:6.25   3rd Qu.:201.2   3rd Qu.:201.2  \n F:8     Max.   :8.00   Max.   :261.0   Max.   :261.0  \n\n\nNow, the Variety column is treated as a categorical variable, rather than as characters/strings. If we look at the new summary of the data, we observe that there are 6 categories (A-F) and each category has 8 observations.\n\n\nFitting a model\nNow, let’s fit a model that uses Variety and Block to predict SIZE. That is, we fit a model of the following from:\n\\[\\begin{align}\nSIZE_i = & \\alpha \\times \\mathbb{I}{(\\text{oberservation i has Variety A})} + \\beta \\times \\mathbb{I}{(\\text{oberservation i has Variety B})} \\\\ & + \\gamma \\times \\mathbb{I}{(\\text{oberservation i has Variety C})} + \\delta \\times \\mathbb{I}{(\\text{oberservation i has Variety D})} \\\\ & + \\epsilon \\times \\mathbb{I}{(\\text{oberservation i has Variety E})} + \\zeta \\times \\mathbb{I}{(\\text{oberservation i has Variety F})} \\\\ & + \\eta \\times Block_i + \\hat{\\epsilon}_i,\n\\end{align}\\]\nwhere \\(\\hat{\\epsilon}_i\\) is the error of the ith observation.\n\n\nCode\n```{r}\n###| SIZE ~ Variety + Block\nm1 &lt;- lm(SIZE ~ 0 + Variety + Block, data = agriculture) # Set intercept to 0\n\n###| Output\nm1\n```\n\n\n\nCall:\nlm(formula = SIZE ~ 0 + Variety + Block, data = agriculture)\n\nCoefficients:\nVarietyA  VarietyB  VarietyC  VarietyD  VarietyE  VarietyF     Block  \n  57.625    62.875    69.625    89.500    73.500    79.875     9.083  \n\n\nReading off of the output, we obtain the following model:\n\\[\\begin{align}\nSIZE_i = &57.6 \\times \\mathbb{I}{(\\text{oberservation i has Variety A})} + 62.9 \\times \\mathbb{I}{(\\text{oberservation i has Variety B})} \\\\ & + 69.6 \\times \\mathbb{I}{(\\text{oberservation i has Variety C})} + 89.5 \\times \\mathbb{I}{(\\text{oberservation i has Variety D})} \\\\ & + 73.5 \\times \\mathbb{I}{(\\text{oberservation i has Variety E})} + 79.9 \\times \\mathbb{I}{(\\text{oberservation i has Variety F})} \\\\ & + 9.1 \\times Block_i + \\hat{\\epsilon}_i.\n\\end{align}\\]\nWe interpret the Block variable as before: holding every other variable constant, a 1 unit increase in the Block is expected to result in a 9.1 unit increase in SIZE.\n\n\n\n\n\n\nImportant\n\n\n\nHowever, we cannot interpret the categorical variable, Variety, in the same way. Instead, we will compare what happens as we change category.\n\n\nFor example,\n\n\nFor a Variety A observation, and holding Block constant, we expect the SIZE to be 5.3 units smaller than if it was Variety B.\nFor a Variety F observation, and holding Block constant, we expect the SIZE to be 22.3 units larger than if it was Variety A.\n\n\n\n\nRefrence categories\nUsing no intercept (by adding the 0 term in the lm() command), we can compare across categories easily. However, suppose we want to compare everything to category A. Instead of finding the estimates and then subtracting them, we can ask R to do this directly by setting up reference categories using relevel().\nNow, our model looks like:\n\\[\\begin{align}\nSIZE_i = & \\tilde{\\alpha} + \\tilde{\\beta} \\times \\mathbb{I}{(\\text{oberservation i has Variety B})} \\\\ & + \\tilde{\\gamma} \\times \\mathbb{I}{(\\text{oberservation i has Variety C})} + \\tilde{\\delta} \\times \\mathbb{I}{(\\text{oberservation i has Variety D})} \\\\ & + \\tilde{\\epsilon} \\times \\mathbb{I}{(\\text{oberservation i has Variety E})} + \\tilde{\\zeta} \\times \\mathbb{I}{(\\text{oberservation i has Variety F})} \\\\ & + \\tilde{\\eta} \\times Block_i + \\hat{\\epsilon}_i,\n\\end{align}\\]\nWe code this up as follows:\n\n\nCode\n```{r}\n###| Setting Category A as the reference category\nagriculture$Variety &lt;- relevel(agriculture$Variety, ref = \"A\")\n\n###| New model\nm2 &lt;- lm(SIZE ~ Variety + Block, data = agriculture) # no specified intercept!\n\n###| Output\nm2\n```\n\n\n\nCall:\nlm(formula = SIZE ~ Variety + Block, data = agriculture)\n\nCoefficients:\n(Intercept)     VarietyB     VarietyC     VarietyD     VarietyE     VarietyF  \n     57.625        5.250       12.000       31.875       15.875       22.250  \n      Block  \n      9.083  \n\n\nNow, our model is: \\[\\begin{align}\nSIZE_i = &57.6 + 5.3 \\times \\mathbb{I}{(\\text{oberservation i has Variety B})} \\\\ & + 12 \\times \\mathbb{I}{(\\text{oberservation i has Variety C})} + 31.9 \\times \\mathbb{I}{(\\text{oberservation i has Variety D})} \\\\ & + 15.7 \\times \\mathbb{I}{(\\text{oberservation i has Variety E})} + 22.3 \\times \\mathbb{I}{(\\text{oberservation i has Variety F})} \\\\ & + 9.1 \\times Block_i + \\hat{\\epsilon}_i.\n\\end{align}\\]\nBoth of these models are equivalent! They are reparametrisations of each other, with \\(\\alpha = \\tilde{\\alpha}\\), \\(\\beta = \\tilde{\\alpha} + \\tilde{\\beta}\\), \\(\\gamma = \\tilde{\\alpha} + \\tilde{\\gamma}\\), …, \\(\\zeta = \\tilde{\\alpha} + \\tilde{\\zeta}\\) and \\(\\eta = \\tilde{\\eta}\\). Based on the context, you should choose to include or not to include the intercept term.\n\n\nInteraction terms\nWhat effect does the Variety category have on the Block estimator? We explore this using interaction terms with a model of the form:\n\\[\\begin{align}\nSIZE_i = & \\alpha + \\beta \\times \\mathbb{I}{(\\text{oberservation i has Variety B})} \\\\ & + ... + \\zeta \\times \\mathbb{I}{(\\text{oberservation i has Variety F})} \\\\ & + \\eta \\times Block_i + \\\\ & \\tilde{\\beta} \\times \\mathbb{I}{(\\text{oberservation i has Variety B})} \\times Block_i \\\\ & + ... + \\tilde{\\zeta} \\times \\mathbb{I}{(\\text{oberservation i has Variety F})} \\times Block_i \\\\ & \\hat{\\epsilon}_i,\n\\end{align}\\] where \\(\\hat{\\epsilon}_i\\) is the error of the ith observation.\n\n\nCode\n```{r}\n###| Model with intercept terms\nm3 &lt;- lm(SIZE ~ Variety + Block + Variety:Block, data = agriculture) \n\n###| Output\nm3\n```\n\n\n\nCall:\nlm(formula = SIZE ~ Variety + Block + Variety:Block, data = agriculture)\n\nCoefficients:\n   (Intercept)        VarietyB        VarietyC        VarietyD        VarietyE  \n        88.000         -29.893         -24.429         -23.679         -20.286  \n      VarietyF           Block  VarietyB:Block  VarietyC:Block  VarietyD:Block  \n         3.286           2.333           7.810           8.095          12.345  \nVarietyE:Block  VarietyF:Block  \n         8.036           4.214  \n\n\nWe interpret the model as follows:\n\n\nIf the observation is Variety B and holding block constant, the SIZE will be 29.9 units smaller than if it was Variety A.\n\nIf the observation is Variety C and holding block constant, the SIZE will be 24.4 units smaller than if it was Variety A.\n\nIf the observation is Variety D and holding block constant, the SIZE will be 23.7 units smaller than if it was Variety A.\n\nIf the observation is Variety E and holding block constant, the SIZE will be 20.3 units smaller than if it was Variety A.\n\nIf the observation is Variety F and holding block constant, the SIZE will be 3.3 units larger than if it was Variety A.\nIf the observation is Variety A, increasing block size by 1 will increase the SIZE by 2.3.\nIf the observation is Variety B, increasing block size by 1 will increase the SIZE by 7.8.\nIf the observation is Variety C, increasing block size by 1 will increase the SIZE by 8.1.\nIf the observation is Variety D, increasing block size by 1 will increase the SIZE by 12.3.\nIf the observation is Variety E, increasing block size by 1 will increase the SIZE by 8.0.\nIf the observation is Variety F, increasing block size by 1 will increase the SIZE by 2.3.\n\n\n\n\n\n\n\n\nImportant\n\n\n\nWhen using interaction terms, it is important to remember the principle of parsimony. If we decide to model the interaction between variable A and variable B, we must include both variables in the model.",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Fitting a model with categorical data"
    ]
  },
  {
    "objectID": "Modelling/Check your understanding.html",
    "href": "Modelling/Check your understanding.html",
    "title": "Check your understanding",
    "section": "",
    "text": "data.csv task\nDownload and load in the data.csv file. This contains 3 variables: x1, x2 and y.\n\n\nPlot y against x1; plot y against x2. What do you notice about the relationships?\nWhat does the design matrix of the following model look like? \\[y_i = \\beta_0 +\\beta_1 x_{1i} + \\beta_2 x_{2i} + \\epsilon_i.\\]\nFit this model in R, find the estimates of \\(\\beta_0\\),\\(\\beta_1\\) and \\(\\beta_2\\) and interpret them.\nFit a new model, \\[y_i = \\gamma_0 +\\gamma_1 x_{1i} + \\gamma_2 x_{2i}^2 + \\epsilon_i.\\] How do the estimates of \\(\\gamma\\) compare to the estimates of \\(\\beta\\)?\n\n\n\n\ncoursework.csv task\nDownload and load in the coursework.csv` file. This is a dataset of the coursework marks, out of 20, and the exam marks, out of 100.\n\n\nCreate coursework and exam variables in R.\nPlot exam against coursework with exam on the y axis.\nFor a simple linear regression model with exam as the response and coursework as the predictor variable.\nAccording to the model, for each extra coursework mark, how many extra exam marks were obtained?\nUsing the ‘fitted’ command in R, calculate the fitted values of the model.\n\nUse ?fitted to get more information about this command.\nThe ith fitted value is the value predicted by the model based on the ith observation.\n\nAdd the fitted line to the graph of exam against coursework.\n\nHint: use the abline function.\n\nCreate a new variable, coursework_percentage, that contains the coursework mark as a percentage.\nFit a new model with coursework_percentage as the predictor variable.\nWhat effect does the rescaling of the coursework marks have on the fitted values and on the parameters?\nPredict the exam score for a student that scored full marks in their coursework. Is there an issue with this prediction?\n\n\n\n\nAnswers to data.csv tasks\n\n\nCode\n```{r}\n###| Loading in file\ndata &lt;- read.csv(\"~/Documents/GitHub/Warwick Statistics Society R Course 2024-2025/Data/data.csv\")\n\n###| Plot\nplot(data$x1, data$y, xlab = \"x1\", ylab = \"y\", \n     main = \"Scatter plots of x1 against y\")\nplot(data$x2, data$y, xlab = \"x2\", ylab = \"y\", \n     main = \"Scatter plots of x2 against y\")\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nFrom the 2 graphs, we observe that y increases as x1 increases but y decreases as x2 increases.\nThe design matrix for this model will be of the form:\n\\[\n\\begin{pmatrix} 1 & 2.37 & 20.41 \\\\ 1 & 8.38 & 4.34 \\\\ ... & ... & ... \\end{pmatrix}.\n\\]\n\n\nCode\n```{r}\n###| Fitting the model\nm1 &lt;- lm(y ~ x1 + x2, data = data)\nm1\n```\n\n\n\nCall:\nlm(formula = y ~ x1 + x2, data = data)\n\nCoefficients:\n(Intercept)           x1           x2  \n   5.594377     0.614462    -0.004929  \n\n\nIf \\(x1 = 0\\) and \\(x2= 0\\), we expect \\(y=5.6\\). Assuming \\(x2\\) remains constant, if \\(x1\\) increases by 1, we expect \\(y\\) to increase by 0.6. Assuming \\(x1\\) remains constant, if \\(x1\\) increases by 1, we expect \\(y\\) to decrease by 0.005.\n\n\nCode\n```{r}\n###| Fitting the second model\nm2 &lt;- lm(y ~ x1 + I(x2^2), data = data) # NOTE THAT WE NEED TO USE I()!\nm2\n```\n\n\n\nCall:\nlm(formula = y ~ x1 + I(x2^2), data = data)\n\nCoefficients:\n(Intercept)           x1      I(x2^2)  \n  5.439e+00    6.307e-01   -2.652e-05  \n\n\nIf \\(x1 = 0\\) and \\(x2= 0\\), we expect \\(y=5.4\\). Assuming \\(x2\\) remains constant, if \\(x1\\) increases by 1, we expect \\(y\\) to increase by 0.6. Assuming \\(x1\\) remains constant, if \\(x1\\) increases by 1, we do not expect \\(y\\) to change (to 4 decimal places).\n\n\nAnswers to coursework.csv\n\n\nCode\n```{r}\n###| Loading in file\ncoursework.df &lt;- read.csv(\"~/Documents/GitHub/Warwick Statistics Society R Course 2024-2025/Data/coursework.csv\")\n\n###| Variable assigning\ncoursework &lt;- coursework.df$cw\nexam &lt;- coursework.df$exam\n\n###| Scatter plot\nplot(coursework,exam, xlab = \"Raw coursework mark\",\n     ylab = \"Exam mark\", main = \"Coursework against exam marks\")\n```\n\n\n\n\n\n\n\n\n\nFrom the scatter plot, we observe that a linear relationship between the raw coursework mark and the exam mark, where students who performed well in coursework also performed well on the exam and students who performed badly in the coursework also performed badly in the exam.\n\n\nCode\n```{r}\n###| Raw coursework model\nm1 &lt;- lm(exam ~ cw, data = coursework.df)\nm1\n```\n\n\n\nCall:\nlm(formula = exam ~ cw, data = coursework.df)\n\nCoefficients:\n(Intercept)           cw  \n     19.377        4.383  \n\n\nFor each extra mark obtained on the coursework, we expect the student to gain an extra \\(4.4\\%\\) in the exam.\n\n\nCode\n```{r}\n###| Fitted values\nfitted &lt;- fitted(m1)\n\n###| Adding fitted values as points\nplot(coursework,exam, xlab = \"Raw coursework mark\",\n     ylab = \"Exam mark\", main = \"Coursework against exam marks\")\npoints(coursework,fitted, col = \"blue\")\n\n###| Adding a line through the fitted points\nplot(coursework,exam, xlab = \"Raw coursework mark\",\n     ylab = \"Exam mark\", main = \"Coursework against exam marks\")\npoints(coursework,fitted, col = \"blue\")\nabline(a = m1$coefficients[1], b = m1$coefficients[2], col = \"blue\")\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCode\n```{r}\n###| Coursework as a percentage\ncoursework.df$coursework_perc &lt;- coursework/20\n\nm2 &lt;- lm(exam ~ coursework_perc, data = coursework.df)\nm2\n\nfitted_perc &lt;- fitted(m2)\nfitted_perc\n```\n\n\n\nCall:\nlm(formula = exam ~ coursework_perc, data = coursework.df)\n\nCoefficients:\n    (Intercept)  coursework_perc  \n          19.38            87.66  \n\n       1        2        3        4        5        6 \n41.29114 41.29114 63.20570 71.97152 80.73734 89.50316 \n\n\nThe fitted values and the intercept estimate are the same! However, the \\(\\beta\\) estimate is now 87.66, \\(20\\times 4.383\\).\nIf a student achieves \\(100\\%\\) in their coursework, we expect them to achive \\(19.38 + 87.66 = 107.04\\%\\) in the exam. This is a big problem as a student cannot achieve \\(&gt;100\\%\\)! It is very important to be careful when extrapolating as it is very easy to end up in unrealistic (or even impossible) situations.",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Check your understanding"
    ]
  },
  {
    "objectID": "Data/Raw data.html#agriculture-data",
    "href": "Data/Raw data.html#agriculture-data",
    "title": "Data",
    "section": "Agriculture data",
    "text": "Agriculture data\n\n\nCode\n```{r}\n###| Agriculture data\nagriculture &lt;- data.frame(\"Variety\" = c(\"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\", \"A\", \"B\", \"C\", \"D\", \"E\", \"F\"),\n                          \"Block\" = c(1, 1, 1, 1, 1, 1, 2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 4, 4, 4, 4, 4, 4, 5, 5, 5, 5, 5, 5, 6, 6, 6, 6, 6, 6, 7, 7, 7, 7, 7, 7, 8, 8, 8, 8, 8, 8),\n                          \"SIZE\" = c(28, 23,27, 24,30,30,202,145,188,201,202,228,22,26,24,28,26, 25,165,201,185,231,178,221,27,28,27,30,26,27, 191,203,185,238,198,207,19,24,28,30,29,24,134,180,220,261,226,204),\n                          \"YIELD\" = c(28, 23,27, 24,30,30,202,145,188,201,202,228,22,26,24,28,26,25,165,201,185,231,178,221,27,28,27,30,26,27,191,203,185,238,198,207,19,24,28,30,29,24,134,180,220,261,226,204))\n```"
  },
  {
    "objectID": "Modelling/Diagnostics.html",
    "href": "Modelling/Diagnostics.html",
    "title": "Model diagnostics",
    "section": "",
    "text": "In the previous pages, we covered what a linear model is and how to fit a linear model. In this page, we will cover how to check a model fits the 4 key assumptions and what to do if it fails to meet an assumption.\nTo recap, the 4 key assumptions are:",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "Modelling/Diagnostics.html#generating-data",
    "href": "Modelling/Diagnostics.html#generating-data",
    "title": "Model diagnostics",
    "section": "Generating data",
    "text": "Generating data\nWe are going to create some data to illustrate good and bad model assumptions.\n\n\nCode\n```{r}\n###| For reproducability, we set a seed\nset.seed(419)\n\n###| Generating random numbers\na &lt;- runif(n = 100, min = 0, max = 10)\nb &lt;- rpois(n = 100, lambda = 2)\nc &lt;- runif(n = 100, min = -5, max = 10)\n\n###| Generating the response variables\ny1 &lt;- a + b\ny2 &lt;- a + b^2\ny3 &lt;- log(a)\n\n###| Storing as a dataframe\ndata &lt;- data.frame(\"a\" = a,\n                   \"b\" = b,\n                   \"c\" = c,\n                   \"y1\" = y1,\n                   \"y2\" = y2,\n                   \"y3\" = y3)\n\n###| First 6 rows\nhead(data)\n```\n\n\n         a b         c        y1        y2        y3\n1 9.786378 2 -4.673051 11.786378 13.786378 2.2809914\n2 3.318455 5 -3.948319  8.318455 28.318455 1.1994994\n3 8.329044 0 -4.219525  8.329044  8.329044 2.1197487\n4 2.087712 4 -2.410313  6.087712 18.087712 0.7360689\n5 3.048667 1  8.473026  4.048667  4.048667 1.1147043\n6 4.070241 1  3.784801  5.070241  5.070241 1.4037022"
  },
  {
    "objectID": "Modelling/Diagnostics.html#diagnostic-plots",
    "href": "Modelling/Diagnostics.html#diagnostic-plots",
    "title": "Model diagnostics",
    "section": "Diagnostic plots",
    "text": "Diagnostic plots\nOne way to test model assumptions is to look at the residual plots.\n\n\n\n\n\n\nNote\n\n\n\nA residual (or fitting deviation) is an estimate of the unobsereved statistical error. It is the difference between the observed \\(y_i\\) and the estimated \\(\\hat{y}_i\\), the value predicted by the model residuals.\n\n\nWe want our model’s predictions to be as close to the true observed values as possible, that is, we want to minimise the residuals. We also use the residuals to check if we are meeting the linearity and normality assumptions. We can also identify outliers this way.\n\nA good model\nTo explore what ideal residual plots look like, we are going to exactly fit the model \\[y1_i = a_i + b_i + \\epsilon_i.\\] Based on the way we have defined \\(y1\\), we know that this model should fit the data exactly. We plot several different diagnostic plots using the ‘plot()’ command.\nWe will explore each plot one by one:\n\n\nCode\n```{r}\n###| Fitting the model\nm1 &lt;- lm(y1 ~ a + b, data = data)\n\n###| Residual v fitted plot\nplot(m1, which = 1)\n```\n\n\n\n\n\n\n\n\n\nThe residuals v fitted values plot is a scatter plot of with the residuals on the y-axis and the fitted/predicted values on the x-axis. The red line (called the smoother) is a curve fitted to the residuals. If the structural component of the model is correct (i.e. the explanatory variables and not the errors), the smoother will be horizontal around 0.\nIn this example, we see a very flat smoother, centered at 0, with 3 labelled points that deviate from the general trend.\n\n\nCode\n```{r}\n###| Q-Q plot\nplot(m1, which = 2)\n```\n\n\n\n\n\n\n\n\n\nRecall that a model assumption is that the errors are normally distributed. The Q-Q plot allows us to check this assumption by plotting the absolute values of the standardized residuals on the y-axis and the theoretical quantiles of the standard half-normal distribution on the x-axis. If the errors are normally distributed, the dashed line will go through the origin and have a gradient of 1.\nIn this example, we see that the residuals do appear to be normally distributed, although at the tails, we do see 3 points that deviate from this.\n\n\nCode\n```{r}\n###| Cook's distance plot\nplot(m1, which = 4)\n```\n\n\n\n\n\n\n\n\n\nThis graph plots the Cook’s distance against the observation number, with the largest 3 values labelled. The Cook’s distance can be thought of as a measure of the influence of a particular observation. That is, it summarises how much the model changes, when you do and don’t include the ith data point.\nYou may notice that we have skipped several plots, namely: the location-scale plot and the standardized resivual v leverage plot. More information about diasgnotstic plots can be found here for those interested.\n\n\nA bad model\nNow, let us purposefully fit a bad model to see how the diagnostic plots change.\n\n\nCode\n```{r}\n###| Bad model example\nm2 &lt;- lm(y2 ~ a + b, data=data)\n\n###| Residual v fitted plot\nplot(m2, which = 1)\n\n###| Q-Q plot\nplot(m2, which = 2)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the residuals v fitted values plot, we observe that the smoother has a parabolic shape. This indicates that there is a problem in how we have constructed our model - specifically we are failing to meet the linearity assumption. If we look back to how we defined y2 (\\(y2 = a + b^2\\)), this makes sense. We cannot capture the \\(b^2\\) nature of our data if we are only using \\(a+b\\) as our explanatory variables.\nIn the Q-Q plot, we note large deviations from the ideal dashed line at the upper tail. This indicates that our errors are not normally distributed, violating that assumption.\nSo, we conclude that \\(y2_i = a_i + b_i +\\epsilon_i\\) is a bad model as it violates the key model assumptions. But, how do we fix this?\n\n\nTransformations\nOn top of altering which explanatory variables we do and do not include in our model, we can transform our variables to make the model fit better.\nFor example, in the \\(y2\\) case, we can create a new variable \\(b2\\) that is \\(b^2\\) and include this transformed variable into our model. However, by the principle of parsimony, we also need to include \\(b\\) into the model as well.\n\n\nCode\n```{r}\n###| Modeeling y2\nm3 &lt;- lm(y2 ~ a + b + I(b^2), data = data)\n\n###| Residuals v fitted plot\nplot(m3, which = 1)\n\n###| Q-Q plot\nplot(m3, which = 2)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the residuals plot, we observe a flat smoother. This indicates we are meeting the linearity assumption!\nIn the Q-Q plot, we see that the data follows the dashed line, baring 3 exceptions at the extreme. Thus, the model appears to meet the normally distributed errors assumption.\n\n\nAnother bad model\nTo further explore diagnostic plots, let us consider one more model: \\[y3_i = a_i + \\epsilon.\\]\n\n\nCode\n```{r}\n###| y3 = a\nm4 &lt;- lm(y3 ~ a, data = data)\n\n###| Residual plot\nplot(m4, which = 1)\n```\n\n\n\n\n\n\n\n\n\nFitting this model, we observe a curve shape in the smoother, indicating that we are violating the linearity assumption. As the shape is not symmetric, it doesn’t seem like this is can be corrected by using \\(a^2\\). Instead, we can try using a logarithm.\n\n\nCode\n```{r}\n###| y3 = log(a)\nm5 &lt;- lm(y3 ~ log(a), data = data)\n\n###| Residual plot\nplot(m5, which = 1)\n```\n\n\n\n\n\n\n\n\n\nNow, we observe the correct behaviour of the smoother."
  },
  {
    "objectID": "Modelling/Diagnostics.html#diagnostic-plots-with-noise",
    "href": "Modelling/Diagnostics.html#diagnostic-plots-with-noise",
    "title": "Model diagnostics",
    "section": "Diagnostic plots with noise",
    "text": "Diagnostic plots with noise\nThus far, we have used formulas to generate our response variables. This has resulted in very clearly defined behaviour from our models and diagnostic plots. However, in a normal scenario, the data we are given will have noise. So, we will add some normally distributed noise to our data and repeat the above.\n\n\nCode\n```{r}\n###| Adding noise\ndata2 &lt;- data\ndata2$y1 &lt;- data2$y1 + rnorm(n = 100, mean = 0, sd = 1)\ndata2$y2 &lt;- data2$y2 + rnorm(n = 100, mean = 0, sd = 1)\ndata2$y3 &lt;- data2$y3 + rnorm(n = 100, mean = 0, sd = 1)\n```\n\n\n\nA good model revisited\nOnce again, we will fit the model \\[y1_i = a_i + b_i + \\epsilon_i.\\] However, now we will use the noisy dataset.\n\n\nCode\n```{r}\n###| y1 model\nM1 &lt;- lm(y1 ~ a + b, data = data2)\nplot(M1,which=1)\nplot(M1, which = 2)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, there is a lot more variation in the residuals. However, we still observe a roughly flat smoother so the model does not appear to be violating the linearity assumption.\nThe Q-Q plot still follows the ideal general trend, athough the tails deviate slightly more.\n\n\nA bad model revisited\nNow, we fit both of the following models: \\[\\begin{align}\ny2_i & = a_i + b_i + \\epsilon_i, \\\\\ny2_i & = a_i + b_i + b_i^2 + \\epsilon_i.\n\\end{align}\\]\n(I have placed the graphs side by side for comparison using the ‘par(mfrow = c(1,2)))’ command. More information about this, and graphics more generally, appears here.)\n\n\nCode\n```{r}\n###| a + b\nM2 &lt;- lm(y2 ~ a + b, data = data2)\n\n###| a + b + b^2\nM3 &lt;- lm(y2 ~ a + b + I(b^2), data = data2)\n\n###| Plotting graphs side by side\npar(mfrow = c(1, 2))\n\nplot(M2,which=1)\nplot(M3,which=1)\n\nplot(M2, which = 2)\nplot(M3, which = 2)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nIn the residuals plot, we see the same general trends in behaviour that we saw before. The graph on the left, fitted with \\(a+b\\), displays the problematic parabolic behaviour in the smoother. The graph on the right, fitted with \\(a+b+b^2\\), has a flatter smoother.\nSimilarly, in the Q-Q plots, we observe that the left graph deviates at the tails more than the right graph.\n\n\nAnother bad model revisited\nNow, we fit both of the following models: \\[\\begin{align}\ny2_i & = a_i \\epsilon_i, \\\\\ny2_i & = \\log(a_i) + \\epsilon_i.\n\\end{align}\\]\nAgain, for ease, I have plotted the graphs side by side.\n\n\nCode\n```{r}\n###| a \nM4 &lt;- lm(y3 ~ a, data = data2)\n\n###| log(a)\nM5 &lt;- lm(y3 ~ log(a), data = data2)\n\n###| Plotting graphs side by side\npar(mfrow = c(1, 2))\n\nplot(M4,which=1)\nplot(M5,which=1)\n\nplot(M4, which = 2)\nplot(M5, which = 2)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis time, it is much less clear which of these two models is better.\nComparing the Q-Q plots, the left graph appears to deviate more on the lower tail. So, the \\(\\log(a)\\) model appears to better meet the normality assumption.\nHowever, both models have issues with the smoother in the residuals v fitted graphs. For the \\(y3 ~ a\\) model, there is a general curve shape to the smoother and residuals that violates the linearity assumption. While this appears to be fixed somewhat by using \\(\\log(a)\\) instead, now the residuals get further and further away from the smoother and the fitted values get larger. (This shape is known as a megaphone.) This violates the assumption that the errors have constant variance.\nThus, we conclude that it appears neither model is a good fit."
  },
  {
    "objectID": "Modelling/Diagnostics.html#a-good-model",
    "href": "Modelling/Diagnostics.html#a-good-model",
    "title": "Model diagnostics",
    "section": "A good model",
    "text": "A good model\nTo explore what ideal residual plots look like, we are going to exactly fit the model \\[y1_i = a_i + b_i + \\epsilon_i.\\] Based on the way we have defined \\(y1\\) (y1 &lt;- a + b), we know that this model should fit the data almost exactly. (The almost stems from the random noise from \\(\\epsilon\\).) We plot several different diagnostic plots using the ‘plot()’ command.\nWe will explore each plot one by one.\n\n\nCode\n```{r}\n###| Fitting the model\nm1 &lt;- lm(y1 ~ a + b, data = data)\n\n###| Residual v fitted plot\nplot(m1, which = 1)\n```\n\n\n\n\n\n\n\n\n\nThe residuals v fitted values plot is a scatter plot of with the residuals on the y-axis and the fitted/predicted values on the x-axis. The red line (called the smoother) is a curve fitted to the residuals. If the structural component of the model is correct (i.e. the explanatory variables and not the errors), the smoother will be horizontal around 0.\nIn this example, we see a very flat smoother, centered at 0, with 3 labelled points that deviate from the general trend.\n\n\nCode\n```{r}\n###| Q-Q plot\nplot(m1, which = 2)\n```\n\n\n\n\n\n\n\n\n\nRecall that a model assumption is that the errors are normally distributed. The Q-Q plot allows us to check this assumption by plotting the absolute values of the standardized residuals on the y-axis and the theoretical quantiles of the standard half-normal distribution on the x-axis. If the errors are normally distributed, the dashed line will go through the origin and have a gradient of 1.\nIn this example, we see that the residuals do appear to be normally distributed, although at the tails, we do see 3 points that deviate from this.\n\n\nCode\n```{r}\n###| Cook's distance plot\nplot(m1, which = 4)\n```\n\n\n\n\n\n\n\n\n\nThis graph plots the Cook’s distance against the observation number, with the largest 3 values labelled. The Cook’s distance can be thought of as a measure of the influence of a particular observation. That is, it summarises how much the model changes, when you do and don’t include the ith data point.\nYou may notice that we have skipped several plots, namely: the location-scale plot and the standardized resivual v leverage plot. More information about diasgnotstic plots can be found here for those interested.",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "Modelling/Diagnostics.html#a-bad-model",
    "href": "Modelling/Diagnostics.html#a-bad-model",
    "title": "Model diagnostics",
    "section": "A bad model",
    "text": "A bad model\nNow, let us purposefully fit a bad model to see how the diagnostic plots change.\n\n\nCode\n```{r}\n###| Bad model example\nm2 &lt;- lm(y2 ~ a + b, data=data)\n\n###| Residual v fitted plot\nplot(m2, which = 1)\n```\n\n\n\n\n\n\n\n\n\nIn the residuals v fitted values plot, we observe that the smoother has a parabolic shape. This indicates that there is a problem in how we have constructed our model - specifically we are failing to meet the linearity assumption. If we look back to how we defined y2 (y2 &lt;- a + b^2), this makes sense. We cannot capture the \\(b^2\\) nature of our data if we are only using \\(a+b\\) as our explanatory variables.\n\n\nCode\n```{r}\n###| Q-Q plot\nplot(m2, which = 2)\n```\n\n\n\n\n\n\n\n\n\nIn the Q-Q plot, we note large deviations from the ideal dashed line at the upper tail. This indicates that our errors are not normally distributed, violating that assumption.\nSo, we conclude that \\(y2_i = a_i + b_i +\\epsilon_i\\) is a bad model as it violates the key model assumptions. But, how do we fix this?",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "Modelling/Diagnostics.html#transformations",
    "href": "Modelling/Diagnostics.html#transformations",
    "title": "Model diagnostics",
    "section": "Transformations",
    "text": "Transformations\nOn top of choosing which variables we include in our model, we can transform our variables to make the model fit better.\nFor example, in the \\(y2\\) case, we can create a new variable \\(b2\\) that is \\(b^2\\) and include this transformed variable into our model. However, by the principle of parsimony, we also need to include \\(b\\) into the model as well.\n\n\nCode\n```{r}\n###| Modeeling y2\nm3 &lt;- lm(y2 ~ a + b + I(b^2), data = data)\n\n###| Residuals v fitted plot\nplot(m3, which = 1)\n```\n\n\n\n\n\n\n\n\n\nIn the residuals plot, we observe a flat smoother. This indicates we are meeting the linearity assumption!\n\n\nCode\n```{r}\n###| Q-Q plot\nplot(m3, which = 2)\n```\n\n\n\n\n\n\n\n\n\nIn the Q-Q plot, we see that the data follows the dashed line, baring 3 exceptions at the extreme. Thus, the model appears to meet the normally distributed errors assumption.",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "Modelling/Diagnostics.html#another-bad-model",
    "href": "Modelling/Diagnostics.html#another-bad-model",
    "title": "Model diagnostics",
    "section": "Another bad model",
    "text": "Another bad model\nTo further explore diagnostic plots, let us consider one more model: \\[y3_i = a_i + \\epsilon.\\]\n\n\nCode\n```{r}\n###| y3 = a\nm4 &lt;- lm(y3 ~ a, data = data)\n\n###| Residual plot\nplot(m4, which = 1)\n```\n\n\n\n\n\n\n\n\n\nFitting this model, we observe a curve shape in the smoother, indicating that we are violating the linearity assumption. As the shape is not symmetric, it doesn’t seem like this is can be corrected by using \\(a^2\\). Instead, we can try using a logarithm.\n\n\nCode\n```{r}\n###| y3 = log(a)\nm5 &lt;- lm(y3 ~ log(a), data = data)\n\n###| Residual plot\nplot(m5, which = 1)\n```\n\n\n\n\n\n\n\n\n\nNow, we observe the correct behaviour of the smoother.",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "Modelling/Diagnostics.html#a-good-model-revisited",
    "href": "Modelling/Diagnostics.html#a-good-model-revisited",
    "title": "Model diagnostics",
    "section": "A good model revisited",
    "text": "A good model revisited\nOnce again, we will fit the model \\[y1_i = a_i + b_i + \\epsilon_i.\\] However, we will use the noisy dataset.\n\n\nCode\n```{r}\n###| y1 model\nM1 &lt;- lm(y1 ~ a + b, data = data2)\nplot(M1,which=1)\nplot(M1, which = 2)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNow, there is a lot more variation in the residuals. However, we still observe a roughly flat smoother so the model does not appear to be violating the linearity assumption.\nThe Q-Q plot still follows the ideal general trend, although the tails deviate slightly more.",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "Modelling/Diagnostics.html#a-bad-model-revisited",
    "href": "Modelling/Diagnostics.html#a-bad-model-revisited",
    "title": "Model diagnostics",
    "section": "A bad model revisited",
    "text": "A bad model revisited\nNow, we fit both of the following models: \\[\\begin{align}\ny2_i & = a_i + b_i + \\epsilon_i, \\\\\ny2_i & = a_i + b_i + b_i^2 + \\epsilon_i.\n\\end{align}\\]\n\n\n\n\n\n\nNote\n\n\n\nI have placed the graphs side by side for comparison using the par(mfrow = c(1,2))) command. More information about this, and graphics more generally, appears here.\n\n\n\n\nCode\n```{r}\n###| a + b\nM2 &lt;- lm(y2 ~ a + b, data = data2)\n\n###| a + b + b^2\nM3 &lt;- lm(y2 ~ a + b + I(b^2), data = data2)\n\n###| Plotting graphs side by side\npar(mfrow = c(1, 2))\nplot(M2,which=1)\nplot(M3,which=1)\n```\n\n\n\n\n\n\n\n\n\n\n\nCode\n```{r}\n###| Plotting graphs side by side\npar(mfrow = c(1, 2))\nplot(M2, which = 2)\nplot(M3, which = 2)\n```\n\n\n\n\n\n\n\n\n\nIn the residuals plot, we see the same general trends in behaviour that we saw before. The graph on the left, fitted with \\(a+b\\), displays the problematic parabolic behaviour in the smoother. The graph on the right, fitted with \\(a+b+b^2\\), has a flatter smoother.\nSimilarly, in the Q-Q plots, we observe that the left graph deviates at the tails more than the right graph.",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "Modelling/Diagnostics.html#another-bad-model-revisited",
    "href": "Modelling/Diagnostics.html#another-bad-model-revisited",
    "title": "Model diagnostics",
    "section": "Another bad model revisited",
    "text": "Another bad model revisited\nNow, we fit both of the following models: \\[\\begin{align}\ny2_i & = a_i \\epsilon_i, \\\\\ny2_i & = \\log(a_i) + \\epsilon_i.\n\\end{align}\\]\nAgain, for ease, I have plotted the graphs side by side.\n\n\nCode\n```{r}\n###| a \nM4 &lt;- lm(y3 ~ a, data = data2)\n\n###| log(a)\nM5 &lt;- lm(y3 ~ log(a), data = data2)\n\n###| Plotting graphs side by side\npar(mfrow = c(1, 2))\n\nplot(M4,which=1)\nplot(M5,which=1)\n\nplot(M4, which = 2)\nplot(M5, which = 2)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nThis time, it is much less clear which of these two models is better.\nComparing the Q-Q plots, the left graph appears to deviate more on the lower tail. So, the \\(\\log(a)\\) model appears to better meet the normality assumption.\nHowever, both models have issues with the smoother in the residuals v fitted graphs. For the \\(y3 ~ a\\) model, there is a general curve shape to the smoother and residuals that violates the linearity assumption. While this appears to be fixed somewhat by using \\(\\log(a)\\) instead, now the residuals get further and further away from the smoother and the fitted values get larger. (This shape is known as a megaphone.) This violates the assumption that the errors have constant variance.\nThus, we conclude that it appears neither model is a good fit.",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Model diagnostics"
    ]
  },
  {
    "objectID": "EDA/Cars dataset.html",
    "href": "EDA/Cars dataset.html",
    "title": "cars dataset",
    "section": "",
    "text": "A key step when first reviewing a dataset is to determine the variables and the important properties of each variable. For categorical data, this may be finding the names of the different categories and the count of each category (the number of elements). For continuous data, this may be finding the minimum, maximum, mean and median values.\nThis page will go through some practical examples using the inbuilt mtcars dataset.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "cars dataset"
    ]
  },
  {
    "objectID": "EDA/Cars dataset.html#vs-and-am-variables",
    "href": "EDA/Cars dataset.html#vs-and-am-variables",
    "title": "cars dataset",
    "section": "vs and am variables",
    "text": "vs and am variables\nIn the above summary output, we observe that a mean is given for the vs and am variables. However, from the history about the dataset, we know that these variables can only take values \\(0\\) and \\(1\\). Thus, it would be better for these values to be treated as factors.\nThere are several ways to do this. I give the first example using base R. The second uses the dplyr package, a very handy package for data manipulation.\n\n\nCode\n```{r}\n###| Factoring using base R\ncars_data$am &lt;- factor(cars_data$am)\ncars_data$vs &lt;- factor(cars_data$vs)\n\n###| Summary\nsummary(cars_data)\n```\n\n\n      mpg             cyl             disp             hp       \n Min.   :10.40   Min.   :4.000   Min.   : 71.1   Min.   : 52.0  \n 1st Qu.:15.43   1st Qu.:4.000   1st Qu.:120.8   1st Qu.: 96.5  \n Median :19.20   Median :6.000   Median :196.3   Median :123.0  \n Mean   :20.09   Mean   :6.188   Mean   :230.7   Mean   :146.7  \n 3rd Qu.:22.80   3rd Qu.:8.000   3rd Qu.:326.0   3rd Qu.:180.0  \n Max.   :33.90   Max.   :8.000   Max.   :472.0   Max.   :335.0  \n      drat             wt             qsec       vs     am          gear      \n Min.   :2.760   Min.   :1.513   Min.   :14.50   0:18   0:19   Min.   :3.000  \n 1st Qu.:3.080   1st Qu.:2.581   1st Qu.:16.89   1:14   1:13   1st Qu.:3.000  \n Median :3.695   Median :3.325   Median :17.71                 Median :4.000  \n Mean   :3.597   Mean   :3.217   Mean   :17.85                 Mean   :3.688  \n 3rd Qu.:3.920   3rd Qu.:3.610   3rd Qu.:18.90                 3rd Qu.:4.000  \n Max.   :4.930   Max.   :5.424   Max.   :22.90                 Max.   :5.000  \n      carb      \n Min.   :1.000  \n 1st Qu.:2.000  \n Median :2.000  \n Mean   :2.812  \n 3rd Qu.:4.000  \n Max.   :8.000  \n\n\n\n\nCode\n```{r}\n###| Package requirement\n# install.packages(\"dplyr\")\nlibrary(dplyr)\n```\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\n```{r}\n###| Save the dataset\ncars_dplyr &lt;- mtcars\n\n###| Factoring variables\ncars_dplyr &lt;- cars_dplyr %&gt;%\n  mutate(vs = factor(vs),\n         am = factor(am))\n\n###| Summary of only vs and am columns\nsummary(cars_data[,c(8,9)])\n```\n\n\n vs     am    \n 0:18   0:19  \n 1:14   1:13",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "cars dataset"
    ]
  },
  {
    "objectID": "EDA/Cars dataset.html#observation-names",
    "href": "EDA/Cars dataset.html#observation-names",
    "title": "cars dataset",
    "section": "Observation names",
    "text": "Observation names\nWe can find the names of each row using the rownames() command.\n\n\nCode\n```{r}\n###| Row/observation names\nrownames(cars_data)\n```\n\n\n [1] \"Mazda RX4\"           \"Mazda RX4 Wag\"       \"Datsun 710\"         \n [4] \"Hornet 4 Drive\"      \"Hornet Sportabout\"   \"Valiant\"            \n [7] \"Duster 360\"          \"Merc 240D\"           \"Merc 230\"           \n[10] \"Merc 280\"            \"Merc 280C\"           \"Merc 450SE\"         \n[13] \"Merc 450SL\"          \"Merc 450SLC\"         \"Cadillac Fleetwood\" \n[16] \"Lincoln Continental\" \"Chrysler Imperial\"   \"Fiat 128\"           \n[19] \"Honda Civic\"         \"Toyota Corolla\"      \"Toyota Corona\"      \n[22] \"Dodge Challenger\"    \"AMC Javelin\"         \"Camaro Z28\"         \n[25] \"Pontiac Firebird\"    \"Fiat X1-9\"           \"Porsche 914-2\"      \n[28] \"Lotus Europa\"        \"Ford Pantera L\"      \"Ferrari Dino\"       \n[31] \"Maserati Bora\"       \"Volvo 142E\"         \n\n\nLooking at the list, we can see some cars are made by the same manufacturer. It may be important to consider this in future analysis. So, let us create a new variable, manufacturer, that will account for this.\n\n\nCode\n```{r}\n###| New variable\ncars_data$manufacturer &lt;- factor(rownames(cars_data))\n\n###| Grouping the manufacturers\nlevels(cars_data$manufacturer) &lt;- list(AMC = \"AMC Javelin\", \n                                       Cadillac = \"Cadillac Fleetwood\" ,\n                                       Camaro = \"Camaro Z28\", \n                                       Datsum = \"Datsun 710\",\n                                       Dodge = \"Dodge Challenger\",\n                                       Duster = \"Duster 360\",\n                                       Ferrari = \"Ferrari Dino\",\n                                       Fiat = c(\"Fiat 128\",\"Fiat X1-9\"),\n                                       Ford = \"Ford Pantera L\",\n                                       Honda = \"Honda Civic\",\n                                       Hornet = c(\"Hornet 4 Drive\",\"Hornet Sportabout\"),\n                                       Lincoln = \"Lincoln Continental\",\n                                       Chrysler = \"Chrysler Imperial\",\n                                       Lotus = \"Lotus Europa\",\n                                       Maserati = \"Maserati Bora\",\n                                       Mazda = c(\"Mazda RX4\",\"Mazda RX4 Wag\"),\n                                       Merc = c(\"Merc 230\",\"Merc 240D\",\"Merc 280\",\n                                                \"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\n                                                \"Merc 450SLC\"),\n                                       Pontiac = \"Pontiac Firebird\",\n                                       Porsche = \"Porsche 914-2\",\n                                       Toyota = c(\"Toyota Corolla\",\"Toyota Corona\"),\n                                       Valiant = \"Valiant\",\n                                       Volvo = \"Volvo 142E\")\n\n###| Checking output\ncars_data$manufacturer\n```\n\n\n [1] Mazda    Mazda    Datsum   Hornet   Hornet   Valiant  Duster   Merc    \n [9] Merc     Merc     Merc     Merc     Merc     Merc     Cadillac Lincoln \n[17] Chrysler Fiat     Honda    Toyota   Toyota   Dodge    AMC      Camaro  \n[25] Pontiac  Fiat     Porsche  Lotus    Ford     Ferrari  Maserati Volvo   \n22 Levels: AMC Cadillac Camaro Datsum Dodge Duster Ferrari Fiat Ford ... Volvo\n\n\nNow we have a variable that lists manufacturers. We can check the categories using the levels() command.\n\n\nCode\n```{r}\n###| Levels()\nlevels(cars_data$manufacturer)\n```\n\n\n [1] \"AMC\"      \"Cadillac\" \"Camaro\"   \"Datsum\"   \"Dodge\"    \"Duster\"  \n [7] \"Ferrari\"  \"Fiat\"     \"Ford\"     \"Honda\"    \"Hornet\"   \"Lincoln\" \n[13] \"Chrysler\" \"Lotus\"    \"Maserati\" \"Mazda\"    \"Merc\"     \"Pontiac\" \n[19] \"Porsche\"  \"Toyota\"   \"Valiant\"  \"Volvo\"",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "cars dataset"
    ]
  },
  {
    "objectID": "EDA/Cars dataset.html#should-it-be-categorical-or-continuous",
    "href": "EDA/Cars dataset.html#should-it-be-categorical-or-continuous",
    "title": "cars dataset",
    "section": "Should it be categorical or continuous?",
    "text": "Should it be categorical or continuous?\nThere a few clear cut instances in the cars dataset where the variables should be categorical. We have addressed these: vm, am and manufacturer. However, there are instances where it is less clear.\nFor example, consider the cylinders variable. In our data, we have observations with 3 distinct categories: 4, 6 and 8. (We can find this with unique() function.)\n\n\nCode\n```{r}\n###| Cylinders\nunique(cars_data$cyl)\n```\n\n\n[1] 6 4 8\n\n\nThus, we could argue we should treat cylinders as categorical data. However, treating cylinders as categorical removes the ability to treat this as a single variable. As one variable, we could test how increasing the number of cylinders impacts performance.\nWhen we come to modelling, we should test models with cylinders treated as a continuous and a categorical variable.",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "cars dataset"
    ]
  },
  {
    "objectID": "EDA/summary tables.html",
    "href": "EDA/summary tables.html",
    "title": "dplyr: summary tables",
    "section": "",
    "text": "We begin by loading in the cars dataset we used on the previous page (the code required is available below if needed). Please note that we want to use the version that has the manufacturers included!\n\n\nCode\n```{r}\n#| code-fold: true\n\n###| Code to create the dataset used in previous pages\ncars_data &lt;- mtcars\n\ncars_data$am &lt;- factor(cars_data$am)\ncars_data$vs &lt;- factor(cars_data$vs)\n\ncars_data$manufacturer &lt;- factor(rownames(cars_data))\nlevels(cars_data$manufacturer) &lt;- list(AMC = \"AMC Javelin\", \n                                       Cadillac = \"Cadillac Fleetwood\" ,\n                                       Camaro = \"Camaro Z28\", \n                                       Datsum = \"Datsun 710\",\n                                       Dodge = \"Dodge Challenger\",\n                                       Duster = \"Duster 360\",\n                                       Ferrari = \"Ferrari Dino\",\n                                       Fiat = c(\"Fiat 128\",\"Fiat X1-9\"),\n                                       Ford = \"Ford Pantera L\",\n                                       Honda = \"Honda Civic\",\n                                       Hornet = c(\"Hornet 4 Drive\",\"Hornet Sportabout\"),\n                                       Lincoln = \"Lincoln Continental\",\n                                       Chrysler = \"Chrysler Imperial\",\n                                       Lotus = \"Lotus Europa\",\n                                       Maserati = \"Maserati Bora\",\n                                       Mazda = c(\"Mazda RX4\",\"Mazda RX4 Wag\"),\n                                       Merc = c(\"Merc 230\",\"Merc 240D\",\"Merc 280\",\n                                                \"Merc 280C\",\"Merc 450SE\",\"Merc 450SL\",\n                                                \"Merc 450SLC\"),\n                                       Pontiac = \"Pontiac Firebird\",\n                                       Porsche = \"Porsche 914-2\",\n                                       Toyota = c(\"Toyota Corolla\",\"Toyota Corona\"),\n                                       Valiant = \"Valiant\",\n                                       Volvo = \"Volvo 142E\")\n```\n\n\n\n\nCode\n```{r}\n###| Checking the data is correct\nhead(cars_data)\n```\n\n\n                   mpg cyl disp  hp drat    wt  qsec vs am gear carb\nMazda RX4         21.0   6  160 110 3.90 2.620 16.46  0  1    4    4\nMazda RX4 Wag     21.0   6  160 110 3.90 2.875 17.02  0  1    4    4\nDatsun 710        22.8   4  108  93 3.85 2.320 18.61  1  1    4    1\nHornet 4 Drive    21.4   6  258 110 3.08 3.215 19.44  1  0    3    1\nHornet Sportabout 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2\nValiant           18.1   6  225 105 2.76 3.460 20.22  1  0    3    1\n                  manufacturer\nMazda RX4                Mazda\nMazda RX4 Wag            Mazda\nDatsun 710              Datsum\nHornet 4 Drive          Hornet\nHornet Sportabout       Hornet\nValiant                Valiant\n\n\n\nA basic summary table\nTo create a summary table, we could manually calculate every entry and then create a dataset of these values and then create the table. However, it is much quicker (and easier) to use the summarise() function that comes with the dplyr package.\n\n\n\n\n\n\nNote\n\n\n\nThe dplyr package will be installed/active if you have tidyverse. Alternatively, you can directly install and activate the package with the following commands:\n\n\nCode\n```{r}\n#| warning: false # will remove output messages\n\n###| Package downloading and activating\n# install.packages(\"dplyr\")\nlibrary(dplyr)\n```\n\n\nIf you are using RStudio, you can check which packages are installed using the bottom right window.\n\n\nThe following code gives some examples of how to extract various summary statistics using the summarise() command.\n\n\nCode\n```{r}\n###| Basic summary table on continuous data\ncars_data %&gt;%\n  summarise(\"Num. observations\" = n(),\n            \"Mean mpg\" = mean(mpg),\n            \"Range displacement\" = max(disp)-min(disp),\n            \"Median horsepower\" = median(hp))\n```\n\n\n  Num. observations Mean mpg Range displacement Median horsepower\n1                32 20.09062              400.9               123\n\n\nThings get more complicated for categorical factors. We may need to use the length(), which() and unique() functions to find our desired statistics.\n\n\nCode\n```{r}\n###| Basic summary table on categorical data\ncars_data %&gt;%\n  summarise(\"vs 1\" = length(which(vs == 1)),\n            \"am 0\" = length(which(am == 0)),\n            \"Num. manufacturers\" = length(unique(manufacturer))\n  )\n```\n\n\n  vs 1 am 0 Num. manufacturers\n1   14   19                 22\n\n\n\n\nCylinder level exploration\nRecall that we identified 3 distinct cylinder levels (check using unique(cars_data$cyl)). To do this, we will use the group_by() function.\n\n\nCode\n```{r}\n###| Basic summary table grouped by cylinders\ncars_data %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(\"Num. observations\" = n(),\n            \"Mean mpg\" = mean(mpg),\n            \"Range displacement\" = max(disp)-min(disp),\n            \"Median horsepower\" = median(hp),\n            \"vs 1\" = length(which(vs == 1)),\n            \"am 0\" = length(which(am == 0)),\n            \"Num. manufacturers\" = length(unique(manufacturer))\n  )\n```\n\n\n# A tibble: 3 × 8\n    cyl `Num. observations` `Mean mpg` `Range displacement` `Median horsepower`\n  &lt;dbl&gt;               &lt;int&gt;      &lt;dbl&gt;                &lt;dbl&gt;               &lt;dbl&gt;\n1     4                  11       26.7                 75.6                 91 \n2     6                   7       19.7                113                  110 \n3     8                  14       15.1                196.                 192.\n# ℹ 3 more variables: `vs 1` &lt;int&gt;, `am 0` &lt;int&gt;, `Num. manufacturers` &lt;int&gt;\n\n\n\n\n\n\n\n\nImportant\n\n\n\nIt is important to note that this summary table had \\(3\\) rows and not \\(1\\)! The statistics in each row correspond to the cars that had X number of cylinders. For example, of the \\(11\\) cars with \\(4\\) cylinders, the mean mpg was \\(26.6\\), the displacement range \\(75.6\\) etc…\n\n\nWhen reading off this table, we can note a few things about the different types of cars:\n\n\nFewer cars had \\(6\\) cylinders than \\(4\\) or \\(8\\).\nAs the number of cylinders increased, the mean mpg decreased, the displacement range increased, and the median horsepower increased.\nNo \\(8\\) cylinder cars had vs 1.\nThere is no clear pattern in the am 0 or manufacturer identifiable from this table.\n\n\n\n\nMaking the tables more readable\nThus far, we have relied on the raw R output to read the tables. This is not ideal for reports or long periods of work. Instead, we can use the kable() function from the kableExtra package for neat tables.\n\n\nCode\n```{r}\n###| Package\n# install.packages(\"kableExtra\")\nlibrary(kableExtra)\n```\n\n\n\nAttaching package: 'kableExtra'\n\n\nThe following object is masked from 'package:dplyr':\n\n    group_rows\n\n\nCode\n```{r}\n###| Grouped summary table \nsummary &lt;- cars_data %&gt;%\n  group_by(cyl) %&gt;%\n  summarise(\"Num. observations\" = n(),\n            \"Mean mpg\" = mean(mpg),\n            \"Range displacement\" = max(disp)-min(disp),\n            \"Median horsepower\" = median(hp),\n            \"vs 1\" = length(which(vs == 1)),\n            \"am 0\" = length(which(am == 0)),\n            \"Num. manufacturers\" = length(unique(manufacturer))\n  )\n\n###| Output\nkable(summary)\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\ncyl\nNum. observations\nMean mpg\nRange displacement\nMedian horsepower\nvs 1\nam 0\nNum. manufacturers\n\n\n\n\n4\n11\n26.66364\n75.6\n91.0\n10\n3\n8\n\n\n6\n7\n19.74286\n113.0\n110.0\n4\n4\n5\n\n\n8\n14\n15.10000\n196.2\n192.5\n0\n12\n12\n\n\n\n\n\nWe can continue to change various arguments of the kable function to get truly beautiful tables. An excellent site for this is LINK. A quick example of some commands are the following:\n\n\nCode\n```{r}\n###| Example kable extra\nkable(summary, booktabs = TRUE, \n      col.names = c(\"Cylinders\",\"Total\",\"Mean MPG\",\"Displacement range\",\n                    \"Median horsepower\",\"Num vs 1\",\"Num am 0\",\n                    \"Num unique manufacturers\")) %&gt;%\n  add_header_above(c(\" \" = 1, \"Summary Statistics\" = 7))\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nSummary Statistics\n\n\n\nCylinders\nTotal\nMean MPG\nDisplacement range\nMedian horsepower\nNum vs 1\nNum am 0\nNum unique manufacturers\n\n\n\n\n4\n11\n26.66364\n75.6\n91.0\n10\n3\n8\n\n\n6\n7\n19.74286\n113.0\n110.0\n4\n4\n5\n\n\n8\n14\n15.10000\n196.2\n192.5\n0\n12\n12",
    "crumbs": [
      "Home",
      "Exploratory data analysis",
      "dplyr: summary tables"
    ]
  },
  {
    "objectID": "Modelling/Linear modelling intro.html",
    "href": "Modelling/Linear modelling intro.html",
    "title": "Introduction",
    "section": "",
    "text": "This section will begin by introducing a linear model, the key assumptions we must make to fit such a model and model diagnostics.\nSpecifically, the pages cover the following:\n\n\nWhat is a linear model?\n\nModel assumptions, with extra notes on linearity\nPrinciple of parsimony\n\nA continuous data example\n\nUsing lm() and anova()\n\nA categorical data example\n\nIntercepts and reference terms\nInteraction terms\n\nCheck your understanding by fitting various models on the data.csv and coursework.csv files.",
    "crumbs": [
      "Home",
      "Linear modelling",
      "Introduction"
    ]
  },
  {
    "objectID": "Data/Raw data.html#data.csv",
    "href": "Data/Raw data.html#data.csv",
    "title": "Data",
    "section": "data.csv",
    "text": "data.csv\n\n\nCode\n```{r}\n###| data.csv data\ndata &lt;- data.frame(\"x1\" = c(2.37, 8.38, 7.70, 5.52, 4.41, 5.98, 3.53, 2.28, 2.88, 5.91, 6.88, 2.75, 8.17, 3.38, 8.39, 3.36, 9.53, 3.61, 7.80, 2.20, 9.04, 5.21, 3.96, 9.07, 4.90, 9.27, 6.81, 6.64, 5.26, 8.31, 6.27, 5.17, 5.37, 3.25, 2.60, 4.68, 8.11, 4.82, 5.15, 4.24, 9.17, 4.48, 9.45, 4.99, 4.84, 9.83, 8.19, 2.56, 2.89, 9.75, 9.78, 7.12, 7.41, 6.30, 8.51, 6.65, 3.19, 5.43, 9.14, 7.60, 7.12, 9.26, 2.42, 9.65, 7.43, 2.67, 2.32, 9.05, 4.44, 7.25, 5.85, 2.26, 2.31, 2.10, 5.88, 8.54, 8.32, 4.32, 2.78, 5.69, 3.67, 2.38, 6.95, 6.19, 7.02, 4.96, 9.28, 7.69, 2.32, 6.96, 6.61, 4.63, 5.66, 3.22, 5.88, 4.41, 8.54, 8.73, 5.41, 8.25),\n                   \"x2\" = c(20.41, 4.34, 8.42, 3.59, 10.91, 9.56, 12.69, 138.84, 23.59, 2.31, 11.70, 59.57, 4.87, 18.75, 2.93, 17.42, 5.20, 5.28, 3.87, 20.17, 3.96, 10.24, 18.88, 3.98, 26.57, 4.15, 6.16, 5.98, 11.39, 3.53, 12.69, 6.26, 10.55, 42.94, 43.11, 52.78, 2.40, 17.09, 9.06, 21.93, 2.74, 10.55, 4.15, 23.82, 13.05, 1.82, 3.47, 31.44, 19.23, 3.43, 1.95, 12.40, 6.09, 3.36, 8.02, 8.91, 35.90, 15.56, 1.27, 5.13, 5.02, 2.90, 40.68, 6.46, 5.82, 12.67, 16.56, 1.34, 23.58, 9.46, 5.93, 13.97, 58.33, 61.60, 5.84, 2.96, 4.49, 8.51, 40.62, 5.56, 14.71, 25.94, 3.14, 13.14, 2.35, 22.47, 2.36, 2.05, 40.24, 6.95, 14.36, 8.20, 7.19, 24.89, 6.31, 28.04, 4.33, 5.26, 17.58, 15.15),\n                  \"y\" =  c(6.53, 10.72, 10.18,  8.85, 8.31, 9.29, 8.06, 6.53, 7.21, 9.28, 9.91, 7.04, 10.49, 7.67, 10.64, 7.86, 11.43, 8.13, 10.48, 6.30, 11.18, 8.71, 7.97, 11.20, 8.57, 10.90, 9.40, 9.51, 8.80, 10.54, 9.66,8.83,  9.12,  7.25,  6.52,  8.54, 10.99,  8.90 , 8.83 , 8.08, 11.07, 8.62, 11.03, 8.43, 8.68, 11.18, 10.58, 6.93, 7.08, 11.41, 11.27, 9.96, 10.48, 9.75, 10.66, 9.85, 7.24, 9.04, 11.18, 10.32, 10.14, 11.04, 6.49, 11.41, 10.06, 7.15, 6.65, 10.91, 8.51, 9.83, 9.42, 6.54, 6.90, 6.36, 9.07, 10.81, 10.91, 8.04, 7.15, 9.14, 7.60, 6.59, 9.97, 9.24, 9.95, 8.35, 11.03, 10.61, 6.57, 9.66, 10.09, 8.74, 9.03, 7.28, 9.52, 8.45, 10.79, 11.08, 9.09, 10.50))\n```"
  },
  {
    "objectID": "Data/Raw data.html#coursework.csv",
    "href": "Data/Raw data.html#coursework.csv",
    "title": "Data",
    "section": "coursework.csv",
    "text": "coursework.csv\n\n\nCode\n```{r}\n###| coursework data\ncoursework &lt;- data.frame(\"cw\" = c(5,5,10,12,14,16),\n                         \"exam\" = c(38,45,69,65,75,96))\n```"
  },
  {
    "objectID": "Mathematics to Code/Mathematics to Code NewtonRaph Example.html",
    "href": "Mathematics to Code/Mathematics to Code NewtonRaph Example.html",
    "title": "Newton–Raphson Method — Translating Math into Code",
    "section": "",
    "text": "Mathematical Intuition\nWe want to find a root of (\\(f(x)=0\\)).\nWe use the tangent line at (\\(x_n\\)) to estimate a better guess:\n\\(x_{n+1} = x_n - \\frac{f(x_n)}{f'(x_n)}\\).\nRepeat until ( \\(\\|x_{n+1} - x_n\\|\\) ) is small.\n\n\n\n\n\n\n\n\n\n\n\n\nMath → Algorithm → Code Mapping\n\n\n\nMath concept\nR translation\n\n\n\n\nupdate rule\nx_new &lt;- x - f(x)/df(x)\n\n\nderivative\nfunction df(x)\n\n\nconvergence\nabs(x_new - x) &lt; tol\n\n\n\n\n\n\n\n\n\nflowchart TD\n  Start([Start: choose x0, tol, max_iter])\n  Compute[\"x_next with the algorithm\"]\n  Check{Is the absolute difference between &lt;br/&gt; the next guess x_next and &lt;br/&gt; the true root x less than the tolerance}\n  Accept([Return x_next])\n  Update[\"x &lt;- x_next\"]\n\n  Start --&gt; Compute --&gt; Check\n  Check -- Yes --&gt; Accept\n  Check -- No --&gt; Update --&gt; Compute\n\n\n\n\n\n\n\n\n\nImplementation in R\n\n\nCode\n```{r}\nnewton &lt;- function(f, df, x0, tol = 1e-10, max_iter = 100) {\n  x &lt;- x0\n  for (i in 1:max_iter) {\n    fx &lt;- f(x)\n    dfx &lt;- df(x)\n    if (dfx == 0) stop(\"Derivative is zero; choose a new x0\")\n    x_new &lt;- x - fx / dfx\n    if (abs(x_new - x) &lt; tol) return(x_new)\n    x &lt;- x_new\n  }\n  stop(\"Did not converge\")\n}\n\n# Examples\nf1 &lt;- function(x) x^2 - 2\ndf1 &lt;- function(x) 2*x\nnewton(f1, df1, x0 = 1)\n```\n\n\n[1] 1.414214\n\n\n\n\n\nReflection\nThe Newton–Raphson method shows how an iterative analytic idea (tangent approximation) translates into a computational fixed-point loop.",
    "crumbs": [
      "Home",
      "Mathematics to R",
      "Newton–Raphson Method — Translating Math into Code"
    ]
  },
  {
    "objectID": "Data Visualisation/advice.html",
    "href": "Data Visualisation/advice.html",
    "title": "Best Practices",
    "section": "",
    "text": "In this section, we go through what makes a good data visualization. These come from the lecture material for ST231, ST237, ST346, and ST404.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Best Practices"
    ]
  },
  {
    "objectID": "Data Visualisation/advice.html#labeling",
    "href": "Data Visualisation/advice.html#labeling",
    "title": "Best Practices",
    "section": "Labeling",
    "text": "Labeling\nEverything on your graph should be clearly labeled. In general, a graph contains:\n\ntitle — a clear short title letting the reader know what they’re looking at\nExample: Relationship between experience and wages by gender\ncaption — a short description of the graph and, where necessary, the source of the data\nExample: A star plot showing the different characteristics of reviewed wines. Source: https://www.kaggle.com/datasets/zynicide/wine-reviews/data\naxis labels — clear labels for the x and y axes\n\nshort but descriptive\ninclude units of measurement\nExamples:\n\nTree diameter (meters)\nSurvival time (days)\nPatient age (years)\n\n\nlegend — short informative title and labels\nExample: You can do this with the legend() function in R before or after the plot() you with to add a legend to\nlines and bars — label any trend lines, annotation lines, and error bars\n\nEssentially, the reader should be able to understand your graph without having to wade through paragraphs of text. When in doubt, show your data visualization to someone unfamiliar with your assignment and ask them if anything is unclear.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Best Practices"
    ]
  },
  {
    "objectID": "Data Visualisation/advice.html#keep-it-simple",
    "href": "Data Visualisation/advice.html#keep-it-simple",
    "title": "Best Practices",
    "section": "Keep it Simple!",
    "text": "Keep it Simple!\nIn data science, the goal of data visualization is to communicate information. Anything that doesn’t support this goal should be reduced or eliminated entirely.\n\nChart Junk — visual elements of charts that aren’t necessary to comprehend the information represented by the chart or that distract from this information.\n\nLet us now go through an example of chart junk.\n\nGraph with chart junk\n\n\nCode\n# Import relevant libraries\nlibrary(ggplot2)\nlibrary(dplyr)\n\n# Setup of the dataframe for plotting use\nmpg_summary &lt;- mtcars |&gt;\nmutate(cyl = factor(cyl)) |&gt;\ngroup_by(cyl) |&gt;\nsummarise(mean_mpg = mean(mpg))\n\n\n\n\nCode\n# LOTS of plotting elements from ggplot2, see the page about ggplot2 for more background on these\nggplot(mpg_summary, aes(x = cyl, y = mean_mpg, fill = cyl)) +\ngeom_col(color = \"black\", width = 0.8) +\nlabs(\ntitle = \"Different Mean Miles Per Gallon for Cars with Different Numbers of Cylinders\",\nx = \"Number of Cylinders\",\ny = \"Mean Miles Per Gallon\",\nfill = \"Cylinder Groups\"\n) +\ntheme(\npanel.background = element_rect(fill = \"lightblue\"),\npanel.border = element_rect(color = \"darkblue\", fill = NA, linewidth = 2),\npanel.grid.major = element_line(color = \"black\"),\npanel.grid.minor = element_line(color = \"black\"),\nplot.title = element_text(size = 16, face = \"bold\")\n)\n\n\n\n\n\nGraph with chart junk from the mtcars dataset\n\n\n\n\nIf the goal is to compare the fuel efficiency of cars with different numbers of cylinders, much of this visualization is unnecessary and distracts from the task: :::\n\nColored background and thick border draw attention away from the bars\nHeavy black grid lines dominate the data\nColor fill adds a legend but doesn’t encode new information\nWordy title (“different”, “numbers of”) adds clutter\nThe reader must constantly look back and forth between the bars and y-axis\n\n:::\nQUESTION FOR THE READER\nLet’s now try to clean up. But before we do, how would YOU clean this up?\n\n\nGraph with chart junk removed\n\n\nCode\n# A better plot using ggplot2 again\nggplot(mpg_summary, aes(x = cyl, y = mean_mpg)) +\ngeom_col(fill = \"grey70\", width = 0.6) +\ngeom_col(\ndata = filter(mpg_summary, cyl == \"8\"),\nfill = \"#D55E00\",\nwidth = 0.6\n) +\ngeom_text(\naes(label = round(mean_mpg, 1)),\nvjust = -0.5,\nsize = 4\n) +\nlabs(\ntitle = \"Average fuel efficiency by number of cylinders\",\ny = \"Miles per gallon\",\nx = NULL\n) +\ntheme_minimal() +\ntheme(\npanel.grid.major.x = element_blank(),\npanel.grid.minor = element_blank()\n)\n\n\n\n\n\nGraph with chart junk removed from the mtcars dataset\n\n\n\n\nThe chart junk has been removed. In addition:\n\nX-axis label removed (cylinder categories are obvious)\nY-axis label made more informative\nthe title has been simplified\nOne bar (8 cylinders) highlighted to aid comparison\nValues added directly to bars, reducing reliance on the axis\nGrid lines softened or removed to avoid distraction\n\nI may or may not have gone a bit far leaving out the x-axis label. It’s a fine line, knowing when to stop simplifying.\nIn general, you want to reduce chart junk to a minimum. In other words, more signal, less noise. At the end of the day, the data visualisation is an art specific to every data scientist, but there is good and bad art!",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Best Practices"
    ]
  },
  {
    "objectID": "Data Visualisation/advice.html#color-choice",
    "href": "Data Visualisation/advice.html#color-choice",
    "title": "Best Practices",
    "section": "Color choice",
    "text": "Color choice\nColor choice is about more than aesthetics. Choose colors that help convey the information contained in the plot.\nThe article How to Choose Colors for Data Visualizations by Mike Yi (link) is a great place to start.\nBasically, think about selecting among sequential, diverging, and qualitative color schemes:\n:::\n\nSequential — for plotting a quantitative variable that goes from low to high\nDiverging — for contrasting the extremes (low, medium, and high) of a quantitative variable\nQualitative — for distinguishing among the levels of a categorical variable\n\n:::\nThe article above can help you to choose among these schemes.\nOther things to keep in mind:\n:::\n\nMake sure that text is legible — avoid dark text on dark backgrounds, light text on light backgrounds, and colors that clash in a discordant fashion (i.e., they hurt to look at!)\nAvoid combinations of red and green — it can be difficult for a colorblind audience to distinguish these colors\n\n:::",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Best Practices"
    ]
  },
  {
    "objectID": "Data Visualisation/advice.html#y-axis-scaling",
    "href": "Data Visualisation/advice.html#y-axis-scaling",
    "title": "Best Practices",
    "section": "y-Axis scaling",
    "text": "y-Axis scaling\nThis is a big one. You can make an effect seem massive or insignificant depending on how you scale a numeric y-axis.\nConsider the following example comparing the 9-month salaries of male and female assistant professors. The data come from the Academic Salaries dataset.\n\n\nCode\n# More library imports\nlibrary(carData)\nlibrary(scales)\nlibrary(patchwork)\n\n# Setup of the dataset to be used\ndata(Salaries, package = \"carData\")\n\ndf &lt;- Salaries %&gt;%\n  filter(rank == \"AsstProf\") %&gt;%\n  group_by(sex) %&gt;%\n  summarize(\n    n = n(),\n    mean = mean(salary),\n    sd = sd(salary),\n    se = sd / sqrt(n),\n    ci = qt(0.975, df = n - 1) * se\n  )\n\ndf\n\n\n# A tibble: 2 × 6\n  sex        n   mean    sd    se    ci\n  &lt;fct&gt;  &lt;int&gt;  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Female    11 78050. 9372. 2826. 6296.\n2 Male      56 81311. 7901. 1056. 2116.\n\n\nNow let us plot the data:\n\n\nCode\n# Create a base plot\np &lt;- ggplot(df, aes(x = sex, y = mean, group = 1)) +\n  geom_point(size = 4) +\n  geom_line() +\n  labs(\n    title = \"Mean salary differences by gender\",\n    subtitle = \"9-month academic salary, 2007–2008\",\n    caption = paste(\n      \"Source: Fox J. and Weisberg S. (2011)\",\n      \"An R Companion to Applied Regression,\",\n      \"Second Edition, Sage\"\n    ),\n    x = \"Gender\",\n    y = \"Salary\"\n  ) +\n  scale_y_continuous(labels = scales::dollar)\n\np\n\n\n\n\n\n\n\n\n\nGreat, now what if we plot the same data, but with the y-axis ranging from 77,000 to 82,000?\n\n\nCode\n# Using ggplot2's feature of \"adding\" and \"subtracting\" features!\np + scale_y_continuous(limits = c(77000, 82000))\n\n\n\n\n\nPlot with limited range of Y\n\n\n\n\nThere now appears to be a very large gender difference! Now let us try to plot with the y-axis ranging from 0 to 125,000:\n\n\nCode\n# Same as the above code snippet!\np + scale_y_continuous(limits = c(0, 125000))\n\n\n\n\n\nPlot with wider range of Y\n\n\n\n\nThere doesn’t appear to be any gender difference! The goal of ethical data visualization is to represent findings with as little distortion as possible. This means choosing an appropriate range for the y-axis. Bar charts should almost always start at y = 0. For other charts, the limits really depend on subject matter knowledge of the expected range of values.\n\nPlots with other scales\nSome data might look better if we plot them on a different y-axis scale! Let us look at the airquality dataset:\n\n\nCode\n# Setup of the new dataset we will be using\nozone_summary &lt;- airquality |&gt;\nfilter(!is.na(Ozone)) |&gt;\ngroup_by(Month) |&gt;\nsummarise(\nmean = mean(Ozone),\nsd = sd(Ozone),\nn = n(),\nse = sd / sqrt(n),\nci = qt(0.975, df = n - 1) * se\n)\n\n\n\n\nCode\n# Using ggplot for the linear and log scale plots\np_linear &lt;- ggplot(ozone_summary, aes(x = factor(Month), y = mean)) +\ngeom_col(fill = \"grey70\", width = 0.6) +\ngeom_errorbar(\naes(ymin = mean - ci, ymax = mean + ci),\nwidth = 0.1\n) +\nlabs(\ntitle = \"Mean ozone concentration by month\",\nsubtitle = \"Linear scale: comparisons reflect absolute differences\",\nx = NULL,\ny = \"Ozone concentration\"\n) +\ntheme_minimal()\n\np_log &lt;- ggplot(ozone_summary, aes(x = factor(Month), y = mean)) +\ngeom_col(fill = \"grey70\", width = 0.6) +\ngeom_errorbar(\naes(ymin = mean - ci, ymax = mean + ci),\nwidth = 0.1\n) +\nscale_y_log10() +\nlabs(\nsubtitle = \"Log scale: equal distances represent equal ratios (percentage changes)\",\nx = \"Month\",\ny = \"Ozone concentration (log scale)\"\n) +\ntheme_minimal()\n\n# Combining the two plots into one through the patchwork package\np_linear / p_log\n\n\n\n\n\nPlot showing the difference between the normal and log scale\n\n\n\n\nThe upper panel uses a linear scale, where visual differences correspond to absolute changes in ozone concentration. The lower panel uses a logarithmic scale, where equal distances represent equal ratios rather than equal differences. For this right-skewed data, the log scale facilitates comparison in relative terms and reduces the dominance of large values.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Best Practices"
    ]
  },
  {
    "objectID": "Data Visualisation/advice.html#further-resources",
    "href": "Data Visualisation/advice.html#further-resources",
    "title": "Best Practices",
    "section": "Further Resources",
    "text": "Further Resources\nIf you would like to learn more about ggplot2 there are several good sources included in the ggplot2 page!\nIf you would like to learn more about data visualization in general, here are some useful resources:\n\nScott Berinato’s Harvard Business Review article Visualizations that really work https://hbr.org/2016/06/visualizations-that-really-work\nWall Street Journal’s guide to information graphics: The dos and don’ts of presenting data, facts and figures (Wong 2010)\nA Practical Guide to Graphics Reporting: Information graphics for print, web & broadcast (George-Palilonis 2017)\nBeautiful Data: The stories behind elegant data solutions (Hammerbacher and Jeff 2009)\nThe Truthful Art: Data, charts, and maps for communication (Cairo 2016)\nThe Information is Beautiful website: https://informationisbeautiful.net\n\nThe best graphs are rarely created on the first attempt. Experiment until you have a visualization that clarifies the data and helps communicate a meaningful story. And have fun!",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Best Practices"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html",
    "href": "Data Visualisation/baseR and ggplot.html",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "",
    "text": "This section offers a primer on how the ggplot2 package operates. Unlike many other plotting systems, ggplot2 is based on the “Grammar of Graphics,” which allows you to build graphs layer by layer. We will construct a complex visualization starting from a blank canvas and adding elements—like data points, trend lines, and labels—one step at a time.\nLet’s start with an example.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html",
    "href": "Data Visualisation/importing data.html",
    "title": "Data Import and Preparation in R",
    "section": "",
    "text": "Before you can visualize or analyze data, you need to get it into R and prepare it properly. This step is crucial, even the most sophisticated analysis will fail if your data isn’t prepared correctly. Think of it like preparing ingredients before cooking: skipping or rushing this step will affect everything that follows.\nIn this guide, we’ll learn how to import data from various file formats and clean it for analysis. We’ll use the palmerpenguins dataset for most examples, which contains measurements of penguins from Antarctica.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#text-and-csv-files",
    "href": "Data Visualisation/importing data.html#text-and-csv-files",
    "title": "cars dataset",
    "section": "Text and CSV files",
    "text": "Text and CSV files\nThe readr package provides functions for importing delimited text files into R data frames.\n\n\nCode\n```{r,  eval=FALSE}\nlibrary(readr)\n\n# import data from a comma delimited file\nSalaries &lt;- read_csv(\"salaries.csv\")\n\n# import data from a tab delimited file\nSalaries &lt;- read_tsv(\"salaries.txt\")\n```\n\n\nThese function assume that the first line of data contains the variable names, values are separated by commas or tabs respectively, and that missing data are represented by blanks. For example, the first few lines of the comma delimited file looks like this.\n“rank”,“discipline”,“yrs.since.phd”,“yrs.service”,“sex”,“salary” “Prof”,“B”,19,18,“Male”,139750 “Prof”,“B”,20,16,“Male”,173200 “AsstProf”,“B”,4,3,“Male”,79750 “Prof”,“B”,45,39,“Male”,115000 “Prof”,“B”,40,41,“Male”,141500 “AssocProf”,“B”,6,6,“Male”,97000\nOptions allow you to alter these assumptions. See the ?read_delim for more details. https://www.rdocumentation.org/packages/readr/versions/0.1.1/topics/read_delim"
  },
  {
    "objectID": "Data Visualisation/importing data.html#other-file-types",
    "href": "Data Visualisation/importing data.html#other-file-types",
    "title": "cars dataset",
    "section": "Other File Types",
    "text": "Other File Types\nThe readxl package can import data from Excel workbooks. Both .xls and .xlsx formats are supported.\n\n\nCode\n```{r,  eval=FALSE}\nlibrary(readxl)\n\n# import data from an Excel workbook\nSalaries &lt;- read_excel(\"salaries.xlsx\", sheet=1)\n```\n\n\nSince workbooks can have more than one worksheet, you can specify the one you want with the sheet option. The default is sheet=1.\nThe haven package provides functions for importing data from a variety of statistical packages without needing to have the packages themselves installed on your machine.\n\n\nCode\n```{r,  eval=FALSE}\nlibrary(haven)\n\n# import data from Stata\nSalaries &lt;- read_dta(\"salaries.dta\")\n\n# import data from SPSS\nSalaries &lt;- read_sav(\"salaries.sav\")\n\n# import data from SAS\nSalaries &lt;- read_sas(\"salaries.sas7bdat\")\n```"
  },
  {
    "objectID": "Data Visualisation/importing data.html#select-function",
    "href": "Data Visualisation/importing data.html#select-function",
    "title": "cars dataset",
    "section": "select function",
    "text": "select function\nThe select function allows you to limit your dataset to specified variables (columns).\n\n\nCode\n```{r}\nlibrary(dplyr)\n```\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\n```{r}\n# keep the variables name, height, and gender\nnewdata &lt;- select(starwars, name, height, gender)\n\nhead(newdata)\n\n# keep the variables name and all variables \n# between mass and species inclusive\nnewdata &lt;- select(starwars, name, mass:species)\n\nhead(newdata)\n\n# keep all variables except birth_year and gender\nnewdata &lt;- select(starwars, -birth_year, -gender)\n\nhead(newdata)\n```\n\n\n# A tibble: 6 × 3\n  name           height gender   \n  &lt;chr&gt;           &lt;int&gt; &lt;chr&gt;    \n1 Luke Skywalker    172 masculine\n2 C-3PO             167 masculine\n3 R2-D2              96 masculine\n4 Darth Vader       202 masculine\n5 Leia Organa       150 feminine \n6 Owen Lars         178 masculine\n# A tibble: 6 × 10\n  name    mass hair_color skin_color eye_color birth_year sex   gender homeworld\n  &lt;chr&gt;  &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    \n1 Luke …    77 blond      fair       blue            19   male  mascu… Tatooine \n2 C-3PO     75 &lt;NA&gt;       gold       yellow         112   none  mascu… Tatooine \n3 R2-D2     32 &lt;NA&gt;       white, bl… red             33   none  mascu… Naboo    \n4 Darth…   136 none       white      yellow          41.9 male  mascu… Tatooine \n5 Leia …    49 brown      light      brown           19   fema… femin… Alderaan \n6 Owen …   120 brown, gr… light      blue            52   male  mascu… Tatooine \n# ℹ 1 more variable: species &lt;chr&gt;\n# A tibble: 6 × 12\n  name      height  mass hair_color skin_color eye_color sex   homeworld species\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;     &lt;chr&gt; &lt;chr&gt;     &lt;chr&gt;  \n1 Luke Sky…    172    77 blond      fair       blue      male  Tatooine  Human  \n2 C-3PO        167    75 &lt;NA&gt;       gold       yellow    none  Tatooine  Droid  \n3 R2-D2         96    32 &lt;NA&gt;       white, bl… red       none  Naboo     Droid  \n4 Darth Va…    202   136 none       white      yellow    male  Tatooine  Human  \n5 Leia Org…    150    49 brown      light      brown     fema… Alderaan  Human  \n6 Owen Lars    178   120 brown, gr… light      blue      male  Tatooine  Human  \n# ℹ 3 more variables: films &lt;list&gt;, vehicles &lt;list&gt;, starships &lt;list&gt;"
  },
  {
    "objectID": "Data Visualisation/importing data.html#selecting-observations",
    "href": "Data Visualisation/importing data.html#selecting-observations",
    "title": "cars dataset",
    "section": "Selecting Observations",
    "text": "Selecting Observations\nThe filter function allows you to limit your dataset to observations (rows) meeting a specific criteria. Multiple criteria can be combined with the & (AND) and | (OR) symbols.\n\n\nCode\n```{r}\n# select females\nnewdata &lt;- filter(starwars, \n                  sex == \"female\")\n\nhead(newdata)\n\n# select females that are from Alderaan\nnewdata &lt;- filter(starwars, \n                  sex == \"female\" & \n                  homeworld == \"Alderaan\")\n\nhead(newdata)\n\n# select individuals that are from Alderaan, Coruscant, or Endor\nnewdata &lt;- filter(starwars, \n                  homeworld == \"Alderaan\" | \n                  homeworld == \"Coruscant\" | \n                  homeworld == \"Endor\")\n\nhead(newdata)\n\n# this can be written more succinctly as\nnewdata &lt;- filter(starwars, \n                  homeworld %in% \n                    c(\"Alderaan\", \"Coruscant\", \"Endor\"))\n\nhead(newdata)\n```\n\n\n# A tibble: 6 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Leia Org…    150    49 brown      light      brown             19 fema… femin…\n2 Beru Whi…    165    75 brown      light      blue              47 fema… femin…\n3 Mon Moth…    150    NA auburn     fair       blue              48 fema… femin…\n4 Padmé Am…    185    45 brown      light      brown             46 fema… femin…\n5 Shmi Sky…    163    NA black      fair       brown             72 fema… femin…\n6 Ayla Sec…    178    55 none       blue       hazel             48 fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n# A tibble: 1 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Leia Org…    150    49 brown      light      brown             19 fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n# A tibble: 6 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Leia Org…    150    49 brown      light      brown             19 fema… femin…\n2 Wicket S…     88    20 brown      brown      brown              8 male  mascu…\n3 Finis Va…    170    NA blond      fair       blue              91 male  mascu…\n4 Adi Gall…    184    50 none       dark       blue              NA fema… femin…\n5 Bail Pre…    191    NA black      tan        brown             67 male  mascu…\n6 Jocasta …    167    NA white      fair       blue              NA fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;\n# A tibble: 6 × 14\n  name      height  mass hair_color skin_color eye_color birth_year sex   gender\n  &lt;chr&gt;      &lt;int&gt; &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;      &lt;chr&gt;          &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; \n1 Leia Org…    150    49 brown      light      brown             19 fema… femin…\n2 Wicket S…     88    20 brown      brown      brown              8 male  mascu…\n3 Finis Va…    170    NA blond      fair       blue              91 male  mascu…\n4 Adi Gall…    184    50 none       dark       blue              NA fema… femin…\n5 Bail Pre…    191    NA black      tan        brown             67 male  mascu…\n6 Jocasta …    167    NA white      fair       blue              NA fema… femin…\n# ℹ 5 more variables: homeworld &lt;chr&gt;, species &lt;chr&gt;, films &lt;list&gt;,\n#   vehicles &lt;list&gt;, starships &lt;list&gt;"
  },
  {
    "objectID": "Data Visualisation/importing data.html#creatingrecoding-variables",
    "href": "Data Visualisation/importing data.html#creatingrecoding-variables",
    "title": "cars dataset",
    "section": "Creating/Recoding Variables",
    "text": "Creating/Recoding Variables\nThe mutate function allows you to create new variables or transform existing ones.\n\n\nCode\n```{r}\n# convert height in centimeters to inches, \n# and mass in kilograms to pounds\nnewdata &lt;- mutate(starwars, \n                  height = height * 0.394,\n                  mass   = mass   * 2.205)\n```\n\n\nThe ifelse function (part of base R) can be used for recoding data. The format is ifelse(test, return if TRUE, return if FALSE).\n\n\nCode\n```{r}\n# if height is greater than 180 then heightcat = \"tall\", \n# otherwise heightcat = \"short\"\n\nnewdata &lt;- mutate(starwars, \n                  heightcat = ifelse(height &gt; 180, \n                                     \"tall\", \n                                     \"short\"))\n                  \n# convert any eye color that is not black, blue or brown, to other.\nnewdata &lt;- mutate(starwars, \n                  eye_color = ifelse(eye_color %in% \n                                     c(\"black\", \"blue\", \"brown\"),\n                                     eye_color,\n                                     \"other\"))\n                  \n# set heights greater than 200 or less than 75 to missing\nnewdata &lt;- mutate(starwars, \n                  height = ifelse(height &lt; 75 | height &gt; 200,\n                                     NA,\n                                     height))\n```\n\n\n2.2.4 Summarizing data\nThe summarize function can be used to reduce multiple values down to a single value (such as a mean). It is often used in conjunction with the by_group function, to calculate statistics by group. In the code below, the na.rm=TRUE option is used to drop missing values before calculating the means.\nlibrary(dplyr)"
  },
  {
    "objectID": "Data Visualisation/importing data.html#a-tibble-1-2",
    "href": "Data Visualisation/importing data.html#a-tibble-1-2",
    "title": "cars dataset",
    "section": "# A tibble: 1 × 2",
    "text": "# A tibble: 1 × 2"
  },
  {
    "objectID": "Data Visualisation/importing data.html#mean_ht-mean_mass",
    "href": "Data Visualisation/importing data.html#mean_ht-mean_mass",
    "title": "cars dataset",
    "section": "mean_ht mean_mass",
    "text": "mean_ht mean_mass"
  },
  {
    "objectID": "Data Visualisation/importing data.html#section-1",
    "href": "Data Visualisation/importing data.html#section-1",
    "title": "cars dataset",
    "section": "1 175. 97.3",
    "text": "1 175. 97.3"
  },
  {
    "objectID": "Data Visualisation/importing data.html#a-tibble-3-3",
    "href": "Data Visualisation/importing data.html#a-tibble-3-3",
    "title": "cars dataset",
    "section": "# A tibble: 3 × 3",
    "text": "# A tibble: 3 × 3"
  },
  {
    "objectID": "Data Visualisation/importing data.html#gender-mean_ht-mean_wt",
    "href": "Data Visualisation/importing data.html#gender-mean_ht-mean_wt",
    "title": "cars dataset",
    "section": "gender mean_ht mean_wt",
    "text": "gender mean_ht mean_wt"
  },
  {
    "objectID": "Data Visualisation/importing data.html#feminine-167.-54.7",
    "href": "Data Visualisation/importing data.html#feminine-167.-54.7",
    "title": "cars dataset",
    "section": "1 feminine 167. 54.7",
    "text": "1 feminine 167. 54.7"
  },
  {
    "objectID": "Data Visualisation/importing data.html#masculine-177.-107.",
    "href": "Data Visualisation/importing data.html#masculine-177.-107.",
    "title": "cars dataset",
    "section": "2 masculine 177. 107.",
    "text": "2 masculine 177. 107."
  },
  {
    "objectID": "Data Visualisation/importing data.html#section-3",
    "href": "Data Visualisation/importing data.html#section-3",
    "title": "cars dataset",
    "section": "3  175 81",
    "text": "3  175 81\nGraphs are often created from summarized data, rather than from the original observations. You will see several examples in Chapter 4. 2.2.5 Using pipes\nPackages like dplyr and tidyr allow you to write your code in a compact format using the pipe %&gt;% operator. Here is an example.\nlibrary(dplyr)"
  },
  {
    "objectID": "Data Visualisation/importing data.html#data.frame-3-obs.-of-1-variable",
    "href": "Data Visualisation/importing data.html#data.frame-3-obs.-of-1-variable",
    "title": "cars dataset",
    "section": "‘data.frame’: 3 obs. of 1 variable:",
    "text": "‘data.frame’: 3 obs. of 1 variable:"
  },
  {
    "objectID": "Data Visualisation/importing data.html#dob-chr-11101963-jan-23-91-1212001",
    "href": "Data Visualisation/importing data.html#dob-chr-11101963-jan-23-91-1212001",
    "title": "cars dataset",
    "section": "$ dob: chr “11/10/1963” “Jan-23-91” “12:1:2001”",
    "text": "$ dob: chr “11/10/1963” “Jan-23-91” “12:1:2001”\nThere are many ways to convert character variables to Date variables. One of they simplest is to use the functions provided in the lubridate package. These include ymd, dmy, and mdy for importing year-month-day, day-month-year, and month-day-year formats respectively.\nlibrary(lubridate) # convert dob from character to date df\\(dob &lt;- mdy(df\\)dob) str(df)"
  },
  {
    "objectID": "Data Visualisation/importing data.html#data.frame-3-obs.-of-1-variable-1",
    "href": "Data Visualisation/importing data.html#data.frame-3-obs.-of-1-variable-1",
    "title": "cars dataset",
    "section": "‘data.frame’: 3 obs. of 1 variable:",
    "text": "‘data.frame’: 3 obs. of 1 variable:"
  },
  {
    "objectID": "Data Visualisation/importing data.html#dob-date-format-1963-11-10-1991-01-23",
    "href": "Data Visualisation/importing data.html#dob-date-format-1963-11-10-1991-01-23",
    "title": "cars dataset",
    "section": "$ dob: Date, format: “1963-11-10” “1991-01-23” …",
    "text": "$ dob: Date, format: “1963-11-10” “1991-01-23” …\nThe values are recorded internally as the number of days since January 1, 1970. Now that the variable is a Date variable, you can perform date arithmetic (how old are they now), extract date elements (month, day, year), and reformat the values (e.g., October 11, 1963). Date variables are important for time-dependent graphs (Chapter 8). 2.2.7 Reshaping data\nSome graphs require the data to be in wide format, while some graphs require the data to be in long format. An example of wide data is given in Table 2.1. Table 2.1: Wide data id name sex height weight 01 Bill Male 70 180 02 Bob Male 72 195 03 Mary Female 62 130\nYou can convert a wide dataset to a long dataset (Table 2.2) using"
  },
  {
    "objectID": "Report Writing/stylereport.html",
    "href": "Report Writing/stylereport.html",
    "title": "Warwick Statistics Society R Course 2024-2025",
    "section": "",
    "text": "4 Content Here are some guidelines. 1. The report must be self-contained: everything should either be stated ex- plicitly, be justified by a reference to some other source, or be common knowledge. In particular, the source of any data should be clearly indi- cated. 2. It is not usually sensible to describe everything that you did in detail. Indeed, if you modify your approach, there is no need to describe the earlier approach more than briefly. 5 Presentation Reports should be attractive and easy to find one’s way through. Hence, they should be neat, clean and tidy, and must be legible. It is advisable that you try to word-process the report– not a difficult thing to do on the School of Mathematics or other University PCs available to you using Word or Latex. 5.1 Figures, Plots, diagrams, etc. Tables, figures, graphs, and diagrams should be titled and numbered (and prefer- ably given a caption). Sections (which should have headings) and pages should 5 also be numbered. Headings should stand out (by being underlined or on a sepa- rate line, for example) and should never appear as the last line of a page. Think of a report as needing signposts, provided by the section numbers. Similarly, graphs need to be neatly drawn, carefully labelled and titled: scales should be chosen sensibly and the units indicated. Freehand graphs, carefully drawn, may be appropriate; a sketch graph with various amendments, and axes carelessly drawn, will never be. As far as possible, tables and diagrams should be placed in the text near to the passage referring to them. 5.2 Tables Tables need titles, numbering and captions, should be boxed in, and figures in columns should be aligned correctly, usually so that the decimal points fall in the same column. Rows and columns usually need labels or headings, and units must be stated in, or perhaps close to, the table. 5.3 Computer output Computer output needs some thought. For a student project, it may not be nec- essary to copy the results from the output so that they have the same appearance as the rest of the report; it should suffice if the output is edited appropriately first and then pasted onto the body of the report or otherwise attach it. A possibly helpful reference is Chapman & Mahon (1986). 6 Use of English It is very important that you think carefully about your use of English. The ob- ject of writing, as of speaking, is to communicate. Writing, in particular, requires very careful use of language, especially when abstract and difficult concepts (such as those involved in statistical modelling and analysis) are to be communicated. It is easy to read and understand English which is well written and follows the rules of good usage. However, as the quality of writing deteriorates, it becomes progressively more difficult for the reader to work out its intended meaning. Fi- nally it becomes impossible. While many project reports are written to a very high standard, many others contain much that is unintelligible. An obvious, and easily corrected, problem occurs when the author fails to use properly constructed sentences. (A good first test check here is to identify the verb in each sentence!) A much more insidious (and very frequent) problem occurs when the author un- consciously assumes that the reader is somehow following his/her quite unwritten train of thought. Thus the reader is presumed to know exactly what the author is talking about—for example, what variables are currently being considered, what is being held fixed, and what is being allowed to vary - when in fact none of this 6 has ever been stated. Attached to this document are some illustrative quotations, with commentary where appropriate, which have been taken from recent project reports. In most cases you will find that the sentences look just fine - until you try to decode the meaning. How should you attempt to improve the quality of your writing? It is important to understand that, like learning to drive, this is something you have to work at. No doubt an ability to write well comes more easily to some than to others, but everyone needs to put some effort into acquir- ing this skill, and anyone can improve if they do. Of course there are plenty of books on how to write well, but perhaps the most useful thing you can do is to read widely (books, articles in quality newspapers, etc ) and learn to observe how others write. 7 Style 1. Be brief and to the point and use shorter rather than longer words. Thus, ‘schools and colleges’ is to be preferred to ‘educational establishments’ if the two phrases are used with the same meaning. If you find that you are writing about something not relevant to your heading then either the material belongs elsewhere, possibly even to a new separate section, or you are rambling and the material should be omitted. 2. In general, try to avoid the use of the personal pronoun “I”, as it can get very irritating to the reader. Impersonal verbs and the passive form are usually to be preferred, e.g. “A regression analysis was carried out” instead of “I carried out a regression analysis”. 3. Use of tenses: A scientific report should be written in the present tense e.g ”The statistical analysis shows that……”, not ”The statistical analysis showed that…..”. You use the past tense only when describing events that occurred in the past e.g ”The data were collected in a study conducted by Royal Statistical Society……”. 4. Try to get your spelling correct! The main problem is knowing which words’ spelling you are shaky on. If in doubt, look up the word in a good dictionary (Oxford, Cassells, for example) or use a spell-checker. 5. Avoid using words inappropriately. For example, the similar-sounding words “tendency” and “trend” are not the same and cannot be used in- terchangeably. Do not use the word “significant” other than in a statistical sense. 6. If it is not too late, try to write grammatically: whole sentences, each with its main verb; no telegraphic style. Thus, the following is not acceptable. 7 “A standard analysis is possible. By regression.” Mathematical/Statistical writing follows the same grammar and syntax as ordinary English. 7. A common error: the word “data” is treated as if it were a singular noun instead of as a plural noun, which it is! (Data=plural of the singular noun datum.) The correct use of the word is “Data are …”. 8. A good style, which is a pleasure to read, is not something which comes naturally to mathematicians on the whole. Nevertheless, it is worth trying to aim for it. The only way to develop a good style is to read good books, practice writing and read critically what you have written. Gowers (1973) and Fowler (1983) (see references at the end) are useful books on grammar, punctuation and style."
  },
  {
    "objectID": "Report Writing/reportintro.html",
    "href": "Report Writing/reportintro.html",
    "title": "Introduction to Report Writing",
    "section": "",
    "text": "“We have the duty of formulating, of summarizing, and of communicating our conclusions, in intelligible form, in recognition of the right of other free minds to utilize them in making their own decisions.” — Henri Poincaré\n\nThe subsections here will introduce core principles of statistical report writing, with a focus on clarity, structure, and effective communication. These conventions are directly relevant to the ST117 final project, ST231, and more broadly to academic and professional statistical writing.\nGood report writing is not about adding complexity, but about presenting results in a way that allows the reader to understand what was done, why it was done, and what the results mean, without ambiguity or unnecessary detail.\nThis section covers:\n\n\nstructuring a statistical report,\nreporting descriptive statistics clearly and appropriately, reporting numerical values and choosing suitable precision, presenting data using tables and graphs,\nuse of English in technical writing, stylistic conventions in statistics,\nappropriate use of large language models and other generative AI tools.",
    "crumbs": [
      "Home",
      "Report Writing",
      "Introduction to Report Writing"
    ]
  },
  {
    "objectID": "Mathematics to Code/Mathematics to Code MonteCarlo Example.html",
    "href": "Mathematics to Code/Mathematics to Code MonteCarlo Example.html",
    "title": "Monte Carlo Approximation of π — Translating Math into Code",
    "section": "",
    "text": "Mathematical Intuition\nWe throw random points (\\((x,y)\\)) uniformly in the square \\(([-1,1]^2)\\).\nThe fraction inside the unit circle (\\(x^2 + y^2 \\le 1\\)) equals \\((\\pi/4)\\):\n\\(\\pi \\approx 4 \\times \\frac{\\text{points inside circle}}{\\text{total points}}\\) .\n\n\n\nMath → Algorithm → Code Mapping\n\n\n\nMath idea\nR translation\n\n\n\n\nsample \\((x, y)\\)\nrunif(N, -1, 1)\n\n\nindicator \\((I_i)\\)\n(x^2 + y^2 &lt;= 1)\n\n\nestimate\n4 * mean(I)\n\n\n\n\n\n\n\n\n\nflowchart TD\n  A([Start: choose N])\n  B[Sample N points in the unit circle]\n  C{Inside unit circle?}\n  D[Count proportion inside]\n  E[pi_hat = 4 x proportion]\n  F([Return pi_hat])\n\n  A --&gt; B --&gt; C --&gt; D --&gt; E --&gt; F\n\n\n\n\n\n\n\n\n\nImplementation in R\n\n\nCode\n```{r}\nset.seed(0)\n\nestimate_pi &lt;- function(N) {\n  x &lt;- runif(N, -1, 1)\n  y &lt;- runif(N, -1, 1)\n  inside &lt;- (x^2 + y^2) &lt;= 1\n  4 * mean(inside)\n}\n\nestimate_pi(100)\n\nestimate_pi(10000)\n\nestimate_pi(1000000)\n```\n\n\n[1] 3.48\n[1] 3.1208\n[1] 3.142376\n\n\n\n\n\nVisualization\n\n\nCode\n```{r}\nset.seed(1)\nN &lt;- 10000\nx &lt;- runif(N, -1, 1)\ny &lt;- runif(N, -1, 1)\ninside &lt;- (x^2 + y^2) &lt;= 1\n\nplot(x[inside], y[inside], col='skyblue', pch=16, cex=0.5,\n     xlab='x', ylab='y', asp=1,\n     main='Monte Carlo π Approximation',\n     xlim=c(-1,1), ylim=c(-1,1))\npoints(x[!inside], y[!inside], col='gray', pch=16, cex=0.5)\nsymbols(0, 0, circles=1, inches=FALSE, add=TRUE, lwd=2)\n```\n\n\n\n\n\n\n\n\n\n\n\n\nReflection\nMonte Carlo estimation translates a probabilistic argument into a simulation-based computation.",
    "crumbs": [
      "Home",
      "Mathematics to R",
      "Monte Carlo Approximation of π — Translating Math into Code"
    ]
  },
  {
    "objectID": "Mathematics to Code/Mathematics to Code intro.html",
    "href": "Mathematics to Code/Mathematics to Code intro.html",
    "title": "Introduction",
    "section": "",
    "text": "“Mathematicians do not deal in objects, but in relations between objects; thus, they are free to replace some objects by others so lone as the relations remain unchanged. Content to them is irrelevant; they are interested in form only.” - Henri Poincaré\n\nThis section goes through the process of transforming a mathematical problem into code. We will connect abstract ideas in mathematics to practical code, with focus on algorithmic thinking, problem decomposition, and structural clarity.\nSpecifically, the pages cover the following:\n\n\nIntuition\n\nAbstracting a problem into mathematics\nRe-writing a mathematics problem into code\n\nEuclidean Algorithm Example (Algebra Example)\nNewton-Raphson Method (Numerics and Analysis Example)\nMonte Carlo approximation of (Probability and Simulation Example)",
    "crumbs": [
      "Home",
      "Mathematics to R",
      "Introduction"
    ]
  },
  {
    "objectID": "Data Visualisation/timegraph.html",
    "href": "Data Visualisation/timegraph.html",
    "title": "Visualizing Time",
    "section": "",
    "text": "A graph is a powerful vehicle for displaying change. While standard bar charts compare static categories, time-dependent graphs capture the motion of data.\nIn this chapter, we move beyond simple line charts to explore more sophisticated ways to visualize temporal change.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Time"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html",
    "href": "Data Visualisation/multivar.html",
    "title": "Visualizing Multiple Variables",
    "section": "",
    "text": "So far, you’ve learned to visualize single variables (like histograms) and relationships between two variables (like scatterplots). But real-world questions often involve multiple variables at once. For example: “How do penguin body characteristics vary by species, island, AND sex?”\nThis chapter covers two techniques for displaying multiple variables:\n\nGrouping: Using colors, shapes, and sizes to distinguish groups within a single plot\nFaceting: Creating separate mini-plots for different groups\n\nLet’s get started with our packages and data:\n\n\nCode\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)\n\n# View our data\nglimpse(penguins)\n\n\nRows: 344\nColumns: 8\n$ species           &lt;fct&gt; Adelie, Adelie, Adelie, Adelie, Adelie, Adelie, Adel…\n$ island            &lt;fct&gt; Torgersen, Torgersen, Torgersen, Torgersen, Torgerse…\n$ bill_length_mm    &lt;dbl&gt; 39.1, 39.5, 40.3, NA, 36.7, 39.3, 38.9, 39.2, 34.1, …\n$ bill_depth_mm     &lt;dbl&gt; 18.7, 17.4, 18.0, NA, 19.3, 20.6, 17.8, 19.6, 18.1, …\n$ flipper_length_mm &lt;int&gt; 181, 186, 195, NA, 193, 190, 181, 195, 193, 190, 186…\n$ body_mass_g       &lt;int&gt; 3750, 3800, 3250, NA, 3450, 3650, 3625, 4675, 3475, …\n$ sex               &lt;fct&gt; male, female, female, NA, female, male, female, male…\n$ year              &lt;int&gt; 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007, 2007…",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/introvisualisation.html",
    "href": "Data Visualisation/introvisualisation.html",
    "title": "Introduction to Data Visualisation",
    "section": "",
    "text": "“The greatest value of a picture is when it forces us to notice what we never expected to see.” - John W. Tukey\n\nThe subsections here will cover a few data visualisation techniques using ggplot2. The ggplot library is extremely powerful for data visualisation for a number of reasons: it is trivial to add and remove features from graphs, it is one of a few R libraries that are always being worked on and updated, and many more reasons will will discover here.\nThis section covers:\n\n\na review on data importing\nhow baseR and ggplot plot graphs,\nhow to approach univariate data,\nhow to approach bivariate data,\nhow to approach time-dependent data,\nhow to approach multivariate data,\nmiscellaneous graphs,\nhow to customise graphs,s\ngeneral advice on data visualisation",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to Data Visualisation"
    ]
  },
  {
    "objectID": "Data Visualisation/othergraphs.html",
    "href": "Data Visualisation/othergraphs.html",
    "title": "Specialized Visualization Techniques",
    "section": "",
    "text": "This chapter covers specialized visualization techniques that don’t fit neatly into other categories but are incredibly useful for specific types of data and questions. Think of these as tools for your “advanced visualization toolkit”—you won’t use them every day, but when you need them, they’re perfect for the job.\nWe’ll explore: - Visualizing three-dimensional relationships - Displaying flows and transitions - Creating comparison matrices - Specialized chart types for specific purposes\nLet’s load our packages:\n\n\nCode\nlibrary(ggplot2)\nlibrary(palmerpenguins)\nlibrary(dplyr)",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Specialized Visualization Techniques"
    ]
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#a-tibble-6-4",
    "href": "Data Visualisation/othergraphs.html#a-tibble-6-4",
    "title": "basic 3-D scatterplot",
    "section": "# A tibble: 6 × 4",
    "text": "# A tibble: 6 × 4"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#groups-class-sex-survived-6",
    "href": "Data Visualisation/othergraphs.html#groups-class-sex-survived-6",
    "title": "basic 3-D scatterplot",
    "section": "# Groups: Class, Sex, Survived [6]",
    "text": "# Groups: Class, Sex, Survived [6]"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#class-sex-survived-n",
    "href": "Data Visualisation/othergraphs.html#class-sex-survived-n",
    "title": "basic 3-D scatterplot",
    "section": "Class Sex Survived n",
    "text": "Class Sex Survived n"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#st-female-no-4",
    "href": "Data Visualisation/othergraphs.html#st-female-no-4",
    "title": "basic 3-D scatterplot",
    "section": "1 1st Female No 4",
    "text": "1 1st Female No 4"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#st-female-yes-141",
    "href": "Data Visualisation/othergraphs.html#st-female-yes-141",
    "title": "basic 3-D scatterplot",
    "section": "2 1st Female Yes 141",
    "text": "2 1st Female Yes 141"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#st-male-no-118",
    "href": "Data Visualisation/othergraphs.html#st-male-no-118",
    "title": "basic 3-D scatterplot",
    "section": "3 1st Male No 118",
    "text": "3 1st Male No 118"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#st-male-yes-62",
    "href": "Data Visualisation/othergraphs.html#st-male-yes-62",
    "title": "basic 3-D scatterplot",
    "section": "4 1st Male Yes 62",
    "text": "4 1st Male Yes 62"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#nd-female-no-13",
    "href": "Data Visualisation/othergraphs.html#nd-female-no-13",
    "title": "basic 3-D scatterplot",
    "section": "5 2nd Female No 13",
    "text": "5 2nd Female No 13"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#nd-female-yes-93",
    "href": "Data Visualisation/othergraphs.html#nd-female-yes-93",
    "title": "basic 3-D scatterplot",
    "section": "6 2nd Female Yes 93",
    "text": "6 2nd Female Yes 93\nNext create an alluvial diagram in ggplot2 using the ggplot, geom_alluvium and geom_stratum functions. The categorical variables are mapped to axes and n to y. This will produce Figure 10.10\nlibrary(ggalluvial) ggplot(titanic_table, aes(axis1 = Class, axis2 = Sex, axis3 = Survived, y = n)) + geom_alluvium(aes(fill = Class)) + geom_stratum() + geom_text(stat = “stratum”, aes(label = after_stat(stratum)))\nBasic alluvial diagram\nFigure 10.10: Basic alluvial diagram\nTo interpret the graph, start with the variable on the left (Class) and follow the flow to the right. The height of the category level represent the proportion of observations in that level. For example the crew made up roughly 40% of the passengers. Roughly, 30% of passengers survived.\nThe height of the flow represents the proportion of observations contained in the two variable levels they connect. About 50% of first class passengers were females and all female first class passengers survived. The crew was overwhelmingly male and roughly 75% of this group perished.\nAs a second example, let’s look at the relationship between the number carburetors, cylinders, gears, and the transmission type (manual or automatic) for the 32 cars in the mtcars dataset. We’ll treat each variable as categorical.\nFirst, we need to prepare the data.\nlibrary(dplyr) data(mtcars) mtcars_table &lt;- mtcars %&gt;% mutate(am = factor(am, labels = c(“Auto”, “Man”)), cyl = factor(cyl), gear = factor(gear), carb = factor(carb)) %&gt;% group_by(cyl, gear, carb, am) %&gt;% count()\nhead(mtcars_table)"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#a-tibble-6-5",
    "href": "Data Visualisation/othergraphs.html#a-tibble-6-5",
    "title": "basic 3-D scatterplot",
    "section": "# A tibble: 6 × 5",
    "text": "# A tibble: 6 × 5"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#groups-cyl-gear-carb-am-6",
    "href": "Data Visualisation/othergraphs.html#groups-cyl-gear-carb-am-6",
    "title": "basic 3-D scatterplot",
    "section": "# Groups: cyl, gear, carb, am [6]",
    "text": "# Groups: cyl, gear, carb, am [6]"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#cyl-gear-carb-am-n",
    "href": "Data Visualisation/othergraphs.html#cyl-gear-carb-am-n",
    "title": "basic 3-D scatterplot",
    "section": "cyl gear carb am n",
    "text": "cyl gear carb am n"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#auto-1",
    "href": "Data Visualisation/othergraphs.html#auto-1",
    "title": "basic 3-D scatterplot",
    "section": "1 4 3 1 Auto 1",
    "text": "1 4 3 1 Auto 1"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#man-4",
    "href": "Data Visualisation/othergraphs.html#man-4",
    "title": "basic 3-D scatterplot",
    "section": "2 4 4 1 Man 4",
    "text": "2 4 4 1 Man 4"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#auto-2",
    "href": "Data Visualisation/othergraphs.html#auto-2",
    "title": "basic 3-D scatterplot",
    "section": "3 4 4 2 Auto 2",
    "text": "3 4 4 2 Auto 2"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#man-2",
    "href": "Data Visualisation/othergraphs.html#man-2",
    "title": "basic 3-D scatterplot",
    "section": "4 4 4 2 Man 2",
    "text": "4 4 4 2 Man 2"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#man-2-1",
    "href": "Data Visualisation/othergraphs.html#man-2-1",
    "title": "basic 3-D scatterplot",
    "section": "5 4 5 2 Man 2",
    "text": "5 4 5 2 Man 2"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#auto-2-1",
    "href": "Data Visualisation/othergraphs.html#auto-2-1",
    "title": "basic 3-D scatterplot",
    "section": "6 6 3 1 Auto 2",
    "text": "6 6 3 1 Auto 2\nNext create the graph. Several options and functions are added to enhance the results. Specifically,\nthe flow borders are set to black (geom_alluvium)\nthe strata are given transparency (geom_strata)\nthe strata are labeled and made wider (scale_x_discrete)\ntitles are added (labs)\nthe theme is simplified (theme_minimal)\nand the legend is suppressed (theme)\nggplot(mtcars_table, aes(axis1 = carb, axis2 = cyl, axis3 = gear, axis4 = am, y = n)) + geom_alluvium(aes(fill = carb), color=“black”) + geom_stratum(alpha=.8) + geom_text(stat = “stratum”, aes(label = after_stat(stratum))) + scale_x_discrete(limits = c(“Carburetors”, “Cylinders”, “Gears”, “Transmission”), expand = c(.1, .1)) + # scale_fill_brewer(palette=“Paired”) + labs(title = “Mtcars data”, subtitle = “stratified by carb, cyl, gear, and am”, y = “Frequency”) + theme_minimal() + theme(legend.position = “none”)\nBasic alluvial diagram for the mtcars dataset\nFigure 10.11: Basic alluvial diagram for the mtcars dataset\nI think that these changes make the graph easier to follow. For example, all 8 carburetor cars have 8 cylinders, 5 gears, and a manual transmission. Most 4 carburetor cars have 8 cylinders, 3 gears, and an automatic transmission.\nSee the ggalluvial website (https://github.com/corybrunson/ggalluvial) for additional details. 10.5 Heatmaps\nA heatmap displays a set of data using colored tiles for each variable value within each observation. There are many varieties of heatmaps. Although base R comes with a heatmap function, we’ll use the more powerful superheat package (I love these names).\nFirst, let’s create a heatmap for the mtcars dataset that come with base R. The mtcars dataset contains information on 32 cars measured on 11 variables."
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#a-tibble-3-6",
    "href": "Data Visualisation/othergraphs.html#a-tibble-3-6",
    "title": "basic 3-D scatterplot",
    "section": "# A tibble: 3 × 6",
    "text": "# A tibble: 3 × 6"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#group-sleep_total-sleep_rem-sleep_cycle-brainwt-bodywt",
    "href": "Data Visualisation/othergraphs.html#group-sleep_total-sleep_rem-sleep_cycle-brainwt-bodywt",
    "title": "basic 3-D scatterplot",
    "section": "group sleep_total sleep_rem sleep_cycle brainwt bodywt",
    "text": "group sleep_total sleep_rem sleep_cycle brainwt bodywt"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#cow-0-0-1-1-1",
    "href": "Data Visualisation/othergraphs.html#cow-0-0-1-1-1",
    "title": "basic 3-D scatterplot",
    "section": "1 Cow 0 0 1 1 1",
    "text": "1 Cow 0 0 1 1 1"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#dog-1-1-0-0-0",
    "href": "Data Visualisation/othergraphs.html#dog-1-1-0-0-0",
    "title": "basic 3-D scatterplot",
    "section": "2 Dog 1 1 0 0 0",
    "text": "2 Dog 1 1 0 0 0"
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#pig-0.836-0.773-0.5-0.312-0.123",
    "href": "Data Visualisation/othergraphs.html#pig-0.836-0.773-0.5-0.312-0.123",
    "title": "basic 3-D scatterplot",
    "section": "3 Pig 0.836 0.773 0.5 0.312 0.123",
    "text": "3 Pig 0.836 0.773 0.5 0.312 0.123"
  },
  {
    "objectID": "Data Visualisation/univariate.html",
    "href": "Data Visualisation/univariate.html",
    "title": "Univariate Data",
    "section": "",
    "text": "A sensible starting point for any data analysis is to look at each important variable on its own. Univariate graphs do exactly this: they show the distribution of a single variable at a time. That variable might be categorical (for example diet, taxonomic order, or conservation status) or quantitative (such as sleep duration, body weight, or brain weight).\nThroughout this chapter we use the msleep dataset, which contains sleep times and weights for 83 different mammals. We focus on three variables: the diet (vore) and taxonomic order (order) of the animals, and the total amount of sleep they get (sleep_total).",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Univariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/univariate.html#section",
    "href": "Data Visualisation/univariate.html#section",
    "title": "simple bar chart",
    "section": "[1] 5.181946",
    "text": "[1] 5.181946",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "simple bar chart"
    ]
  },
  {
    "objectID": "Mathematics to Code/Mathematics to Code Euclidean Example.html",
    "href": "Mathematics to Code/Mathematics to Code Euclidean Example.html",
    "title": "Euclidean Algorithm — Translating Math into Code",
    "section": "",
    "text": "Mathematical Intuition\nRecall from Sets and Numbers the Euclidean algorithm where we want to find the greatest common divisor (GCD) of two integers \\((a, b)\\): \\[\n\\gcd(a,b) =\n\\begin{cases}\n|a|, & \\text{if } b = 0, \\\\\n\\gcd(b, a \\bmod b), & \\text{otherwise.}\n\\end{cases}\n\\]\nThe algorithmic idea: repeatedly replace the pair \\((a, b)\\) with \\((b, a \\mod b)\\) until the second number becomes \\(0\\). Let us now look at a simplified flowchart of what we want our computer program to do.\nQUESTION FOR THE READER: TRY TO DRAW THE FLOWCHART ON YOUR OWN!\n\n\n\nMath → Algorithm → Code Mapping\n\n\n\nMath operation\nR equivalent\nMeaning\n\n\n\n\n\\((a \\mod b)\\)\na %% b\nremainder\n\n\nrecursive call\ngcd(b, a %% b)\nrepeat step\n\n\nstop if (\\(b = 0\\))\nif (b == 0)\nbase case\n\n\n\n\n\n\n\n\n\nflowchart TD\n  A([Start: a, b])\n  B{Is b equal to 0?}\n  C[Set a to b&lt;br/&gt;Set b to a mod b]\n  D([Return absolute value of a])\n\n  A --&gt; B\n  B -- No --&gt; C --&gt; B\n  B -- Yes --&gt; D\n\n\n\n\n\n\n\n\n\n\nImplementation in R\n\n\nCode\n```{r}\n# Define the GCD function\ngcd_iterative &lt;- function(a, b) {\n  a &lt;- abs(as.integer(a))\n  b &lt;- abs(as.integer(b))\n  while (b != 0) {\n    r &lt;- a %% b\n    a &lt;- b\n    b &lt;- r\n  }\n  return(a)\n}\n\n# Set up the recursive part of the function\ngcd_recursive &lt;- function(a, b) {\n  a &lt;- abs(as.integer(a))\n  b &lt;- abs(as.integer(b))\n  if (b == 0) return(a)\n  gcd_recursive(b, a %% b)\n}\n```\n\n\n\n\n\nExample usage\n\n\nCode\n```{r}\ngcd_iterative(384, 24)\n```\n\n\n[1] 24\n\n\n\n\nCode\n```{r}\ngcd_iterative(1071, 462)\n```\n\n\n[1] 21\n\n\nIn this example, we can see how we can break down algebraic reasoning (these are the skills proofs of theorems and lemmas give you!) into their basic building blocks and then express them as a loop or recursive function as you would do on paper! We have successfully turned a symbolic rule into a computational process! Now it is your turn, try to implement any algorithm of your choosing and see if you can get it to work!",
    "crumbs": [
      "Home",
      "Mathematics to R",
      "Euclidean Algorithm — Translating Math into Code"
    ]
  },
  {
    "objectID": "Mathematics to Code/Mathematics to Code intuit.html",
    "href": "Mathematics to Code/Mathematics to Code intuit.html",
    "title": "Mathematical intuition behind coding",
    "section": "",
    "text": "Imagine you’re trying to explain a puzzle to a robot. Robots don’t guess or assume, they only understand exact instructions. That’s what programming is: explaining a math idea to a robot so it can follow it step by step. If your instructions are fuzzy, the robot gets confused, just like someone trying to make a cake with missing steps!",
    "crumbs": [
      "Home",
      "Mathematics to R",
      "Mathematical intuition behind coding"
    ]
  },
  {
    "objectID": "Mathematics to Code/Mathematics to Code intuit.html#why-mathematics-makes-you-a-better-programmer",
    "href": "Mathematics to Code/Mathematics to Code intuit.html#why-mathematics-makes-you-a-better-programmer",
    "title": "Mathematical intuition behind coding",
    "section": "Why Mathematics Makes You a Better Programmer",
    "text": "Why Mathematics Makes You a Better Programmer\nMathematics trains you to:\n\nDefine problems precisely through rigourous proofs (NOTHING is assumed unless stated)\nWork with abstract structures (ideas of groups, rings, fields in algebra)\nThink algorithmically (the many algorithms you will come across e.g. Euclidean, Gram-Schmidt)\nBreak complex problems into parts (this is essentially what you do during a proof!)\nUnderstand edge cases and generalization (this is also a skill gained in proofs!)\n\n\n\n\n\n\n\nMath to Code Mindset\n\n\n\nWhen approaching a coding task, ask yourself:\n\nWhat is the input?\nWhat transformation is required?\nWhat is the output?\nCan I represent it mathematically?\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nYour code is exectued by a machine. Machines do not guess or assume, they only understand exact instructions. Therefore, keep in mind to specify your code.\n\n\n\nHere is a flowchart of how to build mathematical code in 7 easy steps:\n\n\n\n\n\nflowchart TD\n\n  A[Start with a Problem] \n  B[Translate into Math Language]\n  C[Define Inputs and Outputs]\n  D[Design Algorithm Steps]\n  E[Write Code]\n  F[Test and Validate]\n  G[Refactor or Extend]\n\n  A --&gt; B\n  B --&gt; C\n  C --&gt; D\n  D --&gt; E\n  E --&gt; F\n  F --&gt; G",
    "crumbs": [
      "Home",
      "Mathematics to R",
      "Mathematical intuition behind coding"
    ]
  },
  {
    "objectID": "Report Writing/structurereport.html",
    "href": "Report Writing/structurereport.html",
    "title": "Examples of Mathematics translated into code",
    "section": "",
    "text": "There are many ways to structure a report, some better than others. If you are new to report writing, especially in the field of statistics, I will provide a very basic, but versatile structure to help you in your first steps. The report is made up of several components, listed below in the order in which they should appear. Longer reports, such as the final assignments in both ST117 and ST231, are better split into sections, while shorter reports, like the ST231 assignments, to not really require as full a structure as suggested. In any case, you must avoid a narrative approach like: “First I did this, then that, etc.”. This makes the report difficult to follow; in this case, showing is much more effective than telling.\n\n\nEvery report needs a title, preferably on a separate title page, which should also include the author’s name and department or other affiliation, and the date. The title should be short and informative. In general, this much detail won’t be required and it is most likely that your lecturers will have a template with an appropriate title ready.\nA brief summary – the shorter the better, and certainly no more than half a page – should follow, to tell the reader what the report is about, in general terms, and why it may be worth reading. A good abstract is solely a skeleton outline of (a) the problem, (b) what you have done, (c) your conclusions. It should not, if possible, use numbers, symbols or technical terms. Again, this is not required to be as detailed, nut as you move higher up on the academic and corporate ladder, titles and abstracts will play a larger role in gauging interest in your work, so keep this in mind.\nHere is an example of a title and an abstract:\nTitle: Statistical methods for healthcare regulation: rating, screening and surveillance\nAuthor(s): David Spiegelhalter, Christopher Sherlaw-Johnson, Martin Bardsley, Ian Blunt, Christopher Wood, Olivia Grigg\nAffiliation: Medical Research Council Biostatistics Unit, Cambridge, and related institutions\nDate: First published 21 November 2011 Wiley Online Library\nAbstract (as published):\nCurrent demand for accountability and efficiency of healthcare organizations, combined with the greater availability of routine data on clinical care and outcomes, has led to an increased focus on statistical methods in healthcare regulation. We consider three different regulatory functions in which statistical analysis plays a vital role: rating organizations, deciding whom to inspect and continuous surveillance for arising problems. A common approach to data standardization based on (possibly overdispersed) Z-scores is proposed, although specific tools are used for assessing performance against a target, combining indicators when screening for inspection, and continuous monitoring using risk-adjusted sequential testing procedures. We pay particular attention to the problem of simultaneously monitoring over 200 000 indicators for excess mortality, both with respect to the statistical issues surrounding massive multiplicity, and the organizational aspects of dealing with such a complex but high-profile process.\nNotice that the authors: \n\nKeep under half a page.\n\nAvoid numbers or statistical symbols.\n\nDo not interpret results in depth here — just state the what, how, and conclusion.\n\n\n\n3.3 Introduction This should describe the general context and background, including a general description of what data are available, the manner in which and the purpose for which they were collected. It should also provide the aims of the investigation, together with some indication of the methods used. It may well be an expanded version of the summary, and in general should not exceed two pages in length. 3.4 Methods These should now be described in a fair amount of detail, including any theory that is necessary. How much detail is always difficult to decide. Aim to write as though for someone of comparable standing, e.g. a student in the same year as yourself, who does not know anything in detail about the subject of the re- port. The reader should be able to repeat the study on the basis of your report. Don’t be afraid of putting very detailed and long descriptions into one or more appendices. This component may well have to be split into sections, e.g. 3 1. a preliminary analysis, using graphical and simple descriptive methods, 2. a full-scale analysis. The second section may further have to be split into two subsections e.g. (a) a description, explanation or development of the methodology to be used, (b) the actual analysis. Subsection (a) will probably need to be quite technical, certainly more technical than the rest of the report. It should describe in detail the precise mathemat- ical/statistical tools and techniques to be used and reveal the manner in which they lead to the desired results. Do not shy away from using mathematical no- tation (get well familiar with the capabilities of your word-processor to produce mathematical expressions) but be careful to define every bit of the notation you use. A mathematical expression is totally meaningless if any of the symbols used in it is undefined. Similarly pages of mathematical development and explanations are rendered useless and total waste of space if any of the symbols involved in these explanations are not defined. I In the subsection containing the actual analysis it is important that you de- scribe your analysis sufficiently clearly and carefully to enable it to be reproduced by the reader. 3.5 Results, conclusions and recommendations Give the main results and conclusions. Subsidiary results and deductions can be left in this section if they do not detract from the flow of the presentation of the main results and conclusion; otherwise they may be gathered into a separate component, be combined with the description of methods, or be put into an appendix. Your conclusions should be expressed in a way that can be understood by a non-statistician and should make sense even if the reader of your report had omitted to read the middle section on detailed statistical analysis. Try to report your conclusion in the context of the experiment from which the data came. 3.6 General discussion It may be appropriate to give some account of previous investigation of the same or related problems, or to relate the present conclusion to others in a connected area. It is worth discussing how far the original aim was successfully achieved, if it was not, why not, and how you might have done things differently. Reserva- tions about the data also belong here. (In real life, data sets may well contain errors or have been obtained from a badly designed experiment). This general discussion can in fact appear not as a section in itself but can be subsumed in the Introduction. 4 3.7 References These should always be included if you refer to books or papers from journals. The simplest, and widely-used style, is to refer to authors in the text in the format “Surname (Year of Publication)”, and to give the full references in the References section as shown at the end of this article i.e. for a journal article reference give Author’s surname, author’s initials, title of paper including subtitle if there is one, journal name, year of publication, volume, first and last page numbers of article, in that order; for a book reference give, in the following order, the author’es surname, the author’s initials, (year of publication) Title of book (in italics), publishing company, city of publication. 3.8 Appendix Whatever does not fit naturally into the main body of the report and cannot reasonably be omitted should go into an appendix. However, you don’t want the report to suffer from appendicitis! A case of the appendix or appendices tail wagging the main text dog makes the report hard to read.",
    "crumbs": [
      "Home",
      "Report Writing",
      "Examples of Mathematics translated into code"
    ]
  },
  {
    "objectID": "Report Writing/structurereport.html#titles-and-abstracts",
    "href": "Report Writing/structurereport.html#titles-and-abstracts",
    "title": "Examples of Mathematics translated into code",
    "section": "",
    "text": "Every report needs a title, preferably on a separate title page, which should also include the author’s name and department or other affiliation, and the date. The title should be short and informative. In general, this much detail won’t be required and it is most likely that your lecturers will have a template with an appropriate title ready.\nA brief summary – the shorter the better, and certainly no more than half a page – should follow, to tell the reader what the report is about, in general terms, and why it may be worth reading. A good abstract is solely a skeleton outline of (a) the problem, (b) what you have done, (c) your conclusions. It should not, if possible, use numbers, symbols or technical terms. Again, this is not required to be as detailed, nut as you move higher up on the academic and corporate ladder, titles and abstracts will play a larger role in gauging interest in your work, so keep this in mind.\nHere is an example of a title and an abstract:\nTitle: Statistical methods for healthcare regulation: rating, screening and surveillance\nAuthor(s): David Spiegelhalter, Christopher Sherlaw-Johnson, Martin Bardsley, Ian Blunt, Christopher Wood, Olivia Grigg\nAffiliation: Medical Research Council Biostatistics Unit, Cambridge, and related institutions\nDate: First published 21 November 2011 Wiley Online Library\nAbstract (as published):\nCurrent demand for accountability and efficiency of healthcare organizations, combined with the greater availability of routine data on clinical care and outcomes, has led to an increased focus on statistical methods in healthcare regulation. We consider three different regulatory functions in which statistical analysis plays a vital role: rating organizations, deciding whom to inspect and continuous surveillance for arising problems. A common approach to data standardization based on (possibly overdispersed) Z-scores is proposed, although specific tools are used for assessing performance against a target, combining indicators when screening for inspection, and continuous monitoring using risk-adjusted sequential testing procedures. We pay particular attention to the problem of simultaneously monitoring over 200 000 indicators for excess mortality, both with respect to the statistical issues surrounding massive multiplicity, and the organizational aspects of dealing with such a complex but high-profile process.\nNotice that the authors: \n\nKeep under half a page.\n\nAvoid numbers or statistical symbols.\n\nDo not interpret results in depth here — just state the what, how, and conclusion.",
    "crumbs": [
      "Home",
      "Report Writing",
      "Examples of Mathematics translated into code"
    ]
  },
  {
    "objectID": "Report Writing/structurereport.html#to-include-or-not-to-include-that-is-the-question",
    "href": "Report Writing/structurereport.html#to-include-or-not-to-include-that-is-the-question",
    "title": "Examples of Mathematics translated into code",
    "section": "",
    "text": "3.3 Introduction This should describe the general context and background, including a general description of what data are available, the manner in which and the purpose for which they were collected. It should also provide the aims of the investigation, together with some indication of the methods used. It may well be an expanded version of the summary, and in general should not exceed two pages in length. 3.4 Methods These should now be described in a fair amount of detail, including any theory that is necessary. How much detail is always difficult to decide. Aim to write as though for someone of comparable standing, e.g. a student in the same year as yourself, who does not know anything in detail about the subject of the re- port. The reader should be able to repeat the study on the basis of your report. Don’t be afraid of putting very detailed and long descriptions into one or more appendices. This component may well have to be split into sections, e.g. 3 1. a preliminary analysis, using graphical and simple descriptive methods, 2. a full-scale analysis. The second section may further have to be split into two subsections e.g. (a) a description, explanation or development of the methodology to be used, (b) the actual analysis. Subsection (a) will probably need to be quite technical, certainly more technical than the rest of the report. It should describe in detail the precise mathemat- ical/statistical tools and techniques to be used and reveal the manner in which they lead to the desired results. Do not shy away from using mathematical no- tation (get well familiar with the capabilities of your word-processor to produce mathematical expressions) but be careful to define every bit of the notation you use. A mathematical expression is totally meaningless if any of the symbols used in it is undefined. Similarly pages of mathematical development and explanations are rendered useless and total waste of space if any of the symbols involved in these explanations are not defined. I In the subsection containing the actual analysis it is important that you de- scribe your analysis sufficiently clearly and carefully to enable it to be reproduced by the reader. 3.5 Results, conclusions and recommendations Give the main results and conclusions. Subsidiary results and deductions can be left in this section if they do not detract from the flow of the presentation of the main results and conclusion; otherwise they may be gathered into a separate component, be combined with the description of methods, or be put into an appendix. Your conclusions should be expressed in a way that can be understood by a non-statistician and should make sense even if the reader of your report had omitted to read the middle section on detailed statistical analysis. Try to report your conclusion in the context of the experiment from which the data came. 3.6 General discussion It may be appropriate to give some account of previous investigation of the same or related problems, or to relate the present conclusion to others in a connected area. It is worth discussing how far the original aim was successfully achieved, if it was not, why not, and how you might have done things differently. Reserva- tions about the data also belong here. (In real life, data sets may well contain errors or have been obtained from a badly designed experiment). This general discussion can in fact appear not as a section in itself but can be subsumed in the Introduction. 4 3.7 References These should always be included if you refer to books or papers from journals. The simplest, and widely-used style, is to refer to authors in the text in the format “Surname (Year of Publication)”, and to give the full references in the References section as shown at the end of this article i.e. for a journal article reference give Author’s surname, author’s initials, title of paper including subtitle if there is one, journal name, year of publication, volume, first and last page numbers of article, in that order; for a book reference give, in the following order, the author’es surname, the author’s initials, (year of publication) Title of book (in italics), publishing company, city of publication. 3.8 Appendix Whatever does not fit naturally into the main body of the report and cannot reasonably be omitted should go into an appendix. However, you don’t want the report to suffer from appendicitis! A case of the appendix or appendices tail wagging the main text dog makes the report hard to read.",
    "crumbs": [
      "Home",
      "Report Writing",
      "Examples of Mathematics translated into code"
    ]
  },
  {
    "objectID": "Report Writing/useofaireport.html",
    "href": "Report Writing/useofaireport.html",
    "title": "Examples of Mathematics translated into code",
    "section": "",
    "text": "To start off, I want to make this clear: You should NOT be using AI to do your coursework for you! In no way is this section endorsing the use of LLMs like ChatGPT, Gemini, or other models to cut corners on assignments and coursework. This section will simply go through the healthy, and acceptable ways to use AI in your everyday studies.\n\n\nClarifying Concepts\n\n    Use AI to explain statistical ideas (e.g., Bayesian inference, p-values, regression assumptions) in simpler terms.\n\n    Great for quick refreshers before you start writing.\n\nCode Debugging & Optimisation\n\n    Paste your R / Python code and ask AI to find errors or suggest more efficient approaches.\n\n    Useful for spotting typos, missing parentheses, or inefficient loops.\n\nExploratory Data Analysis Help\n\n    Ask AI for ideas on which plots or summary stats to include.\n\n    Use it to remember ggplot2 or matplotlib syntax you’ve forgotten.\n\nReport Structure Guidance\n\n    Get sample outlines, section names, and suggestions for logical flow.\n\n    Ask for alternative ways to present your results (e.g., tables vs plots).\n\nFinding References & Resources\n\n    Ask for academic sources, journal article suggestions, or textbooks.\n\n    Always check the actual source before citing — AI can suggest leads but you must verify.\n\nPlain-Language Drafting\n\n    Use AI to rephrase technical findings for a non-statistical audience.\n\n    Helpful for the Abstract, Discussion, or Recommendations sections.\n\n\n\nCopy-Pasting AI Output as Your Work\n\n    Risks plagiarism and academic misconduct.\n\n    Even if AI generates correct text/code, rewrite in your own words and verify every number.\n\nTrusting Generated Code Blindly\n\n    AI can produce syntactically correct but statistically wrong methods (e.g., mis-specified models, wrong assumptions).\n\n    Always check the logic and outputs yourself.\n\nUsing AI for Fabricated Results\n\n    Never generate fake datasets, fake statistical outputs, or invented references.\n\n    Fabrication can be detected and is a serious offence.\n\nSkipping Understanding\n\n    If you let AI do all the work, you won’t be able to defend your answers if questioned.\n\n    Make sure you can explain every step in your own words.\n\nOverusing AI for Writing\n\n    Avoid making your whole report “AI-polished” to the point it loses your voice.\n\n    Many professors can spot a fully AI-written paper — and it may lack context-specific insight.\n\nIgnoring Data Privacy\n\n    Don’t paste sensitive or unpublished data into AI tools if your university or project has confidentiality requirements."
  },
  {
    "objectID": "Report Writing/useofaireport.html#good-ways-to-use-ai-in-statistics-assignments",
    "href": "Report Writing/useofaireport.html#good-ways-to-use-ai-in-statistics-assignments",
    "title": "Examples of Mathematics translated into code",
    "section": "",
    "text": "Clarifying Concepts\n\n    Use AI to explain statistical ideas (e.g., Bayesian inference, p-values, regression assumptions) in simpler terms.\n\n    Great for quick refreshers before you start writing.\n\nCode Debugging & Optimisation\n\n    Paste your R / Python code and ask AI to find errors or suggest more efficient approaches.\n\n    Useful for spotting typos, missing parentheses, or inefficient loops.\n\nExploratory Data Analysis Help\n\n    Ask AI for ideas on which plots or summary stats to include.\n\n    Use it to remember ggplot2 or matplotlib syntax you’ve forgotten.\n\nReport Structure Guidance\n\n    Get sample outlines, section names, and suggestions for logical flow.\n\n    Ask for alternative ways to present your results (e.g., tables vs plots).\n\nFinding References & Resources\n\n    Ask for academic sources, journal article suggestions, or textbooks.\n\n    Always check the actual source before citing — AI can suggest leads but you must verify.\n\nPlain-Language Drafting\n\n    Use AI to rephrase technical findings for a non-statistical audience.\n\n    Helpful for the Abstract, Discussion, or Recommendations sections."
  },
  {
    "objectID": "Report Writing/useofaireport.html#what-to-avoid-when-using-ai",
    "href": "Report Writing/useofaireport.html#what-to-avoid-when-using-ai",
    "title": "Examples of Mathematics translated into code",
    "section": "",
    "text": "Copy-Pasting AI Output as Your Work\n\n    Risks plagiarism and academic misconduct.\n\n    Even if AI generates correct text/code, rewrite in your own words and verify every number.\n\nTrusting Generated Code Blindly\n\n    AI can produce syntactically correct but statistically wrong methods (e.g., mis-specified models, wrong assumptions).\n\n    Always check the logic and outputs yourself.\n\nUsing AI for Fabricated Results\n\n    Never generate fake datasets, fake statistical outputs, or invented references.\n\n    Fabrication can be detected and is a serious offence.\n\nSkipping Understanding\n\n    If you let AI do all the work, you won’t be able to defend your answers if questioned.\n\n    Make sure you can explain every step in your own words.\n\nOverusing AI for Writing\n\n    Avoid making your whole report “AI-polished” to the point it loses your voice.\n\n    Many professors can spot a fully AI-written paper — and it may lack context-specific insight.\n\nIgnoring Data Privacy\n\n    Don’t paste sensitive or unpublished data into AI tools if your university or project has confidentiality requirements."
  },
  {
    "objectID": "Data Visualisation/customising graphs.html",
    "href": "Data Visualisation/customising graphs.html",
    "title": "Graph Customisation",
    "section": "",
    "text": "When you first create plots in R, the default settings work well for exploring your data. However, when you’re preparing visualizations for presentations, reports, or publications, you’ll want to polish them to make your message clearer and more visually appealing.\nIn this tutorial, we’ll learn how to customize various aspects of ggplot2 graphs including axes, colors, labels, legends, and themes. We’ll primarily use the palmerpenguins dataset, which contains measurements of three penguin species from islands in Antarctica.\nFirst, let’s load our packages and data:\n\n\nCode\nlibrary(ggplot2)\n\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nCode\nlibrary(palmerpenguins)\n\n\nWarning: package 'palmerpenguins' was built under R version 4.4.3\n\n\nCode\nlibrary(dplyr)\n\n\nWarning: package 'dplyr' was built under R version 4.4.3",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#quantitative-axes",
    "href": "Data Visualisation/customising graphs.html#quantitative-axes",
    "title": "Graph Customisation",
    "section": "Quantitative axes",
    "text": "Quantitative axes\nQuantitative x- and y-axes can be modified using the scale_x_continuous and scale_y_continuous functions respectively\nOptions include\nbreaks - a numeric vector of positions\nlimits - a numeric vector with the min and max for the scale"
  },
  {
    "objectID": "Data Visualisation/bivariate.html",
    "href": "Data Visualisation/bivariate.html",
    "title": "Visualizing Bivariate Data",
    "section": "",
    "text": "One of the most fundamental questions in data analysis is: “How does variable A relate to variable B?”\nBivariate analysis allows us to investigate relationships, correlations, and differences between groups. The specific type of graph you choose depends entirely on the data types you are working with (Categorical or Quantitative).",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#a-tibble-12-5",
    "href": "Data Visualisation/bivariate.html#a-tibble-12-5",
    "title": "Dealing with bivariate data",
    "section": "# A tibble: 12 × 5",
    "text": "# A tibble: 12 × 5"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#groups-class-7",
    "href": "Data Visualisation/bivariate.html#groups-class-7",
    "title": "Dealing with bivariate data",
    "section": "# Groups: class [7]",
    "text": "# Groups: class [7]"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#class-drv-n-pct-lbl",
    "href": "Data Visualisation/bivariate.html#class-drv-n-pct-lbl",
    "title": "Dealing with bivariate data",
    "section": "class drv n pct lbl",
    "text": "class drv n pct lbl"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#seater-r-5-1-100",
    "href": "Data Visualisation/bivariate.html#seater-r-5-1-100",
    "title": "Dealing with bivariate data",
    "section": "1 2seater r 5 1 100%",
    "text": "1 2seater r 5 1 100%"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#compact-4-12-0.255-26",
    "href": "Data Visualisation/bivariate.html#compact-4-12-0.255-26",
    "title": "Dealing with bivariate data",
    "section": "2 compact 4 12 0.255 26%",
    "text": "2 compact 4 12 0.255 26%"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#compact-f-35-0.745-74",
    "href": "Data Visualisation/bivariate.html#compact-f-35-0.745-74",
    "title": "Dealing with bivariate data",
    "section": "3 compact f 35 0.745 74%",
    "text": "3 compact f 35 0.745 74%"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#midsize-4-3-0.0732-7",
    "href": "Data Visualisation/bivariate.html#midsize-4-3-0.0732-7",
    "title": "Dealing with bivariate data",
    "section": "4 midsize 4 3 0.0732 7%",
    "text": "4 midsize 4 3 0.0732 7%"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#midsize-f-38-0.927-93",
    "href": "Data Visualisation/bivariate.html#midsize-f-38-0.927-93",
    "title": "Dealing with bivariate data",
    "section": "5 midsize f 38 0.927 93%",
    "text": "5 midsize f 38 0.927 93%"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#minivan-f-11-1-100",
    "href": "Data Visualisation/bivariate.html#minivan-f-11-1-100",
    "title": "Dealing with bivariate data",
    "section": "6 minivan f 11 1 100%",
    "text": "6 minivan f 11 1 100%"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#pickup-4-33-1-100",
    "href": "Data Visualisation/bivariate.html#pickup-4-33-1-100",
    "title": "Dealing with bivariate data",
    "section": "7 pickup 4 33 1 100%",
    "text": "7 pickup 4 33 1 100%"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#subcompact-4-4-0.114-11",
    "href": "Data Visualisation/bivariate.html#subcompact-4-4-0.114-11",
    "title": "Dealing with bivariate data",
    "section": "8 subcompact 4 4 0.114 11%",
    "text": "8 subcompact 4 4 0.114 11%"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#subcompact-f-22-0.629-63",
    "href": "Data Visualisation/bivariate.html#subcompact-f-22-0.629-63",
    "title": "Dealing with bivariate data",
    "section": "9 subcompact f 22 0.629 63%",
    "text": "9 subcompact f 22 0.629 63%"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#subcompact-r-9-0.257-26",
    "href": "Data Visualisation/bivariate.html#subcompact-r-9-0.257-26",
    "title": "Dealing with bivariate data",
    "section": "10 subcompact r 9 0.257 26%",
    "text": "10 subcompact r 9 0.257 26%"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#suv-4-51-0.823-82",
    "href": "Data Visualisation/bivariate.html#suv-4-51-0.823-82",
    "title": "Dealing with bivariate data",
    "section": "11 suv 4 51 0.823 82%",
    "text": "11 suv 4 51 0.823 82%"
  },
  {
    "objectID": "Data Visualisation/bivariate.html#suv-r-11-0.177-18",
    "href": "Data Visualisation/bivariate.html#suv-r-11-0.177-18",
    "title": "Dealing with bivariate data",
    "section": "12 suv r 11 0.177 18%",
    "text": "12 suv r 11 0.177 18%\nNext, we’ll use this dataset and the geom_text function to add labels to each bar segment."
  },
  {
    "objectID": "Data Visualisation/bivariate.html#stacked-bar-charts",
    "href": "Data Visualisation/bivariate.html#stacked-bar-charts",
    "title": "Visualizing Bivariate Data",
    "section": "Stacked Bar Charts",
    "text": "Stacked Bar Charts\nThe default behavior of a bar chart in ggplot2 is to stack categories on top of one another. This gives you a sense of the total count while showing the breakdown of subgroups.\n\n\nCode\nlibrary(ggplot2)\nlibrary(palmerpenguins)\n\npenguins &lt;- palmerpenguins::penguins\npenguins &lt;- na.omit(penguins)\n\n# Stacked bar chart\nggplot(penguins, aes(x = island, fill = species)) + \n  geom_bar(position = \"stack\") +\n  labs(title = \"Species Distribution by Island (Stacked)\")\n\n\n\n\n\n\n\n\n\nFrom this chart, we can quickly see that Biscoe island has the most penguins overall, and Torgersen island is exclusively inhabited by Adelie penguins.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#grouped-bar-charts",
    "href": "Data Visualisation/bivariate.html#grouped-bar-charts",
    "title": "Visualizing Bivariate Data",
    "section": "Grouped Bar Charts",
    "text": "Grouped Bar Charts\nIf you want to compare the specific counts of species side-by-side, the “grouped bar chart is preferred. It un-stacks the bars and places them next to each other.\n\n\nCode\n# Grouped bar chart\nggplot(penguins, aes(x = island, fill = species)) + \n  geom_bar(position = \"dodge\") +\n  labs(title = \"Species Distribution by Island (Grouped)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nBy default, if a category has zero observations (like Chinstrap penguins on Biscoe), the bar is omitted, and the remaining bars expand to fill the space. If you want to preserve the width and show the gap, use position_dodge(preserve = \"single\").",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#segmented-100-fill-bar-charts",
    "href": "Data Visualisation/bivariate.html#segmented-100-fill-bar-charts",
    "title": "Visualizing Bivariate Data",
    "section": "Segmented (100% Fill) Bar Charts",
    "text": "Segmented (100% Fill) Bar Charts\nSometimes, raw counts matter less than percentages. If we want to know “What proportion of penguins on Dream Island are Chinstraps?”, we use the “fill” position. This stretches every bar to 100%.\n\n\nCode\nlibrary(scales)\n\n# Segmented bar chart\nggplot(penguins, aes(x = island, fill = species)) + \n  geom_bar(position = \"fill\") + \n  scale_y_continuous(labels = percent) +\n  labs(y = \"Proportion\", title = \"Species Proportion by Island\") +\n  theme_minimal()",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#adding-labels",
    "href": "Data Visualisation/bivariate.html#adding-labels",
    "title": "Visualizing Bivariate Data",
    "section": "Adding Labels",
    "text": "Adding Labels\nA segmented bar chart is most effective when the actual percentages are written on the plot. To do this, we must first calculate the percentages manually using dplyr, and then plot the summary data.\n\n\nCode\nlibrary(dplyr)\n\n# 1. Create a summary table with labels\nplotdata &lt;- penguins %&gt;%\n  group_by(island, species) %&gt;%\n  summarize(n = n()) %&gt;% \n  mutate(pct = n / sum(n),\n         lbl = scales::percent(pct, accuracy = 1))\n\n# 2. Plot using the summary data\nggplot(plotdata, aes(x = island, y = pct, fill = species)) + \n  # Use geom_col for pre-calculated data\n  geom_col(position = \"fill\") + \n  geom_text(aes(label = lbl), \n            position = position_fill(vjust = 0.5), \n            color = \"white\", fontface = \"bold\") +\n  scale_y_continuous(labels = percent) +\n  labs(y = \"Percent\", title = \"Species Composition by Island\") +\n  theme_minimal()",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#scatterplots",
    "href": "Data Visualisation/bivariate.html#scatterplots",
    "title": "Visualizing Bivariate Data",
    "section": "Scatterplots",
    "text": "Scatterplots\nThe scatterplot is the standard for bivariate quantitative data. Let’s look at the relationship between Flipper Length and Body Mass.\n\n\nCode\n# Scatterplot\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point(color = \"cornflowerblue\", size = 2, alpha = 0.6) + \n  labs(title = \"Flipper Length vs. Body Mass\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\") +\n  theme_minimal()",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#adding-trend-lines",
    "href": "Data Visualisation/bivariate.html#adding-trend-lines",
    "title": "Visualizing Bivariate Data",
    "section": "2.2 Adding Trend Lines",
    "text": "2.2 Adding Trend Lines\nTo summarize the relationship, we can add a trend line using geom_smooth().\n\nLinear Fit\nThe most common approach is a straight linear regression line (method = \"lm\").\n\n\nCode\n```{r}\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point(color = \"cornflowerblue\", alpha = 0.6) + \n  geom_smooth(method = \"lm\", color = \"black\", size = 1) +\n  labs(title = \"Linear Relationship\")\n```\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\n\n\nPolynomial and Loess Curves\nNot all data is linear. Sometimes relationships curve.\n\nPolynomial: Uses a formula (e.g., quadratic) to fit a curve.\nLoess: A non-parametric method that follows the data locally. This is the default in ggplot2 for smaller datasets.\n\nLet’s look at a different dataset, gapminder, to see a curved relationship between GDP per Capita and Life Expectancy.\n\n\nCode\n```{r}\nlibrary(gapminder)\n```\n\n\nWarning: package 'gapminder' was built under R version 4.4.3\n\n\nCode\n```{r}\n# Loess Curve (Standard Smoothing)\nggplot(gapminder, aes(x = gdpPercap, y = lifeExp)) + \n  geom_point(alpha = 0.3, color = \"darkcyan\") + \n  geom_smooth(method = \"loess\", color = \"red\") + \n  scale_x_log10(labels = scales::dollar) + # Log scale helps visualize GDP\n  labs(title = \"GDP vs Life Expectancy (Loess Fit)\",\n       subtitle = \"Note the logarithmic X axis\")\n```\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#time-series-line-plots",
    "href": "Data Visualisation/bivariate.html#time-series-line-plots",
    "title": "Visualizing Bivariate Data",
    "section": "Time Series (Line Plots)",
    "text": "Time Series (Line Plots)\nIf one of your quantitative variables is Time, a line plot is the standard choice. Below is the change in Life Expectancy in the United States over time.\n\n\nCode\nlibrary(gapminder)\n# Filter for US data\nus_data &lt;- filter(gapminder, country == \"United States\")\n\n# Line plot with points\nggplot(us_data, aes(x = year, y = lifeExp)) + \n  geom_line(size = 1.2, color = \"grey\") + \n  geom_point(size = 3, color = \"steelblue\") + \n  labs(title = \"US Life Expectancy (1952-2007)\",\n       y = \"Life Expectancy (years)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nTimes series are discussed in greater detail on the time series page.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#bar-charts-summary-statistics",
    "href": "Data Visualisation/bivariate.html#bar-charts-summary-statistics",
    "title": "Visualizing Bivariate Data",
    "section": "3.1 Bar Charts (Summary Statistics)",
    "text": "3.1 Bar Charts (Summary Statistics)\nWe can plot the Mean of a variable for each group. Here, we calculate the average body mass for each penguin species.\n\n\nCode\n```{r}\n# Calculate means\nmean_data &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(mean_mass = mean(body_mass_g))\n\n# Plot means\nggplot(mean_data, aes(x = species, y = mean_mass)) + \n  geom_col(fill = \"steelblue\", width = 0.7) + \n  geom_text(aes(label = round(mean_mass, 0)), vjust = -0.5) +\n  scale_y_continuous(limits = c(0, 6000)) +\n  labs(title = \"Average Body Mass by Species\", y = \"Mass (g)\") +\n  theme_minimal()\n```\n\n\n\n\n\n\n\n\n\nWarning: Bar charts of means can be misleading because they hide the spread of the data. A species with highly variable weights looks the same as a species with consistent weights.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#distributions-boxplots-and-violins",
    "href": "Data Visualisation/bivariate.html#distributions-boxplots-and-violins",
    "title": "Visualizing Bivariate Data",
    "section": "Distributions: Boxplots and Violins",
    "text": "Distributions: Boxplots and Violins\nTo see the spread, we use distribution plots.\n\nBoxplots\nBoxplots summarize the distribution using quartiles. They show the median, the IQR (Interquartile Range), and outliers.\n\n\nCode\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  geom_boxplot(alpha = 0.6) + \n  labs(title = \"Body Mass Distribution by Species\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\n\n\nViolin Plots\nViolin plots show the “density” of the data. The wider the violin, the more data points exist at that value.\n\n\nCode\nggplot(penguins, aes(x = species, y = body_mass_g, fill = species)) + \n  geom_violin(trim = FALSE, alpha = 0.6) + \n  geom_boxplot(width = 0.1, fill = \"white\") + # Add boxplot inside for reference\n  labs(title = \"Violin Plot with Embedded Boxplot\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#ridgeline-plots",
    "href": "Data Visualisation/bivariate.html#ridgeline-plots",
    "title": "Visualizing Bivariate Data",
    "section": "Ridgeline Plots",
    "text": "Ridgeline Plots\nRidgeline plots (or “Joyplots”) are excellent when you have many categories. They display density curves stacked vertically.\n\n\nCode\nlibrary(ggridges)\n\nggplot(penguins, aes(x = body_mass_g, y = species, fill = species)) + \n  geom_density_ridges(alpha = 0.7) + \n  labs(title = \"Ridgeline Plot of Body Mass\") +\n  theme_ridges() + \n  theme(legend.position = \"none\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#mean-with-error-bars",
    "href": "Data Visualisation/bivariate.html#mean-with-error-bars",
    "title": "Visualizing Bivariate Data",
    "section": "Mean with Error Bars",
    "text": "Mean with Error Bars\nScientific publications often require plots showing the Mean +/- the Standard Error (SEM) or Confidence Interval.\n\n\nCode\n# Calculate stats\nsummary_stats &lt;- penguins %&gt;%\n  group_by(species, sex) %&gt;%\n  summarize(\n    n = n(),\n    mean = mean(body_mass_g),\n    sd = sd(body_mass_g),\n    se = sd / sqrt(n)\n  )\n\n# Plot Mean +/- SE\npd &lt;- position_dodge(0.2) # To separate overlapping bars\n\nggplot(summary_stats, aes(x = species, y = mean, color = sex, group = sex)) + \n  geom_point(position = pd, size = 4) + \n  geom_errorbar(aes(ymin = mean - se, ymax = mean + se), width = 0.2, position = pd, size = 1) +\n  labs(title = \"Mean Body Mass by Species and Sex\",\n       subtitle = \"Error bars represent Standard Error\",\n       y = \"Body Mass (g)\") +\n  theme_minimal()",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#jitter-and-strip-plots",
    "href": "Data Visualisation/bivariate.html#jitter-and-strip-plots",
    "title": "Visualizing Bivariate Data",
    "section": "Jitter and Strip Plots",
    "text": "Jitter and Strip Plots\nIf your dataset isn’t massive, the most honest way to show the data is to show all the points. To prevent points from overlapping, we “jitter” them (add random noise).\n\n\nCode\nggplot(penguins, aes(x = species, y = body_mass_g, color = species)) + \n  geom_jitter(width = 0.2, alpha = 0.6) + \n  stat_summary(fun = mean, geom = \"point\", shape = 95, size = 10, color = \"black\") + # Add mean bar\n  labs(title = \"Raw Data with Mean Indicator\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#cleveland-dot-plots-lollipop-charts",
    "href": "Data Visualisation/bivariate.html#cleveland-dot-plots-lollipop-charts",
    "title": "Visualizing Bivariate Data",
    "section": "Cleveland Dot Plots (Lollipop Charts)",
    "text": "Cleveland Dot Plots (Lollipop Charts)\nWhen comparing a numeric value across many categories (like countries), a bar chart becomes cluttered. A Cleveland dot plot (or lollipop chart) is a cleaner alternative.\nLet’s look at Life Expectancy in the Americas in 2007.\n\n\nCode\n# Prepare data\namericas &lt;- gapminder %&gt;% \n  filter(continent == \"Americas\" & year == 2007)\n\n# Lollipop Chart\nggplot(americas, aes(x = lifeExp, y = reorder(country, lifeExp))) + \n  geom_segment(aes(x = 60, xend = lifeExp, y = country, yend = country), color = \"grey\") +\n  geom_point(size = 3, color = \"darkcyan\") + \n  labs(title = \"Life Expectancy in the Americas (2007)\",\n       x = \"Life Expectancy (Years)\", y = \"\") +\n  theme_minimal() +\n  theme(panel.grid.major.y = element_blank()) # Remove horizontal grid lines for clarity",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#basic-bubble-chart",
    "href": "Data Visualisation/othergraphs.html#basic-bubble-chart",
    "title": "Specialized Visualization Techniques",
    "section": "Basic Bubble Chart",
    "text": "Basic Bubble Chart\nLet’s explore the relationship between penguin flipper length, body mass, and bill length:\n\n\nCode\nggplot(penguins, aes(x = flipper_length_mm, \n                     y = body_mass_g,\n                     size = bill_length_mm)) +\n  geom_point(alpha = 0.6) +\n  labs(title = \"Penguin Physical Characteristics\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\",\n       size = \"Bill Length (mm)\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Specialized Visualization Techniques"
    ]
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#enhanced-bubble-chart",
    "href": "Data Visualisation/othergraphs.html#enhanced-bubble-chart",
    "title": "Specialized Visualization Techniques",
    "section": "Enhanced Bubble Chart",
    "text": "Enhanced Bubble Chart\nLet’s improve this with better styling and add a fourth variable (species) using color:\n\n\nCode\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     size = bill_length_mm,\n                     color = species)) +\n  geom_point(alpha = 0.6, shape = 16) +\n  scale_size_continuous(range = c(2, 12)) +\n  scale_color_viridis_d() +\n  labs(title = \"Four Variables in One Visualization\",\n       subtitle = \"Bubble size represents bill length\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\",\n       size = \"Bill Length (mm)\",\n       color = \"Species\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe scale_size_continuous(range = c(2, 12)) controls the minimum and maximum bubble sizes.\n\n\n\n\n\n\nNote\n\n\n\nBubble charts are controversial because humans are better at judging length than area. Use them when the approximate relationship is more important than exact values.\n:::\n\nCorrelation Heatmaps\nHeatmaps display data values as colored cells, making patterns immediately visible. They’re particularly useful for comparing many variables at once.\n\nCreating a Correlation Matrix\nLet’s visualize correlations between penguin measurements:\n\n\nCode\n# Prepare data - select numeric columns only\npenguin_numeric &lt;- penguins %&gt;%\n  select(bill_length_mm, bill_depth_mm, \n         flipper_length_mm, body_mass_g) %&gt;%\n  na.omit()\n\n# Calculate correlation matrix\ncor_matrix &lt;- cor(penguin_numeric)\n\n# Convert to long format for ggplot\nlibrary(tidyr)\ncor_long &lt;- cor_matrix %&gt;%\n  as.data.frame() %&gt;%\n  mutate(var1 = rownames(.)) %&gt;%\n  pivot_longer(cols = -var1, \n               names_to = \"var2\", \n               values_to = \"correlation\")\n\n# Create heatmap\nggplot(cor_long, aes(x = var1, y = var2, fill = correlation)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = round(correlation, 2)), \n            color = \"black\", size = 4) +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\",\n                       midpoint = 0, limits = c(-1, 1)) +\n  labs(title = \"Correlation Matrix of Penguin Measurements\",\n       x = \"\", y = \"\", fill = \"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nStrong positive correlations appear in red, negative correlations in blue, and weak correlations in white.\n\n\n\nScatterplot Matrices\nA scatterplot matrix shows relationships between multiple variables in a grid format. It’s like seeing all possible pairs of scatterplots at once.\n\n\nCode\nlibrary(GGally)\n\n# Select variables for the matrix\npenguin_subset &lt;- penguins %&gt;%\n  select(bill_length_mm, bill_depth_mm, \n         flipper_length_mm, body_mass_g, species) %&gt;%\n  na.omit()\n\n# Create scatterplot matrix\nggpairs(penguin_subset, \n        aes(color = species, alpha = 0.5),\n        columns = 1:4) +\n  labs(title = \"Relationships Between Penguin Measurements\") +\n  theme_bw()\n\n\n\n\n\n\n\n\n\nReading the matrix:\n\n\n\n\nDiagonal: Distribution of each variable\nLower triangle: Scatterplots (row variable on y-axis, column variable on x-axis)\nUpper triangle: Correlation coefficients\n\n:::",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Specialized Visualization Techniques"
    ]
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#creating-a-correlation-matrix",
    "href": "Data Visualisation/othergraphs.html#creating-a-correlation-matrix",
    "title": "Specialized Visualization Techniques",
    "section": "Creating a Correlation Matrix",
    "text": "Creating a Correlation Matrix\nLet’s visualize correlations between penguin measurements:\n\n\nCode\n# Prepare data - select numeric columns only\npenguin_numeric &lt;- penguins %&gt;%\n  select(bill_length_mm, bill_depth_mm, \n         flipper_length_mm, body_mass_g) %&gt;%\n  na.omit()\n\n# Calculate correlation matrix\ncor_matrix &lt;- cor(penguin_numeric)\n\n# Convert to long format for ggplot\nlibrary(tidyr)\ncor_long &lt;- cor_matrix %&gt;%\n  as.data.frame() %&gt;%\n  mutate(var1 = rownames(.)) %&gt;%\n  pivot_longer(cols = -var1, \n               names_to = \"var2\", \n               values_to = \"correlation\")\n\n# Create heatmap\nggplot(cor_long, aes(x = var1, y = var2, fill = correlation)) +\n  geom_tile(color = \"white\") +\n  geom_text(aes(label = round(correlation, 2)), \n            color = \"black\", size = 4) +\n  scale_fill_gradient2(low = \"blue\", mid = \"white\", high = \"red\",\n                       midpoint = 0, limits = c(-1, 1)) +\n  labs(title = \"Correlation Matrix of Penguin Measurements\",\n       x = \"\", y = \"\", fill = \"Correlation\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n\n\n\n\nStrong positive correlations appear in red, negative correlations in blue, and weak correlations in white.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Specialized Visualization Techniques"
    ]
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#clustered-heatmap-for-groups",
    "href": "Data Visualisation/othergraphs.html#clustered-heatmap-for-groups",
    "title": "Specialized Visualization Techniques",
    "section": "Clustered Heatmap for Groups",
    "text": "Clustered Heatmap for Groups\nLet’s create a heatmap showing average measurements by species:",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Specialized Visualization Techniques"
    ]
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#custom-scatterplot-matrix",
    "href": "Data Visualisation/othergraphs.html#custom-scatterplot-matrix",
    "title": "Specialized Visualization Techniques",
    "section": "Custom Scatterplot Matrix",
    "text": "Custom Scatterplot Matrix\nYou can customize the appearance with your own functions:\n\n\nCode\n# Custom function for density plots\nmy_density &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_density(alpha = 0.6, fill = \"steelblue\")\n}\n\n# Custom function for scatterplots\nmy_scatter &lt;- function(data, mapping, ...) {\n  ggplot(data = data, mapping = mapping) + \n    geom_point(alpha = 0.4, color = \"steelblue\", size = 1.5) + \n    geom_smooth(method = \"lm\", se = FALSE, color = \"darkred\", linewidth = 0.8)\n}\n\n# Create customized matrix\npenguin_numeric_only &lt;- penguins %&gt;%\n  select(bill_length_mm, bill_depth_mm, \n         flipper_length_mm, body_mass_g) %&gt;%\n  na.omit()\n\nggpairs(penguin_numeric_only,\n        lower = list(continuous = my_scatter),\n        diag = list(continuous = my_density)) +\n  labs(title = \"Penguin Measurements: Detailed Relationships\") +\n  theme_minimal()",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Specialized Visualization Techniques"
    ]
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#basic-alluvial-diagram",
    "href": "Data Visualisation/othergraphs.html#basic-alluvial-diagram",
    "title": "Specialized Visualization Techniques",
    "section": "Basic Alluvial Diagram",
    "text": "Basic Alluvial Diagram\nLet’s examine how penguins are distributed across species, islands, and sex:\n\n\nCode\nlibrary(ggalluvial)\n\n# Prepare data\npenguin_flows &lt;- penguins %&gt;%\n  filter(!is.na(sex)) %&gt;%\n  group_by(species, island, sex) %&gt;%\n  summarise(count = n(), .groups = \"drop\")\n\n# Create alluvial diagram\nggplot(penguin_flows,\n       aes(axis1 = species,\n           axis2 = island,\n           axis3 = sex,\n           y = count)) +\n  geom_alluvium(aes(fill = species), alpha = 0.7) +\n  geom_stratum(alpha = 0.8) +\n  geom_text(stat = \"stratum\", aes(label = after_stat(stratum))) +\n  scale_x_discrete(limits = c(\"Species\", \"Island\", \"Sex\"),\n                   expand = c(0.1, 0.1)) +\n  scale_fill_viridis_d() +\n  labs(title = \"Penguin Distribution Across Categories\",\n       y = \"Count\") +\n  theme_minimal() +\n  theme(legend.position = \"none\")\n\n\n\n\n\n\n\n\n\nHow to read this:\n:::\n\nStart on the left with species\nFollow the flows to see how each species is distributed across islands\nContinue following to see the sex distribution\nFlow width represents the number of penguins\n\n:::",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Specialized Visualization Techniques"
    ]
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#enhanced-alluvial-diagram",
    "href": "Data Visualisation/othergraphs.html#enhanced-alluvial-diagram",
    "title": "Specialized Visualization Techniques",
    "section": "Enhanced Alluvial Diagram",
    "text": "Enhanced Alluvial Diagram\nLet’s create a more detailed version with better styling:\n\n\nCode\nggplot(penguin_flows,\n       aes(axis1 = species,\n           axis2 = island,\n           axis3 = sex,\n           y = count)) +\n  geom_alluvium(aes(fill = species), \n                color = \"black\", \n                alpha = 0.8,\n                curve_type = \"sigmoid\") +\n  geom_stratum(alpha = 0.9, color = \"white\", linewidth = 1) +\n  geom_text(stat = \"stratum\", \n            aes(label = after_stat(stratum)),\n            size = 4, fontface = \"bold\") +\n  scale_x_discrete(limits = c(\"Species\", \"Island\", \"Sex\"),\n                   expand = c(0.15, 0.15)) +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"How Penguins are Distributed\",\n       subtitle = \"Following the flow from species through island to sex\",\n       y = \"Number of Penguins\",\n       fill = \"Species\") +\n  theme_minimal() +\n  theme(axis.text.x = element_text(size = 12, face = \"bold\"))",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Specialized Visualization Techniques"
    ]
  },
  {
    "objectID": "Data Visualisation/othergraphs.html#waterfall-chart-with-net-total",
    "href": "Data Visualisation/othergraphs.html#waterfall-chart-with-net-total",
    "title": "Specialized Visualization Techniques",
    "section": "Waterfall Chart with Net Total",
    "text": "Waterfall Chart with Net Total\nLet’s add a total column to show the final balance:\n\n\nCode\nwaterfall(budget,\n          calc_total = TRUE,\n          total_axis_text = \"Remaining\",\n          total_rect_text_color = \"black\",\n          total_rect_color = \"gold\") +\n  scale_y_continuous(labels = scales::dollar) +\n  labs(title = \"Research Project Financial Flow\",\n       subtitle = \"Tracking income and expenses\",\n       x = \"\", y = \"\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThis shows clearly how the initial grant is allocated across different expenses.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Specialized Visualization Techniques"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#csv-and-text-files",
    "href": "Data Visualisation/importing data.html#csv-and-text-files",
    "title": "Data Import and Preparation in R",
    "section": "CSV and Text Files",
    "text": "CSV and Text Files\nThe readr package provides efficient functions for importing text-based data:\n\n\nCode\nlibrary(readr)\n\n# Import from a CSV file (comma-separated values)\npenguins_data &lt;- read_csv(\"penguins.csv\")\n\n# Import from a tab-delimited text file\npenguins_data &lt;- read_tsv(\"penguins.txt\")\n\n\nThese functions assume that:\n\n\nThe first row contains column names\nValues are separated by commas (CSV) or tabs (TSV)\nMissing data appears as blank cells\n\n\nHere’s what the first few lines of a CSV file look like:\nspecies,island,bill_length_mm,bill_depth_mm,flipper_length_mm,body_mass_g,sex\nAdelie,Torgersen,39.1,18.7,181,3750,male\nAdelie,Torgersen,39.5,17.4,186,3800,female\nAdelie,Torgersen,40.3,18.0,195,3250,female\n\n\n\n\n\n\nTip\n\n\n\nFor more options (like different delimiters or handling special cases), check the help documentation with ?read_csv.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#excel-files",
    "href": "Data Visualisation/importing data.html#excel-files",
    "title": "Data Import and Preparation in R",
    "section": "Excel Files",
    "text": "Excel Files\nThe readxl package handles Excel workbooks (both .xls and .xlsx):\n\n\nCode\nlibrary(readxl)\n\n# Import from an Excel file\npenguins_data &lt;- read_excel(\"penguins.xlsx\", sheet = 1)\n\n# Import from a specific worksheet by name\npenguins_data &lt;- read_excel(\"penguins.xlsx\", sheet = \"Measurements\")\n\n\nSince Excel files can have multiple worksheets, use the sheet option to specify which one you want. The default is the first sheet.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#files-from-other-statistical-software",
    "href": "Data Visualisation/importing data.html#files-from-other-statistical-software",
    "title": "Data Import and Preparation in R",
    "section": "Files from Other Statistical Software",
    "text": "Files from Other Statistical Software\nThe haven package lets you import data from other statistical programs without needing those programs installed:\n\n\nCode\nlibrary(haven)\n\n# Import from Stata\ndata_stata &lt;- read_dta(\"data.dta\")\n\n# Import from SPSS\ndata_spss &lt;- read_sav(\"data.sav\")\n\n# Import from SAS\ndata_sas &lt;- read_sas(\"data.sas7bdat\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#selecting-variables-columns",
    "href": "Data Visualisation/importing data.html#selecting-variables-columns",
    "title": "Data Import and Preparation in R",
    "section": "Selecting Variables (Columns)",
    "text": "Selecting Variables (Columns)\nUse select() to keep only the columns you need:\n\n\nCode\n# Keep just species, island, and body mass\npenguin_subset &lt;- select(penguins, species, island, body_mass_g)\n\nhead(penguin_subset)\n\n\n# A tibble: 6 × 3\n  species island    body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;           &lt;int&gt;\n1 Adelie  Torgersen        3750\n2 Adelie  Torgersen        3800\n3 Adelie  Torgersen        3250\n4 Adelie  Torgersen          NA\n5 Adelie  Torgersen        3450\n6 Adelie  Torgersen        3650\n\n\n\n\nCode\n# Keep species and all columns from bill_length_mm through body_mass_g\npenguin_subset &lt;- select(penguins, species, bill_length_mm:body_mass_g)\n\nhead(penguin_subset)\n\n\n# A tibble: 6 × 5\n  species bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;            &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie            39.1          18.7               181        3750\n2 Adelie            39.5          17.4               186        3800\n3 Adelie            40.3          18                 195        3250\n4 Adelie            NA            NA                  NA          NA\n5 Adelie            36.7          19.3               193        3450\n6 Adelie            39.3          20.6               190        3650\n\n\n\n\nCode\n# Keep everything except year and sex\npenguin_subset &lt;- select(penguins, -year, -sex)\n\nhead(penguin_subset)\n\n\n# A tibble: 6 × 6\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#filtering-observations-rows",
    "href": "Data Visualisation/importing data.html#filtering-observations-rows",
    "title": "Data Import and Preparation in R",
    "section": "Filtering Observations (Rows)",
    "text": "Filtering Observations (Rows)\nUse filter() to keep only rows that meet certain conditions. You can combine conditions with & (AND) and | (OR):\n\n\nCode\n# Select only Adelie penguins\nadelie_only &lt;- filter(penguins, species == \"Adelie\")\n\nhead(adelie_only)\n\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\nCode\n# Select Adelie penguins from Torgersen island\nadelie_torgersen &lt;- filter(penguins, \n                            species == \"Adelie\" & \n                            island == \"Torgersen\")\n\nhead(adelie_torgersen)\n\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\n\n\nCode\n# Select penguins from Biscoe, Dream, OR Torgersen islands\nthree_islands &lt;- filter(penguins, \n                        island == \"Biscoe\" | \n                        island == \"Dream\" | \n                        island == \"Torgersen\")\n\n# A more concise way to write the same thing:\nthree_islands &lt;- filter(penguins, \n                        island %in% c(\"Biscoe\", \"Dream\", \"Torgersen\"))\n\nhead(three_islands)\n\n\n# A tibble: 6 × 8\n  species island    bill_length_mm bill_depth_mm flipper_length_mm body_mass_g\n  &lt;fct&gt;   &lt;fct&gt;              &lt;dbl&gt;         &lt;dbl&gt;             &lt;int&gt;       &lt;int&gt;\n1 Adelie  Torgersen           39.1          18.7               181        3750\n2 Adelie  Torgersen           39.5          17.4               186        3800\n3 Adelie  Torgersen           40.3          18                 195        3250\n4 Adelie  Torgersen           NA            NA                  NA          NA\n5 Adelie  Torgersen           36.7          19.3               193        3450\n6 Adelie  Torgersen           39.3          20.6               190        3650\n# ℹ 2 more variables: sex &lt;fct&gt;, year &lt;int&gt;\n\n\nCommon comparison operators:\n\n\n== equals\n!= not equals\n&gt; greater than\n&lt; less than\n&gt;= greater than or equal to\n&lt;= less than or equal to",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#creating-and-modifying-variables",
    "href": "Data Visualisation/importing data.html#creating-and-modifying-variables",
    "title": "Data Import and Preparation in R",
    "section": "Creating and Modifying Variables",
    "text": "Creating and Modifying Variables\nUse mutate() to create new columns or modify existing ones:\n\n\nCode\n# Convert body mass from grams to kilograms\n# Convert flipper length from mm to cm\npenguins_converted &lt;- mutate(penguins,\n                              body_mass_kg = body_mass_g / 1000,\n                              flipper_length_cm = flipper_length_mm / 10)\n\nselect(penguins_converted, species, body_mass_g, body_mass_kg, \n       flipper_length_mm, flipper_length_cm) %&gt;%\n  head()\n\n\n# A tibble: 6 × 5\n  species body_mass_g body_mass_kg flipper_length_mm flipper_length_cm\n  &lt;fct&gt;         &lt;int&gt;        &lt;dbl&gt;             &lt;int&gt;             &lt;dbl&gt;\n1 Adelie         3750         3.75               181              18.1\n2 Adelie         3800         3.8                186              18.6\n3 Adelie         3250         3.25               195              19.5\n4 Adelie           NA        NA                   NA              NA  \n5 Adelie         3450         3.45               193              19.3\n6 Adelie         3650         3.65               190              19  \n\n\n\nRecoding with ifelse()\nThe ifelse() function helps you create categorical variables based on conditions:\n\n\nCode\n# Create a size category based on body mass\npenguins_sized &lt;- mutate(penguins,\n                         size = ifelse(body_mass_g &gt; 4500,\n                                      \"large\",\n                                      \"small\"))\n\nselect(penguins_sized, species, body_mass_g, size) %&gt;%\n  head()\n\n\n# A tibble: 6 × 3\n  species body_mass_g size \n  &lt;fct&gt;         &lt;int&gt; &lt;chr&gt;\n1 Adelie         3750 small\n2 Adelie         3800 small\n3 Adelie         3250 small\n4 Adelie           NA &lt;NA&gt; \n5 Adelie         3450 small\n6 Adelie         3650 small\n\n\n\n\nCode\n# Create a new island variable with simplified names\npenguins_recoded &lt;- mutate(penguins,\n                           island_short = ifelse(island %in% c(\"Biscoe\", \"Dream\"),\n                                                island,\n                                                \"Other\"))\n\nselect(penguins_recoded, island, island_short) %&gt;%\n  head()\n\n\n# A tibble: 6 × 2\n  island    island_short\n  &lt;fct&gt;     &lt;chr&gt;       \n1 Torgersen Other       \n2 Torgersen Other       \n3 Torgersen Other       \n4 Torgersen Other       \n5 Torgersen Other       \n6 Torgersen Other       \n\n\n\n\nCode\n# Set extreme values to missing (NA)\npenguins_cleaned &lt;- mutate(penguins,\n                           bill_length_mm = ifelse(bill_length_mm &lt; 30 | \n                                                   bill_length_mm &gt; 60,\n                                                   NA,\n                                                   bill_length_mm))",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#summarizing-data",
    "href": "Data Visualisation/importing data.html#summarizing-data",
    "title": "Data Import and Preparation in R",
    "section": "Summarizing Data",
    "text": "Summarizing Data\nUse summarize() to calculate summary statistics. Combine it with group_by() to get statistics for each group:\n\n\nCode\n# Calculate overall mean flipper length and body mass\nsummary_stats &lt;- summarize(penguins,\n                          mean_flipper = mean(flipper_length_mm, na.rm = TRUE),\n                          mean_mass = mean(body_mass_g, na.rm = TRUE))\n\nsummary_stats\n\n\n# A tibble: 1 × 2\n  mean_flipper mean_mass\n         &lt;dbl&gt;     &lt;dbl&gt;\n1         201.     4202.\n\n\n\n\n\n\n\n\nNote\n\n\n\nThe na.rm = TRUE option tells R to ignore missing values when calculating means.\n\n\n\n\nCode\n# Calculate mean flipper length and body mass by species\nspecies_summary &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(mean_flipper = mean(flipper_length_mm, na.rm = TRUE),\n            mean_mass = mean(body_mass_g, na.rm = TRUE),\n            count = n())\n\nspecies_summary\n\n\n# A tibble: 3 × 4\n  species   mean_flipper mean_mass count\n  &lt;fct&gt;            &lt;dbl&gt;     &lt;dbl&gt; &lt;int&gt;\n1 Adelie            190.     3701.   152\n2 Chinstrap         196.     3733.    68\n3 Gentoo            217.     5076.   124\n\n\n\n\nCode\n# Calculate statistics by species AND island\ndetailed_summary &lt;- penguins %&gt;%\n  group_by(species, island) %&gt;%\n  summarize(mean_bill_length = mean(bill_length_mm, na.rm = TRUE),\n            sd_bill_length = sd(bill_length_mm, na.rm = TRUE),\n            count = n(),\n            .groups = \"drop\")\n\ndetailed_summary\n\n\n# A tibble: 5 × 5\n  species   island    mean_bill_length sd_bill_length count\n  &lt;fct&gt;     &lt;fct&gt;                &lt;dbl&gt;          &lt;dbl&gt; &lt;int&gt;\n1 Adelie    Biscoe                39.0           2.48    44\n2 Adelie    Dream                 38.5           2.47    56\n3 Adelie    Torgersen             39.0           3.03    52\n4 Chinstrap Dream                 48.8           3.34    68\n5 Gentoo    Biscoe                47.5           3.08   124\n\n\nMany visualizations work best with summarized data rather than raw observations.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#using-the-pipe-operator",
    "href": "Data Visualisation/importing data.html#using-the-pipe-operator",
    "title": "Data Import and Preparation in R",
    "section": "Using the Pipe Operator",
    "text": "Using the Pipe Operator\nThe pipe operator %&gt;% (or |&gt; in newer R versions) lets you chain operations together, making your code cleaner and easier to read:\n\n\nCode\n# Without pipes - harder to read\ntemp1 &lt;- filter(penguins, species == \"Adelie\")\ntemp2 &lt;- group_by(temp1, island)\nresult &lt;- summarize(temp2, mean_mass = mean(body_mass_g, na.rm = TRUE))\n\nresult\n\n\n# A tibble: 3 × 2\n  island    mean_mass\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Biscoe        3710.\n2 Dream         3688.\n3 Torgersen     3706.\n\n\n\n\nCode\n# With pipes - much clearer!\nresult &lt;- penguins %&gt;%\n  filter(species == \"Adelie\") %&gt;%\n  group_by(island) %&gt;%\n  summarize(mean_mass = mean(body_mass_g, na.rm = TRUE))\n\nresult\n\n\n# A tibble: 3 × 2\n  island    mean_mass\n  &lt;fct&gt;         &lt;dbl&gt;\n1 Biscoe        3710.\n2 Dream         3688.\n3 Torgersen     3706.\n\n\nThe pipe takes the output from the left side and passes it as the first argument to the function on the right side.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#working-with-dates",
    "href": "Data Visualisation/importing data.html#working-with-dates",
    "title": "Data Import and Preparation in R",
    "section": "Working with Dates",
    "text": "Working with Dates\nDates often come into R as text. The lubridate package makes it easy to convert them to proper date format:\n\n\nCode\nlibrary(lubridate)\n\n# Create a sample dataset with dates in various formats\nsample_dates &lt;- data.frame(\n  person = c(\"Alice\", \"Bob\", \"Charlie\"),\n  birth_date = c(\"03/15/1995\", \"Jul-22-98\", \"1:10:2000\")\n)\n\n# View the structure - dates are currently characters\nstr(sample_dates)\n\n\n'data.frame':   3 obs. of  2 variables:\n $ person    : chr  \"Alice\" \"Bob\" \"Charlie\"\n $ birth_date: chr  \"03/15/1995\" \"Jul-22-98\" \"1:10:2000\"\n\n\n\n\nCode\n# Convert to proper date format\nsample_dates$birth_date &lt;- mdy(sample_dates$birth_date)\n\n# Now they're proper dates\nstr(sample_dates)\n\n\n'data.frame':   3 obs. of  2 variables:\n $ person    : chr  \"Alice\" \"Bob\" \"Charlie\"\n $ birth_date: Date, format: \"1995-03-15\" \"1998-07-22\" ...\n\n\nCommon date conversion functions: :::{}\n\nymd() for year-month-day format (2024-01-15)\nmdy() for month-day-year format (01/15/2024)\ndmy() for day-month-year format (15-01-2024)\n\n:::\nOnce converted, you can do date arithmetic and extract components:\n\n\nCode\n# Calculate age in years\nsample_dates$age &lt;- as.numeric(difftime(Sys.Date(), \n                                        sample_dates$birth_date, \n                                        units = \"weeks\")) / 52.25\n\nsample_dates\n\n\n   person birth_date      age\n1   Alice 1995-03-15 30.81887\n2     Bob 1998-07-22 27.46958\n3 Charlie 2000-01-10 26.00137",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#reshaping-data-wide-vs.-long-format",
    "href": "Data Visualisation/importing data.html#reshaping-data-wide-vs.-long-format",
    "title": "Data Import and Preparation in R",
    "section": "Reshaping Data: Wide vs. Long Format",
    "text": "Reshaping Data: Wide vs. Long Format\nSome analyses require data in “wide” format, others need “long” format.\nWide format (each measurement type gets its own column):\n\n\nCode\n# Create example wide data\nwide_data &lt;- data.frame(\n  id = c(\"P01\", \"P02\", \"P03\"),\n  name = c(\"Alice\", \"Bob\", \"Charlie\"),\n  height_cm = c(165, 178, 172),\n  weight_kg = c(62, 75, 70)\n)\n\nwide_data\n\n\n   id    name height_cm weight_kg\n1 P01   Alice       165        62\n2 P02     Bob       178        75\n3 P03 Charlie       172        70\n\n\nLong format (all measurements in one column):\n\n\nCode\nlibrary(tidyr)\n\n# Convert wide to long\nlong_data &lt;- pivot_longer(wide_data,\n                          cols = c(height_cm, weight_kg),\n                          names_to = \"measurement\",\n                          values_to = \"value\")\n\nlong_data\n\n\n# A tibble: 6 × 4\n  id    name    measurement value\n  &lt;chr&gt; &lt;chr&gt;   &lt;chr&gt;       &lt;dbl&gt;\n1 P01   Alice   height_cm     165\n2 P01   Alice   weight_kg      62\n3 P02   Bob     height_cm     178\n4 P02   Bob     weight_kg      75\n5 P03   Charlie height_cm     172\n6 P03   Charlie weight_kg      70\n\n\nConverting back to wide:\n\n\nCode\n# Convert long to wide\nback_to_wide &lt;- pivot_wider(long_data,\n                            names_from = measurement,\n                            values_from = value)\n\nback_to_wide\n\n\n# A tibble: 3 × 4\n  id    name    height_cm weight_kg\n  &lt;chr&gt; &lt;chr&gt;       &lt;dbl&gt;     &lt;dbl&gt;\n1 P01   Alice         165        62\n2 P02   Bob           178        75\n3 P03   Charlie       172        70\n\n\nLong format is often required for ggplot2, while wide format is easier for humans to read.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#checking-for-missing-values",
    "href": "Data Visualisation/importing data.html#checking-for-missing-values",
    "title": "Data Import and Preparation in R",
    "section": "Checking for Missing Values",
    "text": "Checking for Missing Values\nFirst, see how much data is missing:\n\n\nCode\n# Count missing values in each column\ncolSums(is.na(penguins))\n\n\n          species            island    bill_length_mm     bill_depth_mm \n                0                 0                 2                 2 \nflipper_length_mm       body_mass_g               sex              year \n                2                 2                11                 0 \n\n\n\n\nCode\n# Calculate percentage missing\nmissing_pct &lt;- colSums(is.na(penguins)) / nrow(penguins) * 100\nround(missing_pct, 1)\n\n\n          species            island    bill_length_mm     bill_depth_mm \n              0.0               0.0               0.6               0.6 \nflipper_length_mm       body_mass_g               sex              year \n              0.6               0.6               3.2               0.0",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#approach-1-remove-variables-with-too-many-missing-values",
    "href": "Data Visualisation/importing data.html#approach-1-remove-variables-with-too-many-missing-values",
    "title": "Data Import and Preparation in R",
    "section": "Approach 1: Remove Variables with Too Many Missing Values",
    "text": "Approach 1: Remove Variables with Too Many Missing Values\nIf a variable has too many missing values, it might not be useful. But in small quantities, it could be justified. Below is an example of how you can approach this in terms of percentage of NA values:\n\n\nCode\n# Remove columns with more than 1% missing data\nthreshold &lt;- 0.01\nmissing_prop &lt;- colSums(is.na(penguins)) / nrow(penguins)\n\npenguins_cleaned &lt;- penguins %&gt;%\n  select(where(~mean(is.na(.)) &lt; threshold))\n\nncol(penguins)  # original number of columns\n\n\n[1] 8\n\n\nCode\nncol(penguins_cleaned)  # after removal\n\n\n[1] 7\n\n\nAs we can see, one column was removed as it had more NA values than the threshold of 1%. This threshold was arbitrarily selected by me for demonstrative purposes. In real analyses, you must justify this step and threshold you set.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#approach-2-listwise-deletion",
    "href": "Data Visualisation/importing data.html#approach-2-listwise-deletion",
    "title": "Data Import and Preparation in R",
    "section": "Approach 2: Listwise Deletion",
    "text": "Approach 2: Listwise Deletion\nRemove any rows that contain missing values:\n\n\nCode\n# Create dataset with only complete cases\npenguins_complete &lt;- penguins %&gt;%\n  select(species, bill_length_mm, bill_depth_mm, body_mass_g) %&gt;%\n  na.omit()\n\nnrow(penguins)  # original rows\n\n\n[1] 344\n\n\nCode\nnrow(penguins_complete)  # after deletion\n\n\n[1] 342\n\n\n\n\n\n\n\n\nWarning\n\n\n\nThis can remove a lot of data if missingness is spread across many rows!",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#approach-3-imputation",
    "href": "Data Visualisation/importing data.html#approach-3-imputation",
    "title": "Data Import and Preparation in R",
    "section": "Approach 3: Imputation",
    "text": "Approach 3: Imputation\nImputation means replacing missing values with educated guesses. The VIM package offers sophisticated methods:\n\n\nCode\nlibrary(VIM)\n\n# Impute missing values using k-nearest neighbors\npenguins_imputed &lt;- kNN(penguins, k = 5)\n\n\nThis method finds the 5 most similar complete cases and uses their median (for numeric variables) or most common value (for categorical variables) to fill in the missing data.\n\n\n\n\n\n\nImportant\n\n\n\nMissing data can seriously bias your results. If you have substantial missing data, consult with a statistician before deciding how to handle it.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/importing data.html#best-practices-for-missing-data",
    "href": "Data Visualisation/importing data.html#best-practices-for-missing-data",
    "title": "Data Import and Preparation in R",
    "section": "Best Practices for Missing Data",
    "text": "Best Practices for Missing Data\n\nInvestigate why data is missing - Is it random or systematic?\nDocument your decisions - Always note how you handled missing values\nCompare methods - Try multiple approaches and see how results differ\nReport missingness - Tell readers how much data was missing",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Data Import and Preparation in R"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#geoms",
    "href": "Data Visualisation/baseR and ggplot.html#geoms",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.1 Geoms",
    "text": "2.1 Geoms\n“Geoms” (geometric objects) represent the actual marks on the plot: points, bars, lines, etc. We add these using functions starting with geom_. To create a scatterplot, we use geom_point().\nIn ggplot2, we chain these layers together using the + operator.\n\n\nCode\n```{r}\n# Add geometric points\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\n```\n\n\n\n\n\n\n\n\n\nThe plot above shows a strong positive correlation: as flipper length increases, body mass tends to increase.\nWe can customize the appearance of these geoms. Parameters like color, size, and alpha (transparency) can be set directly inside the geom function. Transparency is particularly useful when you have many points overlapping each other.\n\n\nCode\n```{r}\n# Make points blue, larger, and semi-transparent\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point(color = \"darkcyan\", alpha = 0.6, size = 3)\n```",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#smoothers-trend-lines",
    "href": "Data Visualisation/baseR and ggplot.html#smoothers-trend-lines",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.2 Smoothers (Trend Lines)",
    "text": "2.2 Smoothers (Trend Lines)\nTo visualize the trend more clearly, we can add a smoothing layer using geom_smooth(). We can control the method (linear vs. curved), the color, and whether to show the confidence interval. Here, we’ll use a linear model (method = \"lm\").\n\n\nCode\n# Add a line of best fit\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point(color = \"darkcyan\", alpha = 0.6, size = 3) + \n  geom_smooth(method = \"lm\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#grouping-with-aesthetics",
    "href": "Data Visualisation/baseR and ggplot.html#grouping-with-aesthetics",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.3 Grouping with Aesthetics",
    "text": "2.3 Grouping with Aesthetics\nWhile the plot above is informative, it treats all penguins as one homogeneous group. We know our dataset contains different Species.\nWe can map variables to visual characteristics like color or shape inside the aes() function. This allows us to superimpose groups on a single graph. Let’s map species to color.\n\n\nCode\n# Map species to color\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) + \n  geom_point(alpha = 0.6, size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1.5)\n\n\n\n\n\n\n\n\n\nNotice the difference:\n\ncolor = species is inside aes() because it maps data to a visual.\nse = FALSE was added to geom_smooth to remove the shaded confidence intervals for a cleaner look.\n\nWe can now see that Gentoo penguins are generally larger than Adelie or Chinstrap penguins.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#scales",
    "href": "Data Visualisation/baseR and ggplot.html#scales",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.4 Scales",
    "text": "2.4 Scales\n“Scales” control how the data values are translated into visual properties (like specific colors or axis ticks). Scale functions always start with scale_.\nLet’s modify the X-axis breaks and manually define the colors for our species to be more distinct.\n\n\nCode\n# Customize axes and color palette\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) + \n  geom_point(alpha = 0.6, size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 1.5) + \n  scale_x_continuous(breaks = seq(170, 230, 10)) + \n  scale_y_continuous(breaks = seq(2500, 6500, 1000)) +\n  scale_color_manual(values = c(\"coral4\", \"purple\", \"cyan4\"))",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#facets",
    "href": "Data Visualisation/baseR and ggplot.html#facets",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.5 Facets",
    "text": "2.5 Facets\nSometimes a plot becomes too cluttered with multiple groups. Faceting solves this by splitting the plot into sub-plots based on a categorical variable.\nLet’s see if the relationship holds true across the different “Weight Classes” we defined earlier.\n\n\nCode\n# Split the plot by weight class\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) + \n  geom_point(alpha = 0.6) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) + \n  facet_wrap(~weight_class)\n\n\n\n\n\n\n\n\n\nFaceting allows us to simultaneously view the data cut by different dimensions without overlapping points obscuring the view.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#labels",
    "href": "Data Visualisation/baseR and ggplot.html#labels",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.6 Labels",
    "text": "2.6 Labels\nGood data visualization requires clear communication. The labs() function allows you to customize the title, subtitle, captions, and axis labels so the viewer doesn’t have to guess what “flipper_length_mm” means.\n\n\nCode\n```{r}\n# Add infomative labels\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) + \n  geom_point(alpha = 0.6) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) + \n  facet_wrap(~weight_class) + \n  labs(\n    title = \"Penguin Size Characteristics\",\n    subtitle = \"Comparison of Flipper Length vs Body Mass by Species\",\n    caption = \"Source: Palmer Station LTER / palmerpenguins package\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Penguin Species\"\n  )\n```\n\n\n`geom_smooth()` using formula = 'y ~ x'",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#themes",
    "href": "Data Visualisation/baseR and ggplot.html#themes",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.7 Themes",
    "text": "2.7 Themes\nFinally, we can polish the overall look using Themes. Theme functions (starting with theme_) control non-data elements like background colors, fonts, and grid lines. theme_minimal() is a popular choice for a clean, modern look.\n\n\nCode\n# Apply a minimal theme\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) + \n  geom_point(alpha = 0.6) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) + \n  facet_wrap(~weight_class) + \n  labs(\n    title = \"Penguin Size Characteristics\",\n    subtitle = \"Comparison of Flipper Length vs Body Mass by Species\",\n    caption = \"Source: Palmer Station LTER / palmerpenguins package\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Penguin Species\"\n  ) + \n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nYou can manipulate the legend of your graph with the theme function. This can be added with + and is different from the theme_minimal() in the code above. For example if you want to adjust its’ position, use theme(legend.position = ) Options for legend.position: * \"top\", \"bottom\", \"left\", \"right\" * c(x, y) for exact placement (e.g., c(0.9, 0.2) for bottom-right inside plot) * \"none\" to remove the legend",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#global-vs-local-mapping",
    "href": "Data Visualisation/baseR and ggplot.html#global-vs-local-mapping",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "3.1 Global vs Local Mapping",
    "text": "3.1 Global vs Local Mapping\nIn the previous examples, we defined our mapping aes(...) inside the main ggplot() function. This is a Global mapping, as in it applies to every layer (both points and lines).\nHowever, you can also place mappings inside a specific Geom. This creates a Local mapping that applies only to that layer.\nObserve the difference when we move color = species into the geom_point() function only:\n\n\nCode\n```{r}\n# Mapping color LOCALLY in geom_point only\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point(aes(color = species), alpha = 0.6, size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\")\n```\n\n\n`geom_smooth()` using formula = 'y ~ x'\n\n\n\n\n\n\n\n\n\nBecause the color mapping was only inside geom_point, the geom_smooth did not “know” about the species groups. Consequently, it drew a single black trend line for the entire dataset, rather than one line per species.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#graphs-as-objects",
    "href": "Data Visualisation/baseR and ggplot.html#graphs-as-objects",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "3.2 Graphs as Objects",
    "text": "3.2 Graphs as Objects\nIn R, a graph is just an object. You can save it to a variable, modify it later, and print it when you want to. This is great for keeping your code clean or generating multiple versions of a plot programmatically.\n\n\nCode\n```{r}\n# 1. Create the base plot object\nmy_plot &lt;- ggplot(data = penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point(aes(color = species))\n\n# 2. Add a theme and labels to the object\nfinal_plot &lt;- my_plot + \n  theme_light() + \n  labs(title = \"Saved Graph Object\")\n\n# 3. Print the final result\nfinal_plot\n```",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#geoms-1",
    "href": "Data Visualisation/baseR and ggplot.html#geoms-1",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.1 Geoms",
    "text": "2.1 Geoms\n“Geoms” (geometric objects) represent the actual marks on the plot—points, bars, lines, etc. We add these using functions starting with geom_. To create a scatterplot, we use geom_point().\nIn ggplot2, we chain these layers together using the + operator.\nquarto-executable-code-5450563D\n# Add geometric points\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point()\nThe plot above shows a strong positive correlation: as flipper length increases, body mass tends to increase.\nWe can customize the appearance of these geoms. Parameters like color, size, and alpha (transparency) can be set directly inside the geom function. Transparency is particularly useful when you have many points overlapping each other.\nquarto-executable-code-5450563D\n# Make points blue, larger, and semi-transparent\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point(color = \"darkcyan\", alpha = 0.6, size = 3)",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#smoothers-trend-lines-1",
    "href": "Data Visualisation/baseR and ggplot.html#smoothers-trend-lines-1",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.2 Smoothers (Trend Lines)",
    "text": "2.2 Smoothers (Trend Lines)\nTo visualize the trend more clearly, we can add a smoothing layer using geom_smooth(). We can control the method (linear vs. curved), the color, and whether to show the confidence interval. Here, we’ll use a linear model (method = \"lm\").\nquarto-executable-code-5450563D\n# Add a line of best fit\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point(color = \"darkcyan\", alpha = 0.6, size = 3) + \n  geom_smooth(method = \"lm\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#grouping-with-aesthetics-1",
    "href": "Data Visualisation/baseR and ggplot.html#grouping-with-aesthetics-1",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.3 Grouping with Aesthetics",
    "text": "2.3 Grouping with Aesthetics\nWhile the plot above is informative, it treats all penguins as one homogeneous group. We know our dataset contains different Species.\nWe can map variables to visual characteristics like color or shape inside the aes() function. This allows us to superimpose groups on a single graph. Let’s map species to color.\nquarto-executable-code-5450563D\n# Map species to color\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) + \n  geom_point(alpha = 0.6, size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 1.5)\nNotice the difference:\n\ncolor = species is inside aes() because it maps data to a visual.\nse = FALSE was added to geom_smooth to remove the shaded confidence intervals for a cleaner look.\n\nWe can now see that Gentoo penguins are generally larger than Adelie or Chinstrap penguins.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#scales-1",
    "href": "Data Visualisation/baseR and ggplot.html#scales-1",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.4 Scales",
    "text": "2.4 Scales\n“Scales” control how the data values are translated into visual properties (like specific colors or axis ticks). Scale functions always start with scale_.\nLet’s modify the X-axis breaks and manually define the colors for our species to be more distinct.\nquarto-executable-code-5450563D\n# Customize axes and color palette\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) + \n  geom_point(alpha = 0.6, size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE, size = 1.5) + \n  scale_x_continuous(breaks = seq(170, 230, 10)) + \n  scale_y_continuous(breaks = seq(2500, 6500, 1000)) +\n  scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\"))",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#facets-1",
    "href": "Data Visualisation/baseR and ggplot.html#facets-1",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.5 Facets",
    "text": "2.5 Facets\nSometimes a plot becomes too cluttered with multiple groups. Faceting solves this by splitting the plot into sub-plots (small multiples) based on a categorical variable.\nLet’s see if the relationship holds true across the different “Weight Classes” we defined earlier.\nquarto-executable-code-5450563D\n# Split the plot by weight class\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) + \n  geom_point(alpha = 0.6) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) + \n  facet_wrap(~weight_class)\nFaceting allows us to simultaneously view the data cut by different dimensions without overlapping points obscuring the view.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#labels-1",
    "href": "Data Visualisation/baseR and ggplot.html#labels-1",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.6 Labels",
    "text": "2.6 Labels\nGood data visualization requires clear communication. The labs() function allows you to customize the title, subtitle, captions, and axis labels so the viewer doesn’t have to guess what “flipper_length_mm” means.\nquarto-executable-code-5450563D\n# Add professional labels\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) + \n  geom_point(alpha = 0.6) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) + \n  facet_wrap(~weight_class) + \n  labs(\n    title = \"Penguin Size Characteristics\",\n    subtitle = \"Comparison of Flipper Length vs Body Mass by Species\",\n    caption = \"Source: Palmer Station LTER / palmerpenguins package\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Penguin Species\"\n  )",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#themes-1",
    "href": "Data Visualisation/baseR and ggplot.html#themes-1",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "2.7 Themes",
    "text": "2.7 Themes\nFinally, we can polish the overall look using Themes. Theme functions (starting with theme_) control non-data elements like background colors, fonts, and grid lines. theme_minimal() is a popular choice for a clean, modern look.\nquarto-executable-code-5450563D\n# Apply a minimal theme\nggplot(data = penguins, mapping = aes(x = flipper_length_mm, y = body_mass_g, color = species)) + \n  geom_point(alpha = 0.6) + \n  geom_smooth(method = \"lm\", se = FALSE) + \n  scale_color_manual(values = c(\"darkorange\", \"purple\", \"cyan4\")) + \n  facet_wrap(~weight_class) + \n  labs(\n    title = \"Penguin Size Characteristics\",\n    subtitle = \"Comparison of Flipper Length vs Body Mass by Species\",\n    caption = \"Source: Palmer Station LTER\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Penguin Species\"\n  ) + \n  theme_minimal()\nFrom this final plot, we can conclude:\n\nThere is a positive linear relationship between flipper length and body mass.\nGentoo penguins (cyan) are distinctively larger than the other two species.\nThe relationship between size and mass appears consistent across species (similar slopes).",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#global-vs-local-mapping-1",
    "href": "Data Visualisation/baseR and ggplot.html#global-vs-local-mapping-1",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "3.1 Global vs Local Mapping",
    "text": "3.1 Global vs Local Mapping\nIn the previous examples, we defined our mapping aes(...) inside the main ggplot() function. This is a Global mapping—it applies to every layer (both points and lines).\nHowever, you can also place mappings inside a specific Geom. This creates a Local mapping that applies only to that layer.\nObserve the difference when we move color = species into the geom_point() function only:\nquarto-executable-code-5450563D\n# Mapping color LOCALLY in geom_point only\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point(aes(color = species), alpha = 0.6, size = 3) + \n  geom_smooth(method = \"lm\", se = FALSE, color = \"black\")\nBecause the color mapping was only inside geom_point, the geom_smooth did not “know” about the species groups. Consequently, it drew a single black trend line for the entire dataset, rather than one line per species.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/baseR and ggplot.html#graphs-as-objects-1",
    "href": "Data Visualisation/baseR and ggplot.html#graphs-as-objects-1",
    "title": "Introduction to ggplot2: The Grammar of Graphics",
    "section": "3.2 Graphs as Objects",
    "text": "3.2 Graphs as Objects\nIn R, a graph is just an object. You can save it to a variable, modify it later, and print it when ready. This is excellent for keeping your code clean or generating multiple versions of a plot programmatically.\nquarto-executable-code-5450563D\n# 1. Create the base plot object\nmy_plot &lt;- ggplot(data = penguins, aes(x = flipper_length_mm, y = body_mass_g)) + \n  geom_point(aes(color = species))\n\n# 2. Add a theme and labels to the object\nfinal_plot &lt;- my_plot + \n  theme_light() + \n  labs(title = \"Saved Graph Object\")\n\n# 3. Print the final result\nfinal_plot",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Introduction to ggplot2: The Grammar of Graphics"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#numeric-axes",
    "href": "Data Visualisation/customising graphs.html#numeric-axes",
    "title": "Graph Customisation",
    "section": "Numeric Axes",
    "text": "Numeric Axes\nYou can control numeric axes using scale_x_continuous() and scale_y_continuous(). The most useful options are:\n\nbreaks: Where to place tick marks and labels\nlimits: The minimum and maximum values to display\n\n\n\nCode\n# Basic plot\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point() +\n  scale_x_continuous(breaks = seq(30, 60, by = 5),\n                     limits = c(30, 60)) +\n  scale_y_continuous(breaks = seq(13, 22, by = 2),\n                     limits = c(13, 22)) +\n  labs(title = \"Penguin Bill Dimensions\",\n       x = \"Bill Length (mm)\",\n       y = \"Bill Depth (mm)\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nTip\n\n\n\nThe seq() function creates a sequence of numbers. For example, seq(30, 60, by = 5) creates: 30, 35, 40, 45, 50, 55, 60.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#formatting-numbers",
    "href": "Data Visualisation/customising graphs.html#formatting-numbers",
    "title": "Graph Customisation",
    "section": "Formatting Numbers",
    "text": "Formatting Numbers\nThe scales package provides helpful functions for formatting axis labels:\n\ncomma: Adds commas to large numbers (1000 becomes 1,000)\ndollar: Adds currency symbols\npercent: Converts decimals to percentages\n\nIn the code chunk below, we create a random dataset by sampling from normal and uniform distributions for demomstrative purposes. Don’t worry, the penguins will come back in the next example!\n\n\nCode\n```{r}\n# Create sample data\nset.seed(42)\npopulation_data &lt;- data.frame(\n  city = paste(\"City\", 1:40),\n  population = rnorm(40, 250000, 80000),\n  employment_rate = runif(40, 0.55, 0.85),\n  median_income = rnorm(40, 65000, 15000)\n)\n\nggplot(population_data, aes(x = population, y = employment_rate)) +\n  geom_point(color = \"steelblue\", size = 3, alpha = 0.6) +\n  scale_x_continuous(labels = scales::comma) +\n  scale_y_continuous(labels = scales::percent) +\n  labs(title = \"Population vs Employment Rate\",\n       x = \"City Population\",\n       y = \"Employment Rate\")\n```",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#categorical-axes",
    "href": "Data Visualisation/customising graphs.html#categorical-axes",
    "title": "Graph Customisation",
    "section": "Categorical Axes",
    "text": "Categorical Axes\nFor categorical variables, use scale_x_discrete() or scale_y_discrete(). Returning to our penguins dataset:\n\n\nCode\n```{r}\n# Apply categorical variable scaling\nggplot(penguins, aes(x = species)) +\n  geom_bar(fill = \"coral\") +\n  scale_x_discrete(labels = c(\"Adélie\\nPenguin\", \n                              \"Chinstrap\\nPenguin\", \n                              \"Gentoo\\nPenguin\")) +\n  labs(title = \"Number of Penguins by Species\",\n       x = \"Species\",\n       y = \"Count\")\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\n\\n creates a line break in text labels.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#date-axes",
    "href": "Data Visualisation/customising graphs.html#date-axes",
    "title": "Graph Customisation",
    "section": "Date Axes",
    "text": "Date Axes\nWorking with dates requires scale_x_date() or scale_y_date(). We use the economics dataset for this:\n\n\nCode\n```{r}\n# Using the economics dataset\nggplot(economics, aes(x = date, y = unemploy / 1000)) +\n  geom_line(color = \"darkblue\", linewidth = 1) +\n  scale_x_date(date_breaks = \"10 years\",\n               date_labels = \"%Y\") +\n  labs(title = \"US Unemployment Over Time\",\n       x = \"Year\",\n       y = \"Unemployed (thousands)\")\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nWe use date_ for breaks and labels for dates.\n\n\nCommon date format codes:\n\n\n\nCode\nMeaning\nExample\n\n\n\n\n%d\nDay of month\n01-31\n\n\n%m\nMonth number\n01-12\n\n\n%b\nAbbreviated month\nJan\n\n\n%B\nFull month\nJanuary\n\n\n%y\n2-digit year\n24\n\n\n%Y\n4-digit year\n2024",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#setting-colors-manually",
    "href": "Data Visualisation/customising graphs.html#setting-colors-manually",
    "title": "Graph Customisation",
    "section": "Setting Colors Manually",
    "text": "Setting Colors Manually\nUse color for points and lines, and fill for bars and areas:\n\n\nCode\n# Single color\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(color = \"darkgreen\", size = 2) +\n  labs(title = \"Penguin Bill Measurements\")\n\n\n\n\n\n\n\n\n\nTo assign specific colors to categories, use scale_color_manual() or scale_fill_manual():\n\n\nCode\nggplot(penguins, aes(x = species, fill = species)) +\n  geom_bar() +\n  scale_fill_manual(values = c(\"Adelie\" = \"darkorange\",\n                                \"Chinstrap\" = \"purple\",\n                                \"Gentoo\" = \"cyan4\")) +\n  labs(title = \"Penguin Species Count\",\n       x = \"Species\",\n       y = \"Count\",\n       fill = \"Species\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#using-color-palettes",
    "href": "Data Visualisation/customising graphs.html#using-color-palettes",
    "title": "Graph Customisation",
    "section": "Using Color Palettes",
    "text": "Using Color Palettes\nPre-designed color palettes might be a starting point to inspire the final aesthetic you choose for your graph.\n\nColorBrewer Palettes\nColorBrewer provides carefully designed color schemes:\n\n\nCode\nggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar() +\n  scale_fill_brewer(palette = \"Set2\") +\n  labs(title = \"Penguin Species by Island\",\n       x = \"Species\",\n       y = \"Count\",\n       fill = \"Island\")\n\n\n\n\n\n\n\n\n\n\n\nViridis Palettes\nViridis palettes are colorblind-friendly and print well in grayscale:\n\n\nCode\nggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar() +\n  scale_fill_viridis_d() +\n  labs(title = \"Penguin Species by Island\",\n       x = \"Species\",\n       y = \"Count\",\n       fill = \"Island\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nUse _c for continuous variables (numbers) and _d for discrete variables (categories).",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#point-shapes",
    "href": "Data Visualisation/customising graphs.html#point-shapes",
    "title": "Graph Customisation",
    "section": "Point Shapes",
    "text": "Point Shapes\nChange point shapes with the shape parameter:\n\n\nCode\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, \n                     shape = species)) +\n  geom_point(size = 3) +\n  labs(title = \"Bill Dimensions by Species\",\n       shape = \"Species\")\n\n\n\n\n\n\n\n\n\nShapes 21-25 allow both fill and border colors:\n\n\nCode\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm, \n                     fill = species)) +\n  geom_point(shape = 21, size = 3, color = \"black\") +\n  labs(title = \"Bill Dimensions by Species\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#line-types",
    "href": "Data Visualisation/customising graphs.html#line-types",
    "title": "Graph Customisation",
    "section": "Line Types",
    "text": "Line Types\nYou can modiify line appearance with linetype:\n\n\nCode\n```{r}\nggplot(economics, aes(x = date)) +\n  geom_line(aes(y = unemploy), linetype = \"solid\", color = \"blue\") +\n  geom_line(aes(y = unemploy * 1.1), linetype = \"dashed\", color = \"red\") +\n  labs(title = \"Unemployment Trends\",\n       y = \"Number Unemployed\")\n```",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#adding-labels",
    "href": "Data Visualisation/customising graphs.html#adding-labels",
    "title": "Graph Customisation",
    "section": "Adding Labels",
    "text": "Adding Labels\nUse the labs() function to add or modify all text in your plot:\n\n\nCode\n```{r}\n#| warning: false\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, \n                     color = species)) +\n  geom_point(size = 2, alpha = 0.7) +\n  labs(\n    title = \"Penguin Body Mass vs Flipper Length\",\n    subtitle = \"Measurements from Palmer Station, Antarctica\",\n    caption = \"Data source: palmerpenguins package\",\n    x = \"Flipper Length (mm)\",\n    y = \"Body Mass (g)\",\n    color = \"Penguin Species\"\n  ) +\n  theme_minimal()\n```",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#legend-position",
    "href": "Data Visualisation/customising graphs.html#legend-position",
    "title": "Graph Customisation",
    "section": "Legend Position",
    "text": "Legend Position\nControl legend placement with theme():\n\n\nCode\n```{r}\n#| warning: false\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, \n                     color = species)) +\n  geom_point(size = 2) +\n  labs(title = \"Penguin Measurements\",\n       color = \"Species\") +\n  theme_minimal() +\n  theme(legend.position = \"top\")\n```\n\n\n\n\n\n\n\n\n\nOptions for legend.position: - \"top\", \"bottom\", \"left\", \"right\" - c(x, y) for exact placement (e.g., c(0.9, 0.2) for bottom-right inside plot) - \"none\" to remove the legend",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#text-annotations",
    "href": "Data Visualisation/customising graphs.html#text-annotations",
    "title": "Graph Customisation",
    "section": "Text Annotations",
    "text": "Text Annotations\nUse annotate() to add text anywhere on your plot:\n\n\nCode\npenguin_summary &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  summarise(mean_mass = mean(body_mass_g, na.rm = TRUE))\n\nggplot(penguins, aes(x = species, y = body_mass_g)) +\n  geom_boxplot(fill = \"lightblue\") +\n  annotate(\"text\", x = 2, y = 6000, \n           label = \"Gentoo penguins are\\nthe heaviest species\",\n           color = \"darkred\", size = 4) +\n  labs(title = \"Body Mass Distribution by Species\",\n       x = \"Species\",\n       y = \"Body Mass (g)\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#adding-reference-lines",
    "href": "Data Visualisation/customising graphs.html#adding-reference-lines",
    "title": "Graph Customisation",
    "section": "Adding Reference Lines",
    "text": "Adding Reference Lines\nUse geom_hline() and geom_vline() for horizontal and vertical lines:\n\n\nCode\nmean_mass &lt;- mean(penguins$body_mass_g, na.rm = TRUE)\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g, \n                     color = species)) +\n  geom_point(alpha = 0.6) +\n  geom_hline(yintercept = mean_mass, \n             linetype = \"dashed\", color = \"darkred\") +\n  annotate(\"text\", x = 175, y = mean_mass + 200,\n           label = \"Overall Mean\", color = \"darkred\") +\n  labs(title = \"Penguin Measurements with Mean Body Mass\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#highlighting-specific-groups",
    "href": "Data Visualisation/customising graphs.html#highlighting-specific-groups",
    "title": "Graph Customisation",
    "section": "Highlighting Specific Groups",
    "text": "Highlighting Specific Groups\nThe gghighlight package makes it easy to emphasize particular data:\n\n\nCode\nlibrary(gghighlight)\n\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point(size = 2, color = \"coral\") +\n  gghighlight(species == \"Gentoo\") +\n  labs(title = \"Highlighting Gentoo Penguins\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#built-in-themes",
    "href": "Data Visualisation/customising graphs.html#built-in-themes",
    "title": "Graph Customisation",
    "section": "Built-in Themes",
    "text": "Built-in Themes\nggplot2 includes several pre-made themes:\n\n\nCode\np &lt;- ggplot(penguins, aes(x = species, fill = island)) +\n  geom_bar() +\n  labs(title = \"Penguins by Species and Island\")\n\n# Try different themes\np + theme_minimal()  # Clean and simple\n\n\n\n\n\n\n\n\n\n\n\nCode\n```{r}\n#| warning: false\np + theme_classic()  # Traditional look\n```\n\n\n\n\n\n\n\n\n\n\n\nCode\n```{r}\n#| warning: false\np + theme_dark()  # Dark background\n```",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/customising graphs.html#custom-themes-with-ggthemes",
    "href": "Data Visualisation/customising graphs.html#custom-themes-with-ggthemes",
    "title": "Graph Customisation",
    "section": "Custom Themes with ggthemes",
    "text": "Custom Themes with ggthemes\nThe ggthemes package offers many professional themes:\n\n\nCode\nlibrary(ggthemes)\n\nbase_plot &lt;- ggplot(penguins, aes(x = flipper_length_mm, \n                                   y = body_mass_g, \n                                   color = species)) +\n  geom_point(size = 2) +\n  labs(title = \"Penguin Physical Measurements\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\")\n\n# Economist style\nbase_plot + theme_economist() + scale_color_economist()\n\n\n\n\n\n\n\n\n\n\n\nCode\n# FiveThirtyEight style\nbase_plot + theme_fivethirtyeight()",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Graph Customisation"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#starting-simple",
    "href": "Data Visualisation/multivar.html#starting-simple",
    "title": "Visualizing Multiple Variables",
    "section": "Starting Simple",
    "text": "Starting Simple\nLet’s begin with a basic relationship: flipper length vs. body mass.\n\n\nCode\nggplot(penguins, aes(x = flipper_length_mm, y = body_mass_g)) +\n  geom_point() +\n  labs(title = \"Penguin Body Measurements\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#adding-color-to-show-groups",
    "href": "Data Visualisation/multivar.html#adding-color-to-show-groups",
    "title": "Visualizing Multiple Variables",
    "section": "Adding Color to Show Groups",
    "text": "Adding Color to Show Groups\nNow let’s add species as a third variable using color:\n\n\nCode\nggplot(penguins, aes(x = flipper_length_mm, \n                     y = body_mass_g,\n                     color = species)) +\n  geom_point() +\n  labs(title = \"Penguin Measurements by Species\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\",\n       color = \"Species\")\n\n\n\n\n\n\n\n\n\nEach species now has its own color, making patterns much clearer!",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#adding-shape-for-another-variable",
    "href": "Data Visualisation/multivar.html#adding-shape-for-another-variable",
    "title": "Visualizing Multiple Variables",
    "section": "Adding Shape for Another Variable",
    "text": "Adding Shape for Another Variable\nWe can add a fourth variable (sex) using different point shapes:\n\n\nCode\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     color = species,\n                     shape = sex)) +\n  geom_point(size = 3, alpha = 0.7) +\n  labs(title = \"Penguin Measurements by Species and Sex\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\",\n       color = \"Species\",\n       shape = \"Sex\")\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\n\nMappings to variables go inside aes() (like color = species)\nFixed values go outside aes() (like size = 3)",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#creating-bubble-plots-with-size",
    "href": "Data Visualisation/multivar.html#creating-bubble-plots-with-size",
    "title": "Visualizing Multiple Variables",
    "section": "Creating Bubble Plots with Size",
    "text": "Creating Bubble Plots with Size\nYou can map a continuous variable to point size, creating a “bubble plot”:\n\n\nCode\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     color = species,\n                     size = bill_length_mm)) +\n  geom_point(alpha = 0.6) +\n  labs(title = \"Four Variables in One Plot\",\n       subtitle = \"Size represents bill length\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\",\n       color = \"Species\",\n       size = \"Bill Length (mm)\")\n\n\n\n\n\n\n\n\n\nThis plot shows FOUR variables at once! But be careful, too many variables can make plots hard to read.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#adding-trend-lines-by-group",
    "href": "Data Visualisation/multivar.html#adding-trend-lines-by-group",
    "title": "Visualizing Multiple Variables",
    "section": "Adding Trend Lines by Group",
    "text": "Adding Trend Lines by Group\nLet’s explore the relationship between bill length and depth, with separate trend lines for each species:\n\n\nCode\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = species)) +\n  geom_point(alpha = 0.5, size = 2) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1.2) +\n  labs(title = \"Bill Dimensions with Species-Specific Trends\",\n       x = \"Bill Length (mm)\",\n       y = \"Bill Depth (mm)\",\n       color = \"Species\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\nThe geom_smooth() automatically creates separate lines for each color group.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#a-polished-example",
    "href": "Data Visualisation/multivar.html#a-polished-example",
    "title": "Visualizing Multiple Variables",
    "section": "A Polished Example",
    "text": "A Polished Example\nHere’s a well-formatted plot using multiple customization techniques:\n\n\nCode\nggplot(penguins, aes(x = flipper_length_mm,\n                     y = body_mass_g,\n                     color = sex)) +\n  geom_point(alpha = 0.4, size = 2.5) +\n  geom_smooth(method = \"lm\", \n              formula = y ~ poly(x, 2),\n              se = FALSE,\n              linewidth = 1.3) +\n  scale_color_brewer(palette = \"Set1\",\n                     labels = c(\"Female\", \"Male\", \"Unknown\")) +\n  labs(title = \"Body Mass vs. Flipper Length by Sex\",\n       subtitle = \"Palmer Station, Antarctica (2007-2009)\",\n       x = \"Flipper Length (mm)\",\n       y = \"Body Mass (g)\",\n       color = \"Sex\") +\n  theme_minimal()",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#basic-faceting-with-facet_wrap",
    "href": "Data Visualisation/multivar.html#basic-faceting-with-facet_wrap",
    "title": "Visualizing Multiple Variables",
    "section": "Basic Faceting with facet_wrap()",
    "text": "Basic Faceting with facet_wrap()\nLet’s compare body mass distributions across species:\n\n\nCode\nggplot(penguins, aes(x = body_mass_g)) +\n  geom_histogram(fill = \"steelblue\", bins = 20) +\n  facet_wrap(~species, ncol = 1) +\n  labs(title = \"Body Mass Distribution by Species\",\n       x = \"Body Mass (g)\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\nThe ~species notation means “create separate plots for each species.” The ncol = 1 arranges them in a single column.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#faceting-with-two-variables",
    "href": "Data Visualisation/multivar.html#faceting-with-two-variables",
    "title": "Visualizing Multiple Variables",
    "section": "Faceting with Two Variables",
    "text": "Faceting with Two Variables\nUse facet_grid() to create a grid based on two variables:\n\n\nCode\nggplot(penguins, aes(x = body_mass_g / 1000)) +\n  geom_histogram(fill = \"coral\", bins = 15) +\n  facet_grid(sex ~ species) +\n  labs(title = \"Body Mass by Species and Sex\",\n       x = \"Body Mass (kg)\",\n       y = \"Count\")\n\n\n\n\n\n\n\n\n\nThe formula sex ~ species means rows are sex and columns are species.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#combining-grouping-and-faceting",
    "href": "Data Visualisation/multivar.html#combining-grouping-and-faceting",
    "title": "Visualizing Multiple Variables",
    "section": "Combining Grouping and Faceting",
    "text": "Combining Grouping and Faceting\nYou can use both techniques together for even more insight:\n\n\nCode\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = sex)) +\n  geom_point(alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE) +\n  facet_wrap(~species, ncol = 1) +\n  labs(title = \"Bill Dimensions by Species and Sex\",\n       x = \"Bill Length (mm)\",\n       y = \"Bill Depth (mm)\",\n       color = \"Sex\")\n\n\n\n\n\n\n\n\n\nNow we can see how the relationship between bill dimensions differs across species AND sex.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#making-facets-more-readable",
    "href": "Data Visualisation/multivar.html#making-facets-more-readable",
    "title": "Visualizing Multiple Variables",
    "section": "Making Facets More Readable",
    "text": "Making Facets More Readable\nLet’s improve the previous plot with better formatting:\n\n\nCode\nggplot(penguins, aes(x = bill_length_mm,\n                     y = bill_depth_mm,\n                     color = sex)) +\n  geom_point(size = 2, alpha = 0.6) +\n  geom_smooth(method = \"lm\", se = FALSE, linewidth = 1.2) +\n  facet_wrap(~species, ncol = 3) +\n  scale_color_brewer(palette = \"Set1\",\n                     labels = c(\"Female\", \"Male\", \"Unknown\")) +\n  theme_minimal() +\n  labs(title = \"Bill Dimensions Vary by Species and Sex\",\n       subtitle = \"Measurements from Palmer Station, Antarctica\",\n       x = \"Bill Length (mm)\",\n       y = \"Bill Depth (mm)\",\n       color = \"Sex\") +\n  theme(legend.position = \"bottom\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#faceting-with-time-series",
    "href": "Data Visualisation/multivar.html#faceting-with-time-series",
    "title": "Visualizing Multiple Variables",
    "section": "Faceting with Time Series",
    "text": "Faceting with Time Series\nFaceting works great for comparing trends over time. Let’s use the economics dataset to compare different economic indicators:\n\n\nCode\n# Prepare data for multiple time series\necon_data &lt;- economics %&gt;%\n  select(date, unemploy, psavert, pop) %&gt;%\n  tidyr::pivot_longer(cols = -date,\n                      names_to = \"variable\",\n                      values_to = \"value\")\n\n# Create labels for facets\necon_labels &lt;- c(\n  \"unemploy\" = \"Unemployment (thousands)\",\n  \"psavert\" = \"Personal Savings Rate (%)\",\n  \"pop\" = \"Population (thousands)\"\n)\n\nggplot(econ_data, aes(x = date, y = value)) +\n  geom_line(color = \"darkblue\") +\n  facet_wrap(~variable, \n             ncol = 1,\n             scales = \"free_y\",\n             labeller = labeller(variable = econ_labels)) +\n  theme_minimal() +\n  labs(title = \"US Economic Indicators Over Time\",\n       x = \"Year\",\n       y = NULL) +\n  theme(strip.text = element_text(face = \"bold\", size = 10))\n\n\n\n\n\n\n\n\n\nThe scales = \"free_y\" option lets each facet have its own y-axis scale, which is crucial when variables have different ranges.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/multivar.html#many-facets-small-multiples",
    "href": "Data Visualisation/multivar.html#many-facets-small-multiples",
    "title": "Visualizing Multiple Variables",
    "section": "Many Facets: Small Multiples",
    "text": "Many Facets: Small Multiples\nWhen you have many groups, faceting creates a “small multiples” display. Here’s an example with different islands:\n\n\nCode\n# Compare trends across islands\nggplot(penguins, aes(x = bill_length_mm, y = bill_depth_mm)) +\n  geom_point(aes(color = species), size = 2, alpha = 0.7) +\n  facet_wrap(~island) +\n  scale_color_viridis_d() +\n  theme_minimal() +\n  labs(title = \"Bill Dimensions Across Different Islands\",\n       x = \"Bill Length (mm)\",\n       y = \"Bill Depth (mm)\",\n       color = \"Species\")",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Multiple Variables"
    ]
  },
  {
    "objectID": "Data Visualisation/timegraph.html#simple-time-series",
    "href": "Data Visualisation/timegraph.html#simple-time-series",
    "title": "Visualizing Time",
    "section": "Simple Time Series",
    "text": "Simple Time Series\n\n\n\n\n\n\nMath to Code Mindset\n\n\n\n\nInput: Time \\(t\\) and value \\(y\\)\nTransformation: Sort by \\(t\\)\nOutput: Line segments connecting consecutive points\n\n\n\n\n\nCode\nlibrary(ggplot2)\nlibrary(scales)\n\nggplot(economics, aes(x = date, y = psavert)) +\ngeom_line(color = \"indianred3\", linewidth = 1) +\ngeom_smooth(method = \"loess\", se = FALSE) +\nscale_x_date(date_breaks = \"5 years\", labels = date_format(\"%b-%y\")) +\nlabs(\ntitle = \"Personal Savings Rate (1967–2015)\",\ny = \"Savings Rate\",\nx = \"\"\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nImportant\n\n\n\nDates must be stored as Date or POSIXct objects. Text values are sorted alphabetically, not chronologically.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Time"
    ]
  },
  {
    "objectID": "Data Visualisation/timegraph.html#multivariate-time-series",
    "href": "Data Visualisation/timegraph.html#multivariate-time-series",
    "title": "Visualizing Time",
    "section": "Multivariate Time Series",
    "text": "Multivariate Time Series\n\n\nCode\nlibrary(quantmod)\nlibrary(dplyr)\n\ngetSymbols(c(\"AAPL\", \"META\"), from = \"2023-01-01\", to = \"2023-07-31\")\n\n\n[1] \"AAPL\" \"META\"\n\n\nCode\nmseries &lt;- bind_rows(\ndata.frame(Date = index(AAPL), Close = as.numeric(AAPL$AAPL.Close), Company = \"Apple\"),\ndata.frame(Date = index(META), Close = as.numeric(META$META.Close), Company = \"Meta\")\n)\n\nggplot(mseries, aes(x = Date, y = Close, color = Company)) +\ngeom_line(linewidth = 1) +\nscale_color_brewer(palette = \"Dark2\") +\nlabs(\ntitle = \"Tech Stock Performance\",\nsubtitle = \"Apple vs Meta (Jan–July 2023)\",\ny = \"Closing Price ($)\",\nx = \"\"\n) +\ntheme_minimal()",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Time"
    ]
  },
  {
    "objectID": "Data Visualisation/timegraph.html#stacked-area-charts",
    "href": "Data Visualisation/timegraph.html#stacked-area-charts",
    "title": "Visualizing Time",
    "section": "Stacked Area Charts",
    "text": "Stacked Area Charts\n\n\nCode\ndata(uspopage, package = \"gcookbook\")\n\nggplot(\nuspopage,\naes(x = Year, y = Thousands / 1000, fill = forcats::fct_rev(AgeGroup))\n) +\ngeom_area(color = \"black\", linewidth = 0.2) +\nscale_fill_brewer(palette = \"Set2\") +\nlabs(\ntitle = \"US Population Composition\",\ny = \"Population (Millions)\",\nfill = \"Age Group\",\nx = \"\"\n) +\ntheme_minimal()",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Time"
    ]
  },
  {
    "objectID": "Data Visualisation/univariate.html#categorical-variables",
    "href": "Data Visualisation/univariate.html#categorical-variables",
    "title": "Univariate Data",
    "section": "Categorical variables",
    "text": "Categorical variables\nBoth the diet of the mammals and their taxonomic order are categorical variables. The distribution of a single categorical variable is most commonly shown using a bar chart. Other options include pie charts, and—less frequently—tree maps or waffle charts.\n\nBar charts\nFigure 4.1 uses a simple bar chart to display the distribution of mammals by diet (vore).\n\n\nCode\n```{r}\n# simple bar chart\nlibrary(ggplot2)\n```\n\n\nWarning: package 'ggplot2' was built under R version 4.4.3\n\n\nCode\n```{r}\ndata(msleep, package = \"ggplot2\")\n\n# plot the distribution of diet (vore)\n# We filter out NAs for a cleaner plot\nlibrary(dplyr)\n```\n\n\nWarning: package 'dplyr' was built under R version 4.4.3\n\n\n\nAttaching package: 'dplyr'\n\n\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n\n\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n\n\nCode\n```{r}\nmsleep_clean &lt;- msleep %&gt;% filter(!is.na(vore))\n\nggplot(msleep_clean, aes(x = vore)) +\n  geom_bar()\n```\n\n\n\n\n\n\n\n\n\nFigure 4.1: Simple barchart\nFrom this plot we can immediately see that most mammals in this dataset are herbivores, followed closely by omnivores and carnivores, with insectivores being the smallest group.\nYou can customise bar charts by changing colours, labels, and titles. In ggplot2, the fill argument controls the colour of areas such as bars, while color controls outlines and borders.\n\n\nCode\n```{r}\n# plot the distribution of diet with modified colors and labels\nggplot(msleep_clean, aes(x=vore)) +\n  geom_bar(fill = \"cornflowerblue\",\n           color=\"black\") +\n  labs(x = \"Diet\",\n       y = \"Frequency\",\n       title = \"Mammals by diet\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.2: Barchart with modified colours, labels, and title\n\n\n4.1.1.1 Percentages\nInstead of showing raw counts, bars can also represent percentages. In a bar chart, aes(x = vore) is shorthand for aes(x = vore, y = after_stat(count)), where count is the frequency in each category. By working with this quantity explicitly, we can convert counts into percentages.\n\n\nCode\n```{r}\n# plot the distribution as percentages\nggplot(msleep_clean,\n       aes(x = vore, y = after_stat(count/sum(count)))) +\n  geom_bar() +\n  labs(x = \"Diet\",\n       y = \"Percent\",\n       title  = \"Mammals by diet\") +\n  scale_y_continuous(labels = scales::percent)\n```\n\n\n\n\n\n\n\n\n\nFigure 4.3: Barchart with percentages\nHere, the scales package is used to format the y-axis labels as percentages.\n\n\n\n4.1.1.2 Sorting categories\nIt is often helpful to order categories by their frequency. To do this, we first calculate the counts explicitly and then reorder the categories using reorder(). Setting stat = \"identity\" tells ggplot2 not to compute counts internally.\n\n\nCode\n```{r}\n# calculate number of mammals in each diet category\nplotdata &lt;- msleep_clean %&gt;%\n  count(vore)\n```\n\n\nThe resulting dataset is shown below.\nTable 4.1: Frequency of diet categories\n\n\n\nvore\nn\n\n\n\n\ncarni\n19\n\n\nherbi\n32\n\n\ninsecti\n5\n\n\nomni\n20\n\n\n\nWe can now use this dataset to create a sorted bar chart.\n\n\nCode\n```{r}\n# plot the bars in ascending order\nggplot(plotdata,\n       aes(x = reorder(vore, n), y = n)) +\n  geom_bar(stat=\"identity\") +\n  labs(x = \"Diet\",\n       y = \"Frequency\",\n       title  = \"Mammals by diet\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.4: Sorted bar chart\nThe bars are ordered from smallest to largest frequency. Using reorder(vore, -n) would reverse this order.\n\n\n\n4.1.1.3 Labelling bars\nSometimes it is useful to display the numerical values directly on the bars.\n\n\nCode\n```{r}\n# plot the bars with numeric labels\nggplot(plotdata,\n       aes(x = vore, y = n)) +\n  geom_bar(stat=\"identity\") +\n  geom_text(aes(label = n), vjust=-0.5) +\n  labs(x = \"Diet\",\n       y = \"Frequency\",\n       title  = \"Mammals by diet\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.5: Bar chart with numeric labels\nHere, geom_text() adds the labels, and vjust controls their vertical positioning.\nPutting these ideas together, we can create a bar chart showing percentages, ordered from most to least frequent, and labelled accordingly.\n\n\nCode\n```{r}\nlibrary(scales)\n```\n\n\nWarning: package 'scales' was built under R version 4.4.3\n\n\nCode\n```{r}\nplotdata &lt;- msleep_clean %&gt;%\n  count(vore) %&gt;%\n  mutate(pct = n / sum(n),\n         pctlabel = paste0(round(pct*100), \"%\"))\n\n# plot the bars as percentages,\n# in descending order with bar labels\nggplot(plotdata,\n       aes(x = reorder(vore, -pct), y = pct)) +\n  geom_bar(stat=\"identity\", fill=\"indianred3\", color=\"black\") +\n  geom_text(aes(label = pctlabel), vjust=-0.25) +\n  scale_y_continuous(labels = percent) +\n  labs(x = \"Diet\",\n       y = \"Percent\",\n       title  = \"Mammals by diet\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.6: Sorted bar chart with percent labels\n\n\n\n4.1.1.4 Overlapping labels\nWhen there are many categories or when labels are long, axis labels can overlap. This is clear when plotting the taxonomic order of the mammals.\n\n\nCode\n```{r}\n# basic bar chart with overlapping labels\nggplot(msleep, aes(x=order)) +\n  geom_bar() +\n  labs(x = \"Taxonomic Order\",\n       y = \"Frequency\",\n       title = \"Mammals by Order\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.7: Barchart with problematic labels\nOne solution is to flip the axes.\n\n\nCode\n```{r}\n# horizontal bar chart\nggplot(msleep, aes(x = order)) +\n  geom_bar() +\n  labs(x = \"\",\n       y = \"Frequency\",\n       title = \"Mammals by Order\") +\n  coord_flip()\n```\n\n\n\n\n\n\n\n\n\nFigure 4.8: Horizontal barchart\nAlternatively, labels can be rotated.\n\n\nCode\n```{r}\n# bar chart with rotated labels\nggplot(msleep, aes(x=order)) +\n  geom_bar() +\n  labs(x = \"\",\n       y = \"Frequency\",\n       title = \"Mammals by Order\") +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n```\n\n\n\n\n\n\n\n\n\nFigure 4.9: Barchart with rotated labels\nA third option is to stagger the labels by inserting line breaks.\n\n\nCode\n```{r}\n# bar chart with staggered labels\nlbls &lt;- paste0(c(\"\",\"\\n\"), unique(msleep$order))\n# Note: In real usage, ensure labels match factor levels order\nggplot(msleep,\n       aes(x=factor(order, labels = lbls))) +\n  geom_bar() +\n  labs(x = \"\",\n       y = \"Frequency\",\n       title = \"Mammals by Order\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.10: Barchart with staggered labels\nIn practice, it is usually best to avoid rotated labels where possible, as they are harder to read and increase cognitive load for the reader.\n\n\n\n\n4.1.2 Pie charts\nPie charts are often debated in statistics. For comparing category frequencies, bar charts are generally superior because humans judge lengths more accurately than areas. However, if the goal is to show how each category relates to the whole, and there are only a few categories, pie charts can still be useful.\nA pie chart in ggplot2 is essentially a bar chart plotted on a polar coordinate system.\n\n\nCode\n```{r}\n# Prepare data for pie chart\nplotdata &lt;- msleep_clean %&gt;%\n  count(vore) %&gt;%\n  mutate(prop = n / sum(n)) %&gt;%\n  mutate(label = scales::percent(prop))\n\n# Create pie chart\nggplot(plotdata, aes(x = \"\", y = prop, fill = vore)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(\"y\", start = 0) +\n  theme_void() +\n  labs(title = \"Mammals by diet\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.11: Basic pie chart with legend\nWe use coord_polar(\"y\") to transform the bar chart into a circle. theme_void() is used to remove the background, axis grid, and labels that are not necessary for a pie chart.\nTo place labels inside the slices, we can use geom_text() with position_stack().\n\n\nCode\n```{r}\n# Create pie chart with labels\nggplot(plotdata, aes(x = \"\", y = prop, fill = vore)) +\n  geom_bar(stat = \"identity\", width = 1, color = \"white\") +\n  coord_polar(\"y\", start = 0) +\n  geom_text(aes(label = label),\n            position = position_stack(vjust = 0.5),\n            color = \"white\",\n            size = 4) +\n  theme_void() +\n  labs(title = \"Mammals by diet\") +\n  theme(legend.position = \"right\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.12: Pie chart with percent labels\nThis plot makes it clear, for example, that roughly 42% of the mammals in this dataset are herbivores.\n\n\n\n4.1.3 Tree maps\nTree maps provide another alternative to pie charts and are particularly useful when there are many categories, such as the taxonomic orders in our dataset.\n\n\nCode\n```{r}\nlibrary(treemapify)\n```\n\n\nWarning: package 'treemapify' was built under R version 4.4.3\n\n\nCode\n```{r}\n# create a treemap of mammal orders\nplotdata &lt;- msleep %&gt;%\n  count(order)\n\nggplot(plotdata,\n       aes(fill = order, area = n)) +\n  geom_treemap() +\n  labs(title = \"Mammals by Order\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.13: Basic treemap\nAdding labels makes the plot more informative.\n\n\nCode\n```{r}\n# create a treemap with tile labels\nggplot(plotdata,\n       aes(fill = order, area = n, label = order)) +\n  geom_treemap() +\n  geom_treemap_text(colour = \"white\", place = \"centre\") +\n  labs(title = \"Mammals by Order\") +\n  theme(legend.position = \"none\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.14: Treemap with labels\n\n\n\n4.1.4 Waffle charts\nA waffle chart (also called a grid plot or square pie chart) represents proportions using a grid of squares. While dedicated packages exist, we can create a waffle chart in standard ggplot2 by treating the data as a grid of tiles.\nWe begin by sorting the data and assigning logical x and y coordinates to each mammal to create a 10-column grid.\n\n\nCode\n```{r}\n# Prepare data: Sort by diet to cluster colors\nwaffle_data &lt;- msleep_clean %&gt;%\n  arrange(vore) %&gt;%\n  mutate(\n    # Create x and y coordinates for a 10-column grid\n    x = (row_number() - 1) %% 10,\n    y = (row_number() - 1) %/% 10\n  )\n\n# Create the chart using geom_tile\nggplot(waffle_data, aes(x = x, y = y, fill = vore)) +\n  geom_tile(color = \"white\") +  # white borders create the \"grid\" look\n  coord_fixed() +               # ensures squares remain square\n  theme_void() +                # removes axes and backgrounds\n  labs(title = \"Mammals by Diet\",\n       caption = \"1 square = 1 mammal\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.15: Basic waffle chart using geom_tile\nThis chart displays every individual mammal as a single square, making it easy to visualise the sample size alongside the proportions.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Univariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/univariate.html#quantitative-variables",
    "href": "Data Visualisation/univariate.html#quantitative-variables",
    "title": "Univariate Data",
    "section": "4.2 Quantitative variables",
    "text": "4.2 Quantitative variables\nIn the msleep dataset, sleep_total (total sleep in hours) is a quantitative variable. Common ways to visualise the distribution of a single quantitative variable include histograms, kernel density plots, and dot plots.\n\n4.2.1 Histograms\nHistograms are the most widely used tool for visualising quantitative data. The range of values is divided into adjacent bins of equal width, and the number of observations in each bin is shown as a bar.\n\n\nCode\n```{r}\nlibrary(ggplot2)\n\n# plot the sleep distribution using a histogram\nggplot(msleep, aes(x = sleep_total)) +\n  geom_histogram() +\n  labs(title = \"Mammal sleep distribution\",\n       x = \"Total Sleep (hours)\")\n```\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nFigure 4.17: Basic histogram\nThe distribution appears somewhat bimodal, with many mammals sleeping around 10 hours, but a significant number sleeping much less or much more.\nColours can be customised using fill and color.\n\n\nCode\n```{r}\n# plot the histogram with blue bars and white borders\nggplot(msleep, aes(x = sleep_total)) +\n  geom_histogram(fill = \"cornflowerblue\",\n                 color = \"white\") +\n  labs(title=\"Mammal sleep distribution\",\n       x = \"Total Sleep (hours)\")\n```\n\n\n`stat_bin()` using `bins = 30`. Pick better value `binwidth`.\n\n\n\n\n\n\n\n\n\nFigure 4.18: Histogram with specified fill and border colours\n\n\n4.2.1.1 Bins and bandwidths\nThe choice of the number of bins (or equivalently the bin width) has a large impact on the appearance of a histogram. The default is 30 bins, but it is often useful to experiment.\n\n\nCode\n```{r}\n# plot the histogram with 15 bins\nggplot(msleep, aes(x = sleep_total)) +\n  geom_histogram(fill = \"cornflowerblue\",\n                 color = \"white\",\n                 bins = 15) +\n  labs(title=\"Mammal sleep distribution\",\n       subtitle = \"number of bins = 15\",\n       x = \"Total Sleep (hours)\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.19: Histogram with a specified number of bins\nAlternatively, you can specify the bin width directly.\n\n\nCode\n```{r}\n# plot the histogram with a binwidth of 2 hours\nggplot(msleep, aes(x = sleep_total)) +\n  geom_histogram(fill = \"cornflowerblue\",\n                 color = \"white\",\n                 binwidth = 2) +\n  labs(title=\"Mammal sleep distribution\",\n       subtitle = \"binwidth = 2 hours\",\n       x = \"Total Sleep (hours)\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.20: Histogram with a specified bin width\nAs before, the y-axis can show counts or percentages.\n\n\nCode\n```{r}\nlibrary(scales)\nggplot(msleep,\n       aes(x = sleep_total, y= after_stat(count/sum(count)))) +\n  geom_histogram(fill = \"cornflowerblue\",\n                 color = \"white\",\n                 binwidth = 2) +\n  labs(title=\"Mammal sleep distribution\",\n       y = \"Percent\",\n       x = \"Total Sleep (hours)\") +\n  scale_y_continuous(labels = percent)\n```\n\n\n\n\n\n\n\n\n\nFigure 4.21: Histogram with percentages on the y-axis\n\n\n\n\n4.2.2 Kernel density plots\nA kernel density plot provides a smooth alternative to a histogram. The idea is to estimate the underlying probability density function of a continuous variable, producing a smooth curve whose total area equals one.\n\n\nCode\n```{r}\n# Create a kernel density plot of sleep\nggplot(msleep, aes(x = sleep_total)) +\n  geom_density() +\n  labs(title = \"Mammal sleep distribution\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.22: Basic kernel density plot\nThe area under the curve between two time points represents the proportion of mammals in that sleep range.\n\n\nCode\n```{r}\n# Create a kernel density plot of sleep\nggplot(msleep, aes(x = sleep_total)) +\n  geom_density(fill = \"indianred3\") +\n  labs(title = \"Mammal sleep distribution\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.23: Kernel density plot with fill\n\n\n4.2.2.1 Smoothing parameter\nThe amount of smoothing is controlled by the bandwidth parameter bw. Larger values produce smoother curves, while smaller values reveal more detail.\n\n\nCode\n```{r}\n# default bandwidth for the sleep variable\nbw.nrd0(msleep$sleep_total)\n```\n\n\n[1] 1.637473\n\n\n\n\nCode\n```{r}\n# Create a kernel density plot of sleep\nggplot(msleep, aes(x = sleep_total)) +\n  geom_density(fill = \"deepskyblue\",\n               bw = 1) +\n  labs(title = \"Mammal sleep distribution\",\n       subtitle = \"bandwidth = 1\")\n```\n\n\n\n\n\n\n\n\n\nFigure 4.24: Kernel density plot with a specified bandwidth\nKernel density plots are useful for identifying where observations are concentrated, although the interpretation of the y-axis can be less intuitive for non-statisticians.\n\n\n\n\n4.2.3 Dot plots\nA dot plot is another alternative to the histogram. The data are grouped into bins, but each individual observation is shown as a dot. This approach works best for relatively small datasets like msleep.\n\n\nCode\n```{r}\n# plot the sleep distribution using a dotplot\nggplot(msleep, aes(x = sleep_total)) +\n  geom_dotplot() +\n  labs(title = \"Mammal sleep distribution\",\n       y = \"Proportion\",\n       x = \"Total Sleep (hours)\")\n```\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nFigure 4.25: Basic dotplot\nColours can again be customised.\n\n\nCode\n```{r}\n# Plot sleep as a dot plot using\n# gold dots with black borders\nggplot(msleep, aes(x = sleep_total)) +\n  geom_dotplot(fill = \"gold\",\n               color=\"black\") +\n  labs(title = \"Mammal sleep distribution\",\n       y = \"Proportion\",\n       x = \"Total Sleep (hours)\")\n```\n\n\nBin width defaults to 1/30 of the range of the data. Pick better value with\n`binwidth`.\n\n\n\n\n\n\n\n\n\nFigure 4.26: Dotplot with a specified colour scheme\nMany additional options are available; see ?geom_dotplot for further details.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Univariate Data"
    ]
  },
  {
    "objectID": "Data Visualisation/timegraph.html#from-stacked-areas-to-streams",
    "href": "Data Visualisation/timegraph.html#from-stacked-areas-to-streams",
    "title": "Visualizing Time",
    "section": "From Stacked Areas to Streams",
    "text": "From Stacked Areas to Streams\n\n\n\n\n\n\nMath to Code Mindset\n\n\n\n\nInput: Time \\(t\\), group \\(g\\), value \\(y_{tg}\\)\nNormalize: \\(p_{tg} = y_{tg} / \\sum_g y_{tg}\\)\nStack: cumulative sums within each \\(t\\)\nRecenter: shift stack midpoint to zero\nOutput: Filled ribbons around a central baseline",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Time"
    ]
  },
  {
    "objectID": "Data Visualisation/timegraph.html#data-transformation-explicit",
    "href": "Data Visualisation/timegraph.html#data-transformation-explicit",
    "title": "Visualizing Time",
    "section": "5.2 Data Transformation (Explicit)",
    "text": "5.2 Data Transformation (Explicit)\n\n\nCode\n```{r}\nlibrary(dplyr)\n\nstream_data &lt;- uspopage %&gt;%\ngroup_by(Year) %&gt;%\nmutate(\nprop = Thousands / sum(Thousands),\nymax = cumsum(prop),\nymin = ymax - prop\n) %&gt;%\nungroup() %&gt;%\ngroup_by(Year) %&gt;%\nmutate(\ncenter = (max(ymax) + min(ymin)) / 2,\nymin = ymin - center,\nymax = ymax - center\n)\n```\n\n\n\nWhat this does mathematically\nFor each year:\n\nConvert population counts to proportions\nStack age groups using cumulative sums\nCompute the vertical midpoint of the stack\nShift the entire stack so its midpoint is zero\n\nThis removes the fixed baseline that causes distortion in stacked area charts.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Time"
    ]
  },
  {
    "objectID": "Data Visualisation/timegraph.html#stream-graph-rendering-ggplot-only",
    "href": "Data Visualisation/timegraph.html#stream-graph-rendering-ggplot-only",
    "title": "Visualizing Time",
    "section": "5.3 Stream Graph Rendering (ggplot only)",
    "text": "5.3 Stream Graph Rendering (ggplot only)\n\n\nCode\n```{r}\nggplot(stream_data) +\ngeom_ribbon(\naes(\nx = Year,\nymin = ymin,\nymax = ymax,\nfill = forcats::fct_rev(AgeGroup)\n),\ncolor = \"black\",\nlinewidth = 0.15\n) +\nscale_fill_brewer(palette = \"Set2\") +\nlabs(\ntitle = \"US Population Demographics\",\nsubtitle = \"Stream graph constructed from centered stacked areas\",\ny = \"Centered Proportion\",\nx = \"Year\",\nfill = \"Age Group\"\n) +\ntheme_minimal()\n```\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA stream graph is not a new chart type — it is a stacked area chart with a moving, centered baseline. Packages like ggstream automate this transformation, but the underlying logic is purely geometric.\n\n\nThe widening of upper bands toward the right visualizes the aging of the US population.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Time"
    ]
  },
  {
    "objectID": "Data Visualisation/timegraph.html#conceptual-summary",
    "href": "Data Visualisation/timegraph.html#conceptual-summary",
    "title": "Visualizing Time",
    "section": "Conceptual Summary",
    "text": "Conceptual Summary\n\n\n\nChart type\nBaseline\nEmphasis\n\n\n\n\nArea chart\nFixed at 0\nAbsolute magnitude\n\n\nStacked area\nFixed at 0\nPart-to-whole\n\n\nStream graph\nCentered\nFlow & relative change",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Time"
    ]
  },
  {
    "objectID": "Data Visualisation/timegraph.html#data-transformation",
    "href": "Data Visualisation/timegraph.html#data-transformation",
    "title": "Visualizing Time",
    "section": "Data Transformation",
    "text": "Data Transformation\n\n\nCode\nlibrary(dplyr)\n\nstream_data &lt;- uspopage %&gt;%\ngroup_by(Year) %&gt;%\nmutate(\nprop = Thousands / sum(Thousands),\nymax = cumsum(prop),\nymin = ymax - prop\n) %&gt;%\nungroup() %&gt;%\ngroup_by(Year) %&gt;%\nmutate(\ncenter = (max(ymax) + min(ymin)) / 2,\nymin = ymin - center,\nymax = ymax - center\n)\n\n\n\nWhat this does mathematically\nFor each year:\n\nConvert population counts to proportions\nStack age groups using cumulative sums\nCompute the vertical midpoint of the stack\nShift the entire stack so its midpoint is zero\n\nThis removes the fixed baseline that causes distortion in stacked area charts.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Time"
    ]
  },
  {
    "objectID": "Data Visualisation/timegraph.html#stream-graph-rendering",
    "href": "Data Visualisation/timegraph.html#stream-graph-rendering",
    "title": "Visualizing Time",
    "section": "Stream Graph Rendering",
    "text": "Stream Graph Rendering\n\n\nCode\nggplot(stream_data) +\ngeom_ribbon(\naes(\nx = Year,\nymin = ymin,\nymax = ymax,\nfill = forcats::fct_rev(AgeGroup)\n),\ncolor = \"black\",\nlinewidth = 0.15\n) +\nscale_fill_brewer(palette = \"Set2\") +\nlabs(\ntitle = \"US Population Demographics\",\nsubtitle = \"Stream graph constructed from centered stacked areas\",\ny = \"Centered Proportion\",\nx = \"Year\",\nfill = \"Age Group\"\n) +\ntheme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nNote\n\n\n\nA stream graph is not a new chart type — it is a stacked area chart with a moving, centered baseline.\n\n\nThe widening of upper bands toward the right visualizes the aging of the US population.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Time"
    ]
  },
  {
    "objectID": "Data Visualisation/bivariate.html#bar-charts",
    "href": "Data Visualisation/bivariate.html#bar-charts",
    "title": "Visualizing Bivariate Data",
    "section": "Bar Charts",
    "text": "Bar Charts\nWe can plot the Mean of a variable for each group. Here, we calculate the average body mass for each penguin species.\n\n\nCode\n# Calculate means\nmean_data &lt;- penguins %&gt;%\n  group_by(species) %&gt;%\n  summarize(mean_mass = mean(body_mass_g))\n\n# Plot means\nggplot(mean_data, aes(x = species, y = mean_mass)) + \n  geom_col(fill = \"steelblue\", width = 0.7) + \n  geom_text(aes(label = round(mean_mass, 0)), vjust = -0.5) +\n  scale_y_continuous(limits = c(0, 6000)) +\n  labs(title = \"Average Body Mass by Species\", y = \"Mass (g)\") +\n  theme_minimal()\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nCaution\n\n\n\nBar charts of means can be misleading because they hide the spread of the data. A species with highly variable weights looks the same as a species with consistent weights.",
    "crumbs": [
      "Home",
      "Data Visualisation",
      "Visualizing Bivariate Data"
    ]
  },
  {
    "objectID": "Report Writing/reportreporting.html",
    "href": "Report Writing/reportreporting.html",
    "title": "How to Report Statistical Results",
    "section": "",
    "text": "The presentation of a statistical report is not just cosmetic. Clear, well-organised presentation is important for communicating results accurately and efficiently. A well-presented report allows the reader to focus on the statistical content rather than expending effort deciphering layout, notation, or formatting.\nReports should therefore be neat, clean, legible, and logically organised. RMarkdown makes it straightforward to produce professional-quality documents.",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  },
  {
    "objectID": "Report Writing/reportreporting.html#presentation",
    "href": "Report Writing/reportreporting.html#presentation",
    "title": "How to Report Statistical Results",
    "section": "",
    "text": "The presentation of a statistical report is not just cosmetic. Clear, well-organised presentation is important for communicating results accurately and efficiently. A well-presented report allows the reader to focus on the statistical content rather than expending effort deciphering layout, notation, or formatting.\nReports should therefore be neat, clean, legible, and logically organised. RMarkdown makes it straightforward to produce professional-quality documents.",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  },
  {
    "objectID": "Report Writing/reportreporting.html#reporting-descriptive-statistics-clearly-and-appropriately",
    "href": "Report Writing/reportreporting.html#reporting-descriptive-statistics-clearly-and-appropriately",
    "title": "Mathematical intuition behind coding",
    "section": "5.1 Reporting Descriptive Statistics Clearly and Appropriately",
    "text": "5.1 Reporting Descriptive Statistics Clearly and Appropriately\nDescriptive statistics summarise key features of the data and are often the reader’s first point of contact with the analysis. They should be chosen and reported with care.\nCommonly reported descriptive statistics include: - measures of location (mean, median), - measures of spread (standard deviation, interquartile range), - sample size, - relevant quantiles or proportions.\nStatistics should always be reported in context. Numbers presented without explanation or interpretation convey little information.\nFor example, it is insufficient to write: &gt; “The mean was 42.7.”\nInstead, write: &gt; “The mean exam score was 42.7 marks (SD = 6.3, n = 120).”\nWhere distributions are skewed or contain outliers, robust summaries such as the median and interquartile range should be preferred to the mean and standard deviation."
  },
  {
    "objectID": "Report Writing/reportreporting.html#reporting-numerical-values-and-choosing-suitable-precision",
    "href": "Report Writing/reportreporting.html#reporting-numerical-values-and-choosing-suitable-precision",
    "title": "How to Report Statistical Results",
    "section": "Reporting Numerical Values and Choosing Suitable Precision",
    "text": "Reporting Numerical Values and Choosing Suitable Precision\nNumerical precision should reflect the accuracy of the data and the purpose of the analysis, not the computing power used to generate the results.\nAs a general rule: * do not report more decimal places than are scientifically meaningful, * ensure consistency across tables and figures, * avoid implying spurious accuracy.\nFor example: * reporting a mean as 12.347891 is almost never appropriate, * reporting 12.3 or 12.35 may be appropriate depending on context.\nAs another example, P-values should typically be reported to two or three significant figures (e.g. p = 0.032), except when extremely small (e.g. p &lt; 0.001).",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  },
  {
    "objectID": "Report Writing/reportreporting.html#figures-plots-and-diagrams",
    "href": "Report Writing/reportreporting.html#figures-plots-and-diagrams",
    "title": "How to Report Statistical Results",
    "section": "Figures, Plots, and Diagrams",
    "text": "Figures, Plots, and Diagrams\nFigures, plots, and diagrams should:\n\nbe numbered sequentially,\nhave clear titles and captions,\nbe referred to explicitly in the text,\nbe placed close to the point where they are discussed.\n\nAxes must be clearly labelled, units indicated, and scales chosen sensibly. Furhtermore, a statistical report should be easy to navigate. Section headings, numbering, and figure references act as signposts that guide the reader through the argument.\nFor more details on data visualisation, refer to the pages dedicated to it.",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  },
  {
    "objectID": "Report Writing/reportreporting.html#tables",
    "href": "Report Writing/reportreporting.html#tables",
    "title": "How to Report Statistical Results",
    "section": "Tables",
    "text": "Tables\nTables are often the most effective way to present numerical summaries, provided they are well constructed.\nEvery table should:\n\nhave a table number,\nhave a clear title or caption,\ninclude labelled rows and columns,\nstate units where relevant,\nalign numerical values correctly (typically on the decimal point).\n\nTables should not duplicate information already presented clearly in figures. Each table should serve a distinct purpose.",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  },
  {
    "objectID": "Report Writing/reportreporting.html#constructing-tables-in-r-using-dplyr-kable-and-kableextra",
    "href": "Report Writing/reportreporting.html#constructing-tables-in-r-using-dplyr-kable-and-kableextra",
    "title": "How to Report Statistical Results",
    "section": "Constructing Tables in R Using dplyr, kable, and kableExtra",
    "text": "Constructing Tables in R Using dplyr, kable, and kableExtra\nIn modern statistical reporting, tables are generated directly from data using reproducible code. This ensures transparency, reduces transcription errors, and guarantees that tables remain consistent with the underlying analysis.\nIn R, this workflow typically combines:\n\ndplyr for data manipulation,\nknitr::kable() for table creation,\nkableExtra for professional formatting.\n\nWe illustrate this workflow using the penguins dataset from the palmerpenguins package.\n\n\nPreparing Data with dplyr\nThe penguins dataset contains measurements on three penguin species collected in Antarctica. We begin by loading the data and performing basic preparation.\n\n\nCode\nlibrary(dplyr)\nlibrary(palmerpenguins)\n\ndata(penguins)\n\n\nAs with most real datasets, penguins contains missing values. For descriptive tables, it is important to handle these explicitly rather than silently ignoring them.\nWe construct a Table 1–style summary, stratified by species.\n\n\nCode\nsummary_table &lt;- penguins %&gt;%\n  filter(!is.na(bill_length_mm),\n         !is.na(body_mass_g)) %&gt;%\n  group_by(species) %&gt;%\n  summarise(\n    n = n(),\n    mean_bill_length = mean(bill_length_mm),\n    sd_bill_length = sd(bill_length_mm),\n    mean_body_mass = mean(body_mass_g),\n    sd_body_mass = sd(body_mass_g)\n  )\n\n\nThis table contains only descriptive statistics and provides an overview of the dataset. No hypothesis tests or p-values are included.\n\n\n\nBasic Tables with kable\nThe kable() function converts a data frame into a clean, publication-ready table.\n\n\nCode\nlibrary(knitr)\n\nkable(\n  summary_table,\n  caption = \"Table 1: Descriptive statistics of penguin characteristics by species\",\n  digits = 2,\n  col.names = c(\n    \"Species\",\n    \"Sample size\",\n    \"Mean bill length (mm)\",\n    \"SD bill length (mm)\",\n    \"Mean body mass (g)\",\n    \"SD body mass (g)\"\n  )\n)\n\n\n\nTable 1: Descriptive statistics of penguin characteristics by species\n\n\n\n\n\n\n\n\n\n\nSpecies\nSample size\nMean bill length (mm)\nSD bill length (mm)\nMean body mass (g)\nSD body mass (g)\n\n\n\n\nAdelie\n151\n38.79\n2.66\n3700.66\n458.57\n\n\nChinstrap\n68\n48.83\n3.34\n3733.09\n384.34\n\n\nGentoo\n123\n47.50\n3.08\n5076.02\n504.12\n\n\n\n\n\nThis already satisfies many principles of good table design:\n\nclear caption,\ninterpretable column headings,\nconsistent numerical precision,\nunits included directly in column labels.\n\n\n\n\nEnhancing Tables with kableExtra\nFor more polished presentation, kableExtra provides tools for alignment, grouping, and emphasis.\n\n\nCode\nlibrary(kableExtra)\n\nkable(\n  summary_table,\n  caption = \"Table 1: Descriptive statistics of penguin characteristics by species\",\n  digits = 2,\n  col.names = c(\n    \"Species\",\n    \"Sample size\",\n    \"Mean bill length (mm)\",\n    \"SD bill length (mm)\",\n    \"Mean body mass (g)\",\n    \"SD body mass (g)\"\n  )\n) %&gt;%\n  kable_styling(\n    full_width = FALSE,\n    position = \"center\",\n    bootstrap_options = c(\"striped\", \"hover\")\n  ) %&gt;%\n  column_spec(1, bold = TRUE) %&gt;%\n  add_header_above(\n    c(\" \" = 2,\n      \"Bill length\" = 2,\n      \"Body mass\" = 2)\n  )\n\n\n\nTable 1: Descriptive statistics of penguin characteristics by species\n\n\n\n\n\n\n\n\n\n\n\n\nBill length\n\n\nBody mass\n\n\n\nSpecies\nSample size\nMean bill length (mm)\nSD bill length (mm)\nMean body mass (g)\nSD body mass (g)\n\n\n\n\nAdelie\n151\n38.79\n2.66\n3700.66\n458.57\n\n\nChinstrap\n68\n48.83\n3.34\n3733.09\n384.34\n\n\nGentoo\n123\n47.50\n3.08\n5076.02\n504.12\n\n\n\n\n\nThis formatting:\n\nvisually groups related variables,\nimproves readability,\nremains suitable for black-and-white printing.\n\n\n\n\nCommon Enhancements and When to Use Them\nSome frequently used kableExtra features include:\n\nRow emphasis Useful for highlighting reference groups or totals.\nrow_spec(0, bold = TRUE)\nFootnotes Essential when clarifying missing data, transformations, or units.\nfootnote(\n  general = \"Values are means with standard deviations in parentheses.\"\n)\nControlled column width Helps prevent overly wide tables in HTML or PDF output.\ncolumn_spec(2:6, width = \"3cm\")\n\nThese tools should be used sparingly. Tables should remain clear without relying on heavy styling.\n\n\n\nThe Role of Table 1 in Statistical Reports\nIn many applied fields, Table 1 serves a specific and recognised purpose:\n\nit summarises the dataset,\nit describes groups before modelling,\nit provides context for later analysis.\n\nA well-constructed Table 1 allows the reader to understand:\n\nsample sizes,\nvariable scales,\nbetween-group differences, before encountering any formal inference.\n\nIncluding hypothesis tests in Table 1 is generally discouraged. Inferential results belong in later sections where assumptions and methodology are discussed.\n\n\n\nReferencing Tables in the Text\nEvery table must be referenced explicitly. For example:\n\n“Table 1 summarises key morphological characteristics of the penguins by species.”\n\nA table that is not discussed does not contribute to the report and should be removed.",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  },
  {
    "objectID": "Report Writing/reportreporting.html#computer-output",
    "href": "Report Writing/reportreporting.html#computer-output",
    "title": "How to Report Statistical Results",
    "section": "Computer Output",
    "text": "Computer Output\nRaw computer output is rarely suitable for direct inclusion in a report, unless asked for. Output should be:\n\nedited,\nselectively included,\nformatted consistently with the rest of the document.\n\nFor student projects, it is usually unnecessary to replicate the appearance of software output exactly. Instead, relevant results should be extracted, summarised, and presented clearly in tables or figures.\nUnedited output dumps obscure key findings and signal a lack of engagement with the analysis.\nYou can choose to place all of your code at the end of your RMarkdown report in an Appendix using the two following chunks:\n\n\nCode\n```{r}\n##| Place this one at the start of your RMarkdown, after the YAML block\n\n# knitr::opts_chunk$set(echo = FALSE,\n#                      message=FALSE,\n#                      out.width = \"80%\",\n#                      fig.align = \"center\")\n#library(tidyverse)\n#library(ggplot2)\n#library(knitr)\n#library(tinytex)\n#library(MASS)\n#...\n# Other Libraries\n```\n\n\nNow for a code chunk that will print all of the code in your RMD file into one continuous code block:\n\n\nCode\n##| Place this one at the end of your RMarkdown and include the following in your chunk options for this R chunk:\n\n# ref.label=knitr::all_labels(),\n\n# I haven't included it since it would re-print all of the code above here.\n\n\nWhile these two chunks will not explicitly create and appendix, they will place all of the code present in the respective RMarkdown (RMD) file at the end of the knitted output. You can then insert markdown text above the last block to create an appendix.",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  },
  {
    "objectID": "Report Writing/reportreporting.html#references",
    "href": "Report Writing/reportreporting.html#references",
    "title": "How to Report Statistical Results",
    "section": "References",
    "text": "References\nChapman, D., & Mahon, J. (1986). Writing and Presenting Statistical Reports. Wiley.\nCleveland, W. S. (1993). Visualizing Data. Hobart Press.\nFew, S. (2012). Show Me the Numbers: Designing Tables and Graphs to Enlighten. Analytics Press.\nWickham, H., Çetinkaya-Rundel, M., & Grolemund, G. (2023). R for Data Science. O’Reilly.",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  },
  {
    "objectID": "Report Writing/reportstructure.html",
    "href": "Report Writing/reportstructure.html",
    "title": "Report Structure",
    "section": "",
    "text": "There are several acceptable ways to structure a statistical report. The structure outlined below is an example of one that does well in terms of clarity, readability, and reproducibility, and closely follows conventions used in professional statistical writing. Though, feel free to diverge from this structure if you feel it necessary.\nThe report is composed of a sequence of components, listed below in the order in which they should appear. Longer reports will usually benefit from being divided explicitly into sections, while very short reports may adopt a simplified version of this structure. Nevertheless, the logical separation of components should always be respected.\nA narrative approach should be avoided. Reports written as a chronological account of actions (for example, “First I did this, then I did that”) are difficult to follow and obscure the logical structure of the analysis. Statistical reports are not diaries of activity; they are structured arguments supported by data.",
    "crumbs": [
      "Home",
      "Report Writing",
      "Introduction to Report Writing"
    ]
  },
  {
    "objectID": "Report Writing/reportstructure.html#structure",
    "href": "Report Writing/reportstructure.html#structure",
    "title": "Report Structure",
    "section": "",
    "text": "There are several acceptable ways to structure a statistical report. The structure outlined below is an example of one that does well in terms of clarity, readability, and reproducibility, and closely follows conventions used in professional statistical writing. Though, feel free to diverge from this structure if you feel it necessary.\nThe report is composed of a sequence of components, listed below in the order in which they should appear. Longer reports will usually benefit from being divided explicitly into sections, while very short reports may adopt a simplified version of this structure. Nevertheless, the logical separation of components should always be respected.\nA narrative approach should be avoided. Reports written as a chronological account of actions (for example, “First I did this, then I did that”) are difficult to follow and obscure the logical structure of the analysis. Statistical reports are not diaries of activity; they are structured arguments supported by data.",
    "crumbs": [
      "Home",
      "Report Writing",
      "Introduction to Report Writing"
    ]
  },
  {
    "objectID": "Report Writing/reportstructure.html#title",
    "href": "Report Writing/reportstructure.html#title",
    "title": "Report Structure",
    "section": "Title",
    "text": "Title\nEvery statistical report requires a title. In most cases, this should appear on a separate title page, which should also include:\n\nthe author’s name,\ninstitutional affiliation (department or organisation),\nthe date of submission.\n\nHowever, for coursework your student ID number, Module code, and Assignment number should suffice.\nNow, back to professional reports. The title should be short, specific, and informative. Its purpose is to tell the reader what the report is about before they read a single paragraph. Vague titles such as “The statistical analysis of two data sets” fail to identify either the problem or the context.\nEffective titles typically indicate:\n\nthe subject or phenomenon under study,\nthe population or data source,\nsometimes (but not always) the general type of analysis.\n\nFor Example:\n\n“A Hybrid Approach to Financial Markets: Integrating SPDEs and Bayesian Neural Networks”\n“Lifetime earnings and the Vietnam era draft lottery: evidence from Social Security administrative records”\n“Using empirical Bayes techniques in the law school validity studies”",
    "crumbs": [
      "Home",
      "Report Writing",
      "Introduction to Report Writing"
    ]
  },
  {
    "objectID": "Report Writing/reportstructure.html#summary-abstract",
    "href": "Report Writing/reportstructure.html#summary-abstract",
    "title": "Report Structure",
    "section": "Summary / Abstract",
    "text": "Summary / Abstract\nA brief summary or abstract should follow the title. You will most likely never need to submit this in any of your coursework, however it is convention in professional works. This section should be concise, ideally no longer than half a page, and is often the most widely read part of the report (turns out researchers are also lazy!).\nThe abstract should provide a detailed overview of:\n\nthe problem being addressed,\nwhat was done,\nthe main conclusions reached.\n\nThe abstract should be written in plain language, avoiding symbols, equations, and numerical detail wherever possible. Technical terms should only be used if they are both unavoidable and widely understood by the intended audience. The purpose of the abstract is not to convince the reader of technical correctness, but to allow them to decide whether the report is relevant and worth reading.\nHere is an excellent example:\n“The analysis of censored failure times is considered. It is assumed that on each individual are available values of one or more explanatory variables. The hazard function (age-specific failure rate) is taken to be a function of the explanatory variables and unknown regression coefficients multiplied by an arbitrary and unknown function of time. A conditional likelihood is obtained, leading to inferences about the unknown regression coefficients. Some generalizations are outlined.” - Cox D. R. ‘Regression Models and Life-Tables.’ Journal of the Royal Statistical Society. Series B (Methodological), vol. 34, no. 2, 1972.",
    "crumbs": [
      "Home",
      "Report Writing",
      "Introduction to Report Writing"
    ]
  },
  {
    "objectID": "Report Writing/reportstructure.html#introduction",
    "href": "Report Writing/reportstructure.html#introduction",
    "title": "Report Structure",
    "section": "Introduction",
    "text": "Introduction\nThe introduction places the analysis in context. It should describe the background to the problem and explain why the investigation is of interest.\nTypically, the introduction includes:\n\na description of the general setting,\nan outline of the available data,\nhow and why the data were collected,\nthe aims and objectives of the investigation.\n\nThe introduction may be viewed as an expanded version of the abstract, but with greater detail and motivation. It should also give a brief indication of the statistical methods employed, without entering into technical detail. As a general guideline, the introduction should not exceed two pages in length. Importantly, it should not contain results or conclusions, which belong in later sections.",
    "crumbs": [
      "Home",
      "Report Writing",
      "Introduction to Report Writing"
    ]
  },
  {
    "objectID": "Report Writing/reportstructure.html#methods",
    "href": "Report Writing/reportstructure.html#methods",
    "title": "Report Structure",
    "section": "Methods",
    "text": "Methods\nThe methods section describes how the analysis was carried out, in sufficient detail for it to be replicated by a reader of comparable background (for example, a student at the same academic level). This section should include:\n\na clear description of the analytical strategy,\nany assumptions made,\nthe statistical models, tests, or procedures used,\nany theoretical background necessary to understand the analysis.\n\nDeciding how much detail to include is often difficult. A useful guiding principle is reproducibility: the reader should be able to repeat the analysis using the information provided. Very detailed material (like derivations, extra plots, extra tables, etc.) may be placed in one or more appendices.\n\nStructure of the Methods Section\nThe methods section often benefits from being divided into subsections, for example:\n\nA preliminary analysis (a more formal version of EDA), using graphical methods and simple descriptive statistics.\nA full-scale analysis, employing more rigorous statistical techniques.\n\nThe full-scale analysis may itself be divided into:\n\n(a) Methodology: a description or development of the statistical tools to be used,\n(b) Implementation: the application of these tools to the data.\n\nSubsection (a) is typically the most technical part of the report. Mathematical notation should be used where appropriate, but every symbol must be explicitly defined. An expression containing undefined symbols is meaningless, regardless of its mathematical sophistication. Similarly, pages of algebra or statistical derivation are of no value if the notation is unclear or inconsistent.\nIn subsection (b), the analysis should be described clearly and systematically, so that a reader could reproduce the results step by step.",
    "crumbs": [
      "Home",
      "Report Writing",
      "Introduction to Report Writing"
    ]
  },
  {
    "objectID": "Report Writing/reportstructure.html#results-conclusions-and-recommendations",
    "href": "Report Writing/reportstructure.html#results-conclusions-and-recommendations",
    "title": "Report Structure",
    "section": "Results, Conclusions, and Recommendations",
    "text": "Results, Conclusions, and Recommendations\nThis section presents the main findings of the analysis and the conclusions drawn from them. Primary results should be presented clearly and concisely. Subsidiary results may be included if they support the main conclusions and do not interrupt the flow of the presentation. Otherwise, they may be:\n\nmoved to an appendix,\nintegrated into the methods section,\nor presented in a separate results subsection.\n\nConclusions should be expressed in language accessible to a non-statistician and should make sense even if the reader has not read the detailed statistical analysis. Whenever possible, conclusions should be framed in the context of the original problem or experiment.\nRecommendations, if appropriate, should follow naturally from the conclusions and be proportionate to the strength of the evidence.",
    "crumbs": [
      "Home",
      "Report Writing",
      "Introduction to Report Writing"
    ]
  },
  {
    "objectID": "Report Writing/reportstructure.html#general-discussion",
    "href": "Report Writing/reportstructure.html#general-discussion",
    "title": "Report Structure",
    "section": "General Discussion",
    "text": "General Discussion\nThe general discussion places the results in a broader context.\nThis may include:\n\ncomparison with previous studies or related work,\ndiscussion of how successfully the original aims were achieved,\nlimitations of the data or methods,\nreflections on alternative approaches.\n\nIn applied work, it is particularly important to acknowledge potential problems with the data, such as measurement error or poor experimental design. In some reports, this discussion may be integrated into the introduction rather than appearing as a separate section. Nevertheless, both approaches allow for more honest scientific communication and allow others to validate your work.",
    "crumbs": [
      "Home",
      "Report Writing",
      "Introduction to Report Writing"
    ]
  },
  {
    "objectID": "Report Writing/reportstructure.html#references",
    "href": "Report Writing/reportstructure.html#references",
    "title": "Report Structure",
    "section": "References",
    "text": "References\nAny books, articles, or external sources referred to in the report must be cited. There are countably infinite (slight exaggeration) citation styles that exist and you must research what they are depending on who you are writing a report for. Despite this there are basic elements that all formats share.\nFor journal articles, references should include:\n\nauthor(s),\nyear of publication,\ntitle of the article,\njournal name,\nvolume number,\npage range.\n\nFor books, references should include:\n\nauthor(s),\nyear of publication,\ntitle (in italics),\npublisher,\nplace of publication.\n\nConsistency of style is essential, so if you choose one style, stick with it!",
    "crumbs": [
      "Home",
      "Report Writing",
      "Introduction to Report Writing"
    ]
  },
  {
    "objectID": "Report Writing/reportstructure.html#appendix",
    "href": "Report Writing/reportstructure.html#appendix",
    "title": "Report Structure",
    "section": "Appendix",
    "text": "Appendix\nMaterial that does not fit naturally into the main body of the report, but cannot reasonably be omitted, should be placed in an appendix. Typical appendix material includes:\n\nlengthy derivations,\nsupplementary tables,\nadditional figures,\nextended methodological details.\n\nHowever, excessive reliance on appendices should be avoided. If the appendices dominate the report, readability suffers. The main text should always remain the primary method for communication!!!",
    "crumbs": [
      "Home",
      "Report Writing",
      "Introduction to Report Writing"
    ]
  },
  {
    "objectID": "Report Writing/reportstructure.html#references-1",
    "href": "Report Writing/reportstructure.html#references-1",
    "title": "Introduction to Report Writing",
    "section": "References",
    "text": "References\nCox, D. R., & Donnelly, C. A. (2011). Principles of Applied Statistics. Cambridge University Press.\nDavison, A. C. (2003). Statistical Models. Cambridge University Press.\nAltman, D. G. (1991). Practical Statistics for Medical Research. Chapman & Hall.\nvon Elm, E., Altman, D. G., Egger, M., Pocock, S. J., Gøtzsche, P. C., & Vandenbroucke, J. P. (2007). The STROBE statement: Guidelines for reporting observational studies. PLoS Medicine, 4(10), e296.\nRoyal Statistical Society. Author guidelines for the Journal of the Royal Statistical Society series."
  },
  {
    "objectID": "Report Writing/reportenglish.html",
    "href": "Report Writing/reportenglish.html",
    "title": "How to Report Statistical Results",
    "section": "",
    "text": "Clear and precise use of English is essential in statistical writing. The primary purpose of writing, as with speaking, is communication. This becomes especially important when explaining abstract or technically demanding ideas, such as those encountered in statistical modelling and data analysis. Well-written English is easy to read and understand; poorly written English quickly obscures meaning and can render an otherwise sound analysis unintelligible.\nA common and easily corrected weakness in project reports is the failure to construct proper sentences. Every sentence should contain a clear main verb, and a useful first check when proofreading is to ask whether this verb can be identified. A more subtle and frequent problem occurs when the writer unconsciously assumes that the reader is following an unstated line of reasoning. Readers are often presumed to know which variables are under discussion, what quantities are being held fixed, and what is allowed to vary, even though none of this has been explicitly stated. Sentences may appear correct on first reading but reveal their ambiguity or emptiness once one attempts to interpret their precise meaning.\nImproving writing quality is an acquired skill that requires sustained effort. As with learning to drive, some people may find it easier than others, but everyone can improve through practice. While many guides to good writing exist, one of the most effective ways to develop a clear style is to read widely and attentively, observing how skilled writers structure arguments and convey complex ideas.\n\n\nSeveral practical rules can greatly improve clarity and readability:\n\nBe concise and precise. Prefer shorter, simpler words where they convey the same meaning. Avoid unnecessary digressions: material that does not fit naturally under a section heading should be moved elsewhere or omitted entirely.\nAvoid the personal pronoun “I”. Scientific and statistical writing is usually impersonal. Passive constructions or neutral phrasing are generally preferable, for example: “A regression analysis was carried out” rather than “I carried out a regression analysis”.\nUse tenses consistently. Reports should generally be written in the present tense when describing results and conclusions (e.g. “The analysis shows that…”). The past tense should be reserved for events that genuinely occurred in the past, such as data collection.\nEnsure correct spelling. If uncertain about a word, consult a reliable dictionary or use a spell-checker. Persistent spelling errors distract the reader and undermine confidence in the work.\nUse words accurately. Similar-sounding terms may have distinct meanings; for example, “trend” and “tendency” are not interchangeable. The word “significant” should be used only in its statistical sense.\nWrite complete, grammatical sentences. Telegraphic fragments such as “A standard analysis is possible. By regression.” are unacceptable. Mathematical and statistical writing follows the same grammatical rules as ordinary English.\nTreat “data” as a plural noun. The correct form is “data are…”, since “data” is the plural of “datum”.\n\nDeveloping a clear and readable style does not come naturally to many mathematicians and statisticians, but it is a worthwhile goal. Reading good prose, practising writing, and critically revising one’s own work are the most reliable ways to improve. Classic references such as Gowers (1973) and Fowler (1983) provide valuable guidance on grammar, punctuation, and style.",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  },
  {
    "objectID": "Report Writing/reportenglish.html#use-of-english-in-statistical-reports",
    "href": "Report Writing/reportenglish.html#use-of-english-in-statistical-reports",
    "title": "How to Report Statistical Results",
    "section": "",
    "text": "Clear and precise use of English is essential in statistical writing. The primary purpose of writing, as with speaking, is communication. This becomes especially important when explaining abstract or technically demanding ideas, such as those encountered in statistical modelling and data analysis. Well-written English is easy to read and understand; poorly written English quickly obscures meaning and can render an otherwise sound analysis unintelligible.\nA common and easily corrected weakness in project reports is the failure to construct proper sentences. Every sentence should contain a clear main verb, and a useful first check when proofreading is to ask whether this verb can be identified. A more subtle and frequent problem occurs when the writer unconsciously assumes that the reader is following an unstated line of reasoning. Readers are often presumed to know which variables are under discussion, what quantities are being held fixed, and what is allowed to vary, even though none of this has been explicitly stated. Sentences may appear correct on first reading but reveal their ambiguity or emptiness once one attempts to interpret their precise meaning.\nImproving writing quality is an acquired skill that requires sustained effort. As with learning to drive, some people may find it easier than others, but everyone can improve through practice. While many guides to good writing exist, one of the most effective ways to develop a clear style is to read widely and attentively, observing how skilled writers structure arguments and convey complex ideas.\n\n\nSeveral practical rules can greatly improve clarity and readability:\n\nBe concise and precise. Prefer shorter, simpler words where they convey the same meaning. Avoid unnecessary digressions: material that does not fit naturally under a section heading should be moved elsewhere or omitted entirely.\nAvoid the personal pronoun “I”. Scientific and statistical writing is usually impersonal. Passive constructions or neutral phrasing are generally preferable, for example: “A regression analysis was carried out” rather than “I carried out a regression analysis”.\nUse tenses consistently. Reports should generally be written in the present tense when describing results and conclusions (e.g. “The analysis shows that…”). The past tense should be reserved for events that genuinely occurred in the past, such as data collection.\nEnsure correct spelling. If uncertain about a word, consult a reliable dictionary or use a spell-checker. Persistent spelling errors distract the reader and undermine confidence in the work.\nUse words accurately. Similar-sounding terms may have distinct meanings; for example, “trend” and “tendency” are not interchangeable. The word “significant” should be used only in its statistical sense.\nWrite complete, grammatical sentences. Telegraphic fragments such as “A standard analysis is possible. By regression.” are unacceptable. Mathematical and statistical writing follows the same grammatical rules as ordinary English.\nTreat “data” as a plural noun. The correct form is “data are…”, since “data” is the plural of “datum”.\n\nDeveloping a clear and readable style does not come naturally to many mathematicians and statisticians, but it is a worthwhile goal. Reading good prose, practising writing, and critically revising one’s own work are the most reliable ways to improve. Classic references such as Gowers (1973) and Fowler (1983) provide valuable guidance on grammar, punctuation, and style.",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  },
  {
    "objectID": "Report Writing/reportai.html",
    "href": "Report Writing/reportai.html",
    "title": "How to Report Statistical Results",
    "section": "",
    "text": "Large language models (LLMs) and other generative AI tools are increasingly available to students and researchers, and their use in statistical work must be governed by clear principles of academic integrity, transparency, and responsibility. When used appropriately, such tools can support learning and productivity; when misused, they risk undermining understanding, originality, and the credibility of reported results.\n\n\nGenerative AI tools may be used as assistive aids, rather than as substitutes for intellectual work. Appropriate uses include:\n\nLanguage and Presentation Support AI tools may be used to improve clarity, grammar, and readability of text that the student has already written. For example, rephrasing sentences for conciseness, correcting grammatical errors, or suggesting clearer transitions between paragraphs is acceptable, provided the underlying ideas, structure, and arguments originate from the author.\nClarification of Concepts AI tools may assist in explaining statistical or mathematical concepts at a high level, helping the student understand definitions, assumptions, or standard interpretations. This is analogous to consulting a textbook, lecture notes, or an online explainer, and should be used to support — not replace — independent understanding.\nCode Debugging and Syntax Assistance Generative AI can be used to help identify syntax errors, suggest alternative implementations, or explain what a piece of code does. For example, asking why an R pipeline fails or how a plotting function works is acceptable. However, the student remains responsible for ensuring that the code is correct, appropriate, and fully understood.\nReproducibility and Workflow Support AI tools may assist in suggesting reproducible workflows, commenting code for clarity, or reminding the user of best practices (e.g. setting seeds, structuring scripts, or documenting functions). These uses support good statistical practice without contributing substantive analytical content.\nIdea Organisation and Planning Using AI to outline report sections, suggest headings, or check logical flow is acceptable, provided the final content is written and justified by the student.\n\nIn all such cases, the AI tool functions as a supporting instrument, similar to a spell-checker, reference book, or programming forum.\n\n\n\n\nCertain uses of generative AI are not appropriate in academic statistical work:\n\nGenerating Substantive Analysis or Results It is not acceptable to use AI tools to generate statistical analyses, interpret results, or draw conclusions that the student does not fully understand or cannot independently justify. This includes AI-generated explanations of model outputs, hypothesis test conclusions, or causal claims presented as the student’s own reasoning.\nWriting Large Sections of the Report Submitting AI-generated text as original work — including introductions, methods, results, or discussion sections — constitutes academic misconduct if not explicitly permitted and acknowledged. This applies even if the text is edited afterwards.\nProducing Code Without Understanding Using AI to generate entire scripts or analyses without understanding how they work is inappropriate. Statistical programming is part of the assessed intellectual work, and blindly using generated code risks errors, misinterpretation, and irreproducibility.\nFabrication of References, Data, or Methods AI tools may produce plausible-sounding but incorrect citations, datasets, or methodological claims. Including fabricated or unverified material in a report is a serious breach of academic standards.\nCircumventing Assessment Objectives Any use of AI that bypasses the intended learning outcomes — such as avoiding engagement with statistical reasoning, derivations, or interpretation — is inappropriate, even if not explicitly prohibited.\n\n\n\n\n\nStudents are responsible for everything submitted under their name, regardless of whether AI tools were used. Good practice includes:\n\nEnsuring all results can be explained and defended orally if required.\nVerifying all claims, references, and code outputs independently.\nAcknowledging AI assistance where required by departmental or university policy.\n\nUltimately, generative AI tools should be viewed as assistive technologies, not as authors, analysts, or decision-makers. Sound statistical reporting depends on human judgement, methodological understanding, and clear reasoning — none of which can be delegated to an automated system.",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  },
  {
    "objectID": "Report Writing/reportai.html#appropriate-use-of-large-language-models-and-other-generative-ai-tools",
    "href": "Report Writing/reportai.html#appropriate-use-of-large-language-models-and-other-generative-ai-tools",
    "title": "How to Report Statistical Results",
    "section": "",
    "text": "Large language models (LLMs) and other generative AI tools are increasingly available to students and researchers, and their use in statistical work must be governed by clear principles of academic integrity, transparency, and responsibility. When used appropriately, such tools can support learning and productivity; when misused, they risk undermining understanding, originality, and the credibility of reported results.\n\n\nGenerative AI tools may be used as assistive aids, rather than as substitutes for intellectual work. Appropriate uses include:\n\nLanguage and Presentation Support AI tools may be used to improve clarity, grammar, and readability of text that the student has already written. For example, rephrasing sentences for conciseness, correcting grammatical errors, or suggesting clearer transitions between paragraphs is acceptable, provided the underlying ideas, structure, and arguments originate from the author.\nClarification of Concepts AI tools may assist in explaining statistical or mathematical concepts at a high level, helping the student understand definitions, assumptions, or standard interpretations. This is analogous to consulting a textbook, lecture notes, or an online explainer, and should be used to support — not replace — independent understanding.\nCode Debugging and Syntax Assistance Generative AI can be used to help identify syntax errors, suggest alternative implementations, or explain what a piece of code does. For example, asking why an R pipeline fails or how a plotting function works is acceptable. However, the student remains responsible for ensuring that the code is correct, appropriate, and fully understood.\nReproducibility and Workflow Support AI tools may assist in suggesting reproducible workflows, commenting code for clarity, or reminding the user of best practices (e.g. setting seeds, structuring scripts, or documenting functions). These uses support good statistical practice without contributing substantive analytical content.\nIdea Organisation and Planning Using AI to outline report sections, suggest headings, or check logical flow is acceptable, provided the final content is written and justified by the student.\n\nIn all such cases, the AI tool functions as a supporting instrument, similar to a spell-checker, reference book, or programming forum.\n\n\n\n\nCertain uses of generative AI are not appropriate in academic statistical work:\n\nGenerating Substantive Analysis or Results It is not acceptable to use AI tools to generate statistical analyses, interpret results, or draw conclusions that the student does not fully understand or cannot independently justify. This includes AI-generated explanations of model outputs, hypothesis test conclusions, or causal claims presented as the student’s own reasoning.\nWriting Large Sections of the Report Submitting AI-generated text as original work — including introductions, methods, results, or discussion sections — constitutes academic misconduct if not explicitly permitted and acknowledged. This applies even if the text is edited afterwards.\nProducing Code Without Understanding Using AI to generate entire scripts or analyses without understanding how they work is inappropriate. Statistical programming is part of the assessed intellectual work, and blindly using generated code risks errors, misinterpretation, and irreproducibility.\nFabrication of References, Data, or Methods AI tools may produce plausible-sounding but incorrect citations, datasets, or methodological claims. Including fabricated or unverified material in a report is a serious breach of academic standards.\nCircumventing Assessment Objectives Any use of AI that bypasses the intended learning outcomes — such as avoiding engagement with statistical reasoning, derivations, or interpretation — is inappropriate, even if not explicitly prohibited.\n\n\n\n\n\nStudents are responsible for everything submitted under their name, regardless of whether AI tools were used. Good practice includes:\n\nEnsuring all results can be explained and defended orally if required.\nVerifying all claims, references, and code outputs independently.\nAcknowledging AI assistance where required by departmental or university policy.\n\nUltimately, generative AI tools should be viewed as assistive technologies, not as authors, analysts, or decision-makers. Sound statistical reporting depends on human judgement, methodological understanding, and clear reasoning — none of which can be delegated to an automated system.",
    "crumbs": [
      "Home",
      "Report Writing",
      "How to Report Statistical Results"
    ]
  }
]